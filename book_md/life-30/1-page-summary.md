![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Life 3.0

Back to Discover

[[book_md/life-30/preview|preview]]

  * [[book_md/life-30|life-30]]
  * Full Book Guide

    * [[book_md/life-30/exercise-reflect-on-artificial-superintelligence|exercise-reflect-on-artificial-superintelligence]]
  * [[book_md/life-30/highlights|highlights]]
  * [[book_md/life-30/community|community]]



![](/img/tutorial-fonts.175b2111.svg)

##### Change text options

Here you can change the font, text size, and reading screen to just how you like it. 

Next

  *   *   *   *   * 


![](/img/tutorial-menu.4c76dd27.svg)

##### Table of contents

Here you’ll find everything else, including the full chapter-by-chapter guide, your highlights, PDF downloads, and book discussions. 

Next

  *   *   *   *   * 


![](/img/tutorial-player.d25b1afb.svg)

##### Audio

Every guide has an audio narration so you can listen on the go. 

Next

  *   *   *   *   * 


![](/img/tutorial-favorite.b948300a.svg)

##### Add to Favorite

Mark your favorite guides here. You can find your favorites on your homepage. 

Next

  *   *   *   *   * 


![](/img/tutorial-night.ddd7fb5c.svg)

##### Night Mode

Like a darker look when you read? Turn dark mode on here. 

Finish

  *   *   *   *   * 


Adding to Favorites 

Removing from Favorites 

## 1-Page Summary

Life on Earth has drastically transformed since it first began. The first single-celled organisms could do little more than replicate themselves. Fast-forward to today: Humans have built a civilization so complex that it would be utterly incomprehensible to the lifeforms that came before us.

Judging by recent technological strides, author Max Tegmark believes that an equally revolutionary change is underway. If an amoeba is “Life 1.0,” and humans are “Life 2.0,” Tegmark contends that **an artificial superintelligence could become “Life 3.0.”** A power like this could either save or destroy humanity, and Tegmark argues that it’s our responsibility to do everything we can to ensure a positive outcome—before it’s too late.

(Shortform note: This view of the history and progression of life is central to a philosophy called “Dataism.” According to Yuval Noah Harari in _Homo Deus_ , Dataists believe that lifeforms are more valuable and meaningful depending on how well they can process complex data. Thus, humans aren’t uniquely more valuable than other forms of life—we’re just the most advanced form of data processor that exists so far, and we should hand control of the planet over to AI when it surpasses us in that regard. Harari contrasts this philosophy with “Techno-Humanism,” which says that humans should continue ruling the Earth and should use technology to upgrade the human mind.)

Tegmark is a physics professor at MIT and president of the Future of Life Institute, a nonprofit dedicated to using technology to avert threats to humanity on a global scale. He wrote _Life 3.0_ in 2017 to popularize urgent AI-related issues and increase the chance that humanity will successfully use AI to create a better future.

We’ll begin this guide by giving some background information on the current situation: What is artificial superintelligence, and might it exist soon? Then, we’ll explain what Tegmark thinks may happen if we create an artificial superintelligence, exploring the theoretical limits of an AI’s power and how it may impact life on Earth in the long term. Finally, we’ll turn our attention to AI-related problems we’ll have to manage in the _near_ future as well as steps we can take today to mitigate the risks of artificial superintelligence.

In our commentary, we’ll offer contrasting perspectives from other leading AI experts—those who think Tegmark’s view of AI is unrealistically alarmist and those who feel it’s an _even more_ urgent concern than Tegmark. We’ll also update Tegmark’s ideas with news regarding AI research and development.

### The Situation: Artificial Superintelligence Might Appear Soon

#### What Is Artificial Superintelligence?

Tegmark defines intelligence as the capacity to successfully achieve complex goals. Thus, **an “artificial superintelligence” is a computer sophisticated enough to understand and accomplish goals far more capably than today’s humans.** For example, a computer that could manage an entire factory at once—designing, manufacturing, and shipping out new products all on its own—would be an artificial superintelligence. By definition, a superintelligent computer would have the power to do things that humans currently can’t; thus, it’s likely that its invention would drastically change the world.

Tegmark asserts that if we ever invent artificial superintelligence, it will probably occur after we’ve already created “artificial general intelligence” (AGI). This term refers to an AI that can accomplish _any_ task with at least human-level proficiency—including the task of _designing more advanced AI_.

Experts disagree regarding how likely it is that computers will reach human-level general intelligence. Some dismiss it as an impossibility, while others only disagree on _when_ it will probably occur. According to a survey Tegmark conducted at a 2015 conference, the average AI expert is 50% certain that we’ll develop AGI by 2055.

AGI is important because, in theory, a computer that can redesign itself to be more intelligent can use that new intelligence to redesign itself _more quickly_. AI experts call this hypothetical accelerating cycle of self-improvement an _intelligence explosion_. If an intelligence explosion were to happen, **the most sophisticated computer on the planet could advance from mildly useful AGI to world-transforming superintelligence in a remarkably short time** —perhaps just a few days or hours**.**

(Shortform note: A more recent and expansive survey of 738 machine learning experts conducted in 2022 confirms that Tegmark’s survey data is still close to representative of expert opinion. The average expert is 50% certain that AI will be able to beat human workers at any task by the year 2059. Fifty-four percent of experts surveyed also believe that, once AGI exists, the odds of an intelligence explosion of some kind happening are “about even” or better on a scale from “very unlikely” to “very likely.”)

> **Counterargument: Why Artificial Superintelligence May Be Impossible**
> 
> Some experts contend that the possibility of an artificial superintelligence, able to accomplish any goal better than humans, is a myth.
> 
> They argue that **Tegmark’s definition of intelligenceis overly simplistic, as it falsely assumes that intelligence is a quantity you can measure with a single metric**. There’s no one mental attribute or process you can use to achieve all complex goals; rather, different goals require different kinds of intelligence. For example, bloodhounds are more intelligent than humans at distinguishing and tracking specific smells, while humans are more intelligent at writing poetry. In short, _there’s no such thing as general intelligence_ , which means a hypothetical computer with superhuman-level general intelligence is impossible.
> 
> Critics of Tegmark’s view argue that part of the reason people believe in general intelligence is that we’re limited by the human point of view. Although it seems like humans can apply their cognitive skills to solve any problem (and therefore have general intelligence), in reality, it only seems that way because we spend all our time solving problems that the human brain is readily able to solve. We only believe that computers have the potential to be much smarter than we are because we can see them doing things humans can’t, like instantly solving complex math calculations. However, they have a different kind of intelligence, not necessarily _more_ intelligence.
> 
> By this logic, an AI simulating the human brain wouldn’t be a true “AGI”—it would just be able to solve the kinds of problems that humans can. Additionally, such an AI probably wouldn’t be able to trigger an intelligence explosion. Even if it could improve its own programming as well as a human could, there’s no reason to believe further iterations would be able to solve problems at an exponentially higher speed. This belief relies on the assumption that the ability to solve any problem is a uniform trait you can procedurally optimize—and as we’ve discussed, this may not be the case.

#### Evidence That Artificial Superintelligence Is Possible

The idea that we could build a computer that’s smarter than we are may seem far-fetched, but some evidence indicates that such technology is on its way, according to Tegmark.

First, the artificial intelligence we’ve created so far is functioning more and more like AGI. Researchers have developed a new way to design sophisticated AI called _deep reinforcement learning_. Essentially, they’ve created computers that can repeatedly modify themselves in an effort to accomplish a certain goal. This means that **machines can now use the _same process_ to learn _different_ skills**—a necessary component of AGI. Currently, there are many human skills that developers can’t teach to computers using deep reinforcement learning, but that list is becoming shorter.

(Shortform note: By using a specific form of deep reinforcement learning called Reinforcement Learning from Human Feedback (RLHF), AI developers can get computers to optimize for _vaguely defined or subjective_ values. For instance, the developers at OpenAI used this process to train the AI-based chatbot ChatGPT: Humans answered sample prompts to show ChatGPT what kinds of answers they wanted. Then, they allowed the chatbot to try answering prompts and scored its answers based on how accurate and desirable they were. The AI then used this “reward” data to create a model describing what qualifies as a “good” answer, allowing it to further train itself.)

Second, Tegmark asserts that **given everything we know about the universe, there’s no obvious reason to believe that artificial superintelligence is impossible.** Although it may seem like our brains possess unique creative powers, they store and process information in much the same way that computers do. The information in our heads is a biological pattern rather than a digital one, but the information itself is the same no matter what material it’s encoded with. In theory, computers can do everything our brains can do.

(Shortform note: Some experts disagree, arguing that although it’s theoretically possible for hardware components to run the same information processes as a human brain, it’s not necessarily true that it could do so at the same speed. The organic material of the human brain is far faster than computers at processing tons of data—we only assume otherwise because the brain processes data unconsciously. If a computer could process information like a brain, it would likely be so slow that it would defeat the purpose of simulating a brain in the first place. By this logic, it might be impossible to create a human-level AI that can improve itself faster than a human could, placing the intelligence explosion out of our grasp.)

Tegmark concedes that AGI and artificial superintelligence might still be a pipe dream, impossible to create for some reason we don’t yet see. However, he contends that if there’s even a small chance that an artificial superintelligence will exist in the near future, it’s our responsibility to do anything in our power to ensure that it has a positive impact on humanity. This is because artificial superintelligence has the power to completely transform the world—or end it—as we’ll see next.

(Shortform note: Some experts contend that because we don’t know if a world-ending artificial superintelligence is possible or how likely it is to happen, we need to pause all ongoing AI development until we understand the science enough to proceed safely. Due to current economic incentives to create profitable AI-based products, research laboratories are advancing AI as fast as they can. As a result, they’re arguably racing to see who can trigger an intelligence explosion first. A development pause (if possible) would require significant cooperation between research firms as well as governments around the world.)

### The Possibilities: How Could Artificial Superintelligence Change the World?

So far, we’ve explained that an artificial superintelligence is a technology that would greatly surpass human capabilities, and we’ve argued why it’s possible that artificial superintelligence might someday exist. Now, let’s discuss what this means for us humans.

We’ll explain why a superintelligence’s “goal” is the primary factor that will determine how it will change the world. Then, we’ll explore how much power such a superintelligence would have. Finally, we’ll discuss what might happen to humanity after a powerful superintelligence enters the world, investigating three optimistic scenarios followed by three pessimistic ones.

#### The Outcome Depends on the Superintelligence’s Goal

Tegmark asserts that if an artificial superintelligence comes into being, **the fate of the human race depends on what that superintelligence sets as its goal.** For instance, if a superintelligence pursues the goal of maximizing human happiness, it could create a utopia for us. If, on the other hand, it sets the goal of maximizing its intelligence, it could kill humanity in its efforts to convert all matter in the universe into computer processors.

It may sound like science fiction to say that an advanced computer program would “have a goal,” but this is less fantastical than it seems. An intelligent entity doesn’t need to have feelings or consciousness to have a goal; for instance, we could say an escalator has the “goal” of lifting people from one floor to another. In a sense, all machines have goals.

One major problem is that **the creators of an artificial superintelligence wouldn’t necessarily have continuous control over its goal and actions,** argues Tegmark. An artificial superintelligence, by definition, would be able to solve its goal more capably than humans can solve theirs. This means that if a human team’s goal was to halt or change an artificial superintelligence’s current goal, the AI could outmaneuver them and become uncontrollable.

For example: Imagine you program an AI to improve its design and make itself more intelligent. Once it reaches a certain level of intelligence, it could predict that you would shut it off to avoid losing control over it as it grows. The AI would realize that this would prevent it from accomplishing its goal of further improving itself, so it would do whatever it could to avoid being shut off—for instance, by pretending to be less intelligent than it really is. This AI wouldn’t be malfunctioning or “turning evil” by escaping your control; on the contrary, it would be pursuing the goal you gave it to the best of its ability.

> **How AI Researchers Are Proactively Discouraging Misaligned Goals**
> 
> Other AI researchers agree with Tegmark that artificial intelligences will have goals with high-stakes consequences, and they treat the threat of misaligned AI goals (goals that aren’t in humanity’s best interests) seriously. OpenAI, a leading AI research laboratory, is attempting to reduce the risk of developing an AI with the wrong goal in three ways.
> 
> First, they’re using copious amounts of human feedback to train their AI models (as described earlier in this guide). An AI programmed to emulate humans is less likely to adopt a goal that’s outright hostile toward humans. This tactic has yielded positive results in the models OpenAI has trained so far, and they anticipate human input to continue being a positive influence in the future.
> 
> Second, OpenAI is training AI to help them create more thorough, constructive feedback for future AI models. For instance, they’ve developed a program that writes critical comments about its own language output and one that fact-checks language output by surfing the web. The developers intend to use these tools to help them fine-tune the goals of other AI models and detect misaligned goals before an AI would become so intelligent that developers lose control over it.
> 
> Third, OpenAI is training AI models to research new ways to ensure goal alignment. They anticipate that future AI models will be able to invent new alignment strategies much more quickly and efficiently than humans could. Although such research-focused AIs would be very intelligent, their intelligence would be tailored for a narrower set of tasks than general-purpose AI models, making them easier to control.

##### Obstacles to Programming a Superintelligence’s Goal

Does this mean that we’re in the clear as long as we’re careful what goal we program into a superintelligence in the first place? Not necessarily.

First of all, Tegmark states that successfully programming an artificial superintelligence with a goal of our choosing would be difficult. While an AI is recursively becoming more intelligent, the only time we could program its ultimate goal would be **_after_ it’s intelligent enough to understand the goal, but _before_ it’s intelligent enough to manipulate us** into helping it accomplish whatever goal it’s set for itself. Given how quickly an intelligence explosion could happen, the AI’s creator might not have enough time to effectively program its goal.

(Shortform note: AI expert Eliezer Yudkowsky has an even more pessimistic perspective on this situation than Tegmark, arguing that humans would almost certainly fail to program an AI’s goal during an intelligence explosion. He asserts that we don’t understand AI systems enough to reliably encode them with human goals: The deep learning methods we use to train today’s artificial intelligence are, by nature, unintelligible, even to the researchers doing the training. Further AI development would result in an artificial superintelligence that adopts what Yudkowsky sees as a machine’s default view of humanity—as valueless clusters of atoms. Such an AI superintelligence would likely have an unfavorable impact on humanity.)

Second, Tegmark argues that **it’s possible for an artificial intelligence to discard the goal we give it and choose a new one.** As the AI grows more intelligent, it might come to see our human goals as inconsequential or undesirable. This could incentivize it to find loopholes in its own programming that allow it to satisfy (or abandon) our goal and free itself to take some other unpredictable action.

Finally, **even if an AI accepts the goals we give it, it could still behave in ways we wouldn’t have predicted (or desired)** , asserts Tegmark**.** No matter how specifically we define an AI’s goal, there’s likely to be some ambiguity in how it chooses to interpret and accomplish that goal. This makes its behavior largely unpredictable. For example, if we gave an artificial superintelligence the goal of enacting world peace, it could do so by trapping all humans in separate cages.

(Shortform note: One possible way to reduce the chance of an AI rejecting human goals or interpreting them in an inhuman way would be to focus AI development on cognition-enhancing neural implants. If we design a superintelligent AI to guide the decision-making of an existing human (rather than make its own decisions), they could collectively be more likely to respect humanist goals and interpret goals in a human way. The prospect of merging human and AI cognition is arguably less outlandish than it may seem—tech companies like Neuralink and Synchron have already developed brain-computer interfaces that allow people to control digital devices with their thoughts.)

#### The Possible Extent of AI Power

Why would an artificial superintelligence’s goal have such a dramatic impact on humanity? An artificial superintelligence would use all the power at its disposal to accomplish its goal. This is dangerous because **such a superintelligence could theoretically gain an unimaginable amount of power** —enough to completely transform our society with a negligible amount of effort.

According to Tegmark, although an artificial superintelligence is a digital program, it could easily exert power in the real world. For instance, it could make money selling digital goods such as software applications, then use those funds to bribe humans into unknowingly working for it (perhaps posing as a human hiring manager on digital job listing platforms). An AI controlling a Fortune 500-sized human task force could do almost anything—including creating robots that the AI could control directly.

Tegmark asserts that, in theory, an artificial superintelligence could eventually attain godlike power over the universe. By using its intelligence to create increasingly advanced technology, an AI could eventually create machines able to rearrange the fundamental particles of matter—turning anything into anything else—as well as generate nearly unlimited energy to power those machines.

> **AI’s Power in the Digital World**
> 
> While Tegmark focuses primarily on the ways AI could influence the physical world, Yuval Noah Harari emphasizes the danger posed by AI’s influence solely in the digital world. For instance, AI-controlled social media accounts could earn the trust of human users, distort their view of the world, and influence their behavior for political or economic ends.
> 
> Additionally, Harari contends that this kind of AI will threaten to unravel our society long before we develop superintelligent AI—in fact, he asserts that we should be aware of this potential danger _today_. People already regularly consult online resources to dictate their decisions. For instance, they use product reviews to determine what to buy, and they conduct online research to determine who to vote for. AI (or people controlling AI) therefore wouldn’t need to pay workers or manufacture reality-bending technology to totally reshape human society. Transforming the digital landscape we consult every day would be enough.
> 
> There’s evidence that this kind of distortion of the digital world has already begun—for instance, social media bots were used to discredit 2017 French presidential candidate Emmanual Macron by amplifying the spread of his leaked emails across social media platforms.

#### Optimistic Possibilities for Humanity

What might happen to humanity if an artificial superintelligence wields nearly unlimited power over the world in service of a single goal? Tegmark describes a number of possible outcomes, each of which results in a wildly different way of life for humans—or the end of human life.

Let’s begin by discussing three scenarios in which the AI’s goal, whatever it may be, allows humans to live relatively happy lives.

##### Possibility #1: Friendly AI Takes Over

First, Tegmark imagines that an artificial superintelligence could overthrow existing human power structures and use its vast intelligence to create the best possible world for humanity. **No one could challenge the AI’s ultimate authority, but few people would want to,** since they have everything they need to live a fulfilling life.

Tegmark clarifies that **this isn’t a world designed to maximize human pleasure** , which would mean continuously injecting every human with some kind of pleasure-inducing chemical. Rather, this is a world in which humans are free to continuously choose the kind of life they want to live from a diverse set of options. For instance, one human could choose to live in a non-stop party, while another could decide to live in a Buddhist monastery where rowdy, impious behavior wouldn’t be allowed. No matter who you are or what you want, there would be a “paradise” available for you to live in, and you could move to a new one at any time.

> **This Utopia Borrows From Existentialist Philosophy**
> 
> This vision of utopia makes assumptions about humanity that align with Victor Frankl’s existentialist philosophy in _Man’s Search for Meaning_. According to Frankl, the primary contributor to human happiness isn’t pleasure, but meaning—that is, humans need to feel like their actions are valuable in light of a bigger purpose.
> 
> However, Frankl contends that there isn’t one universal “meaning” of life. Rather, **anything can be meaningful, and each individualmust discover for themselves what their life means to them.** This reveals the importance of _personal choice_ , which is central to Tegmark’s vision of utopia. Because the same life won’t feel meaningful to everyone, an AI-created world that maximizes human happiness would need to allow humans to pursue whatever life they believe to be the most meaningful.
> 
> To return to our example, someone living a non-stop party could find meaning through the experience of togetherness, while someone living as a Buddhist monk could find meaning in pious, devoted action. If that partygoer ever realized that a permanent party made them feel purposeless, they could cross over to the Buddhist sector and find meaning there.
> 
> In contrast, a world in which people are constantly taking a pleasure-inducing drug rather than living exciting lives might be OK for the people experiencing it, but most people today would probably view this as a dystopia rather than a utopia. This is because such a world would be _meaningless_ , as every experience would feel exactly the same and nothing would ever change.

##### Possibility #2: Friendly AI Stays Hidden

Tegmark supposes another positive scenario that’s a bit different: Instead of completely taking over the world, **an artificial superintelligence does everything within its power to improve human lives while _keeping its existence a secret_.**

This could happen if the artificial superintelligence—or someone influencing its goals—concluded that to be as happy and fulfilled as possible, humans need to feel in control of their destiny. Arguably, if you knew that an all-powerful computer could give you anything you wanted (as in the previous optimistic scenario), you might still feel like your life is meaningless and be less satisfied because you don’t have _control_ over your life. In this case, the best thing a godlike AI could do for you is help without your knowledge.

> **Is the Universe Secretly a Simulation?**
> 
> If an all-powerful artificial intelligence could adopt the goal of keeping its existence a secret, how do we know one doesn’t exist already? This idea overlaps with _simulation theory_ , the idea that our entire universe, including ourselves, is a complex computer simulation that’s creating our reality yet hiding its true nature.
> 
> This idea is popular among some physicists, who’ve come to this conclusion using facts we know about the universe. Because it’s theoretically possible that humans will at some point create a computer powerful enough to simulate the universe, there’s a chance that another civilization already has—and has created us. If so, Tegmark’s logic could explain why the true nature of this simulation is hidden from us: If we prefer control over our destiny, it would be cruel to take that illusion away from us.

##### Possibility #3: AI Protects Humanity From AI

Third, Tegmark imagines a scenario in which **humans create an artificial superintelligence with the sole purpose of preventing other superintelligences from coming into existence.** This allows humans to continue developing more advanced technology without worrying about the potential dangers of another AI.

(Shortform note: This scenario is arguably relatively unrealistic, as it assumes that we have total control over the superintelligence yet aren’t taking full advantage of its power. If we’re able to successfully program a superintelligence’s goal, we would likely get more ambitious and tell it to design a society for us in which we can be eternally happy and immortal—which would bring us back to one of the previous two optimistic scenarios we’ve discussed.)

According to Tegmark, the advanced technology humans could develop in a world free from superintelligence would eventually allow us to create a bountiful classless society. Robots are able to build anything humans might want, making scarcity a thing of the past. Since robots are constantly generating surplus wealth, the government can give everyone a universal basic income (UBI) that’s high enough to purchase anything they could possibly need. People are free to work for more money, but finding a productive job is near-impossible since everything people might buy is already given to them for free.

(Shortform note: Tegmark also acknowledges the possibility that we develop superintelligence, manage to keep it entirely under our control, and use it to create a humanist utopia. Although he doesn’t specify what he imagines this world would look like, it’s reasonable to assume that Tegmark thinks it would mirror the outcome of one of these three positive scenarios.)

> **How to Transition to a Post-Scarcity Society**
> 
> Becoming a post-scarcity, UBI-driven society like this would require a complete overhaul of our employment-based economy. But, we wouldn’t necessarily have to enact this kind of sweeping change all at once.
> 
> AI expert Lorenzo Pieri describes how a nation might incrementally transition to this kind of society. First, the private companies that serve basic human needs progressively automate their workforces with increasingly sophisticated technology, boosting the nation’s total economic production. As existing taxes channel some of this new wealth into the government, they pass it on to citizens as a small universal basic income. The government grows this UBI alongside the economy as a whole, helping everyone increase their quality of life—especially those in poverty, whose currently unfilled needs are more essential to their well-being than the unfilled needs of people with more money.
> 
> After private companies have automated the production of all basic human needs, the government begins funneling the wealth from the automated production of non-basic goods into _subsidies for basic goods_ rather than further increasing UBI. Eventually, the government can use the productivity gains from automation to pay private companies to supply all basic human needs for free, transitioning into a fully post-scarcity society.
> 
> If citizens desire non-basic goods that aren’t available for free, they still can get jobs producing such luxury goods for others. Pieri terms this dynamic “luxury-capitalism.”

#### Pessimistic Possibilities for Humanity

Next, let’s take a look at some of the existential dangers that an artificial superintelligence poses. Here are three scenarios in which the AI’s goal ruins humans’ chances to live a satisfying life.

##### Possibility #1: AI Kills All Humans

Tegmark contends that **an artificial superintelligence may end up killing all humans in service of some other goal.** If it doesn’t value human life, it could feasibly end humanity just for simplicity’s sake—to reduce the chance that we’ll do something to interfere with its mission.

(Shortform note: Some argue that an artificial superintelligence is unlikely to kill all humans as long as we leave it alone. Conflict takes effort, so an AI might conclude that the simplest option available is to pursue its mission in isolation from humanity. For instance, a superintelligence might be peaceful toward us if we don’t interfere with its goal and allow it to colonize space.)

If an artificial superintelligence decided to drive us extinct, Tegmark predicts that it would do so by some means we currently aren’t aware of (or can’t understand). Just as humans could easily choose to hunt an animal to extinction with weapons the animal wouldn’t be able to understand, an artificial intelligence that’s proportionally smarter than we are could do the same.

(Shortform note: In _Superintelligence_ , Nick Bostrom imagines one way an artificial intelligence could kill all humans through means we would have difficulty understanding or averting: self-replicating “nanofactories.” This would be a microscopic machine with the ability to reproduce and synthesize deadly poison. Bostrom describes a scenario in which an artificial intelligence produces and spreads these nanofactories throughout the atmosphere at such a low concentration that we can’t detect them. Then, all at once, these factories turn our air toxic, killing everyone.)

##### Possibility #2: AI Cages Humanity

Another possibility is that **an artificial intelligence chooses to keep humans alive, but it doesn’t put in the effort to create a utopia for us.** Tegmark argues that an all-powerful superintelligence might decide to keep us alive out of casual curiosity. In this case, an indifferent superintelligence would likely create a relatively unfulfilling cage in which we’re kept alive but feel trapped.

(Shortform note: A carelessly, imperfectly designed world for humans to live in may be intolerable in ways we can’t imagine. This idea is dramatized by the ending of Stanley Kubrick’s 1968 film _2001: A Space Odyssey_. As Kubrick explains in an 1980 interview, the film portrays an astronaut trapped in a “human zoo” created for him by godlike aliens who want to study him. They place the astronaut in a room in which it feels like all of time is happening simultaneously, and he ages and dies all at once.)

##### Possibility #3: Humans Abuse AI

Finally, Tegmark imagines a future in which **humans gain total control over an artificial superintelligence and use it for selfish ends.** Theoretically, someone could use such a machine to become a dictator and oppress or abuse all of humanity.

(Shortform note: This scenario would likely result in even more suffering than if a supreme AI decided to kill all humans. Paul Bloom (_Against Empathy_) asserts that cruelty is a uniquely human act in which someone feels motivated to punish other humans for their moral failings. Thus, a superintelligence under the control of a hateful dictator would be far more likely to intentionally cause suffering (as moral punishment) than an AI deciding for itself what to do.)

### What Should We Do Now?

We’ve covered a range of possible outcomes of artificial superintelligence, from salvation to disaster. However, all these scenarios are merely theoretical—let’s now discuss some of the obstacles we can address today to help create a better future.

We’ll first briefly disregard the idea of superintelligence and discuss some of the less speculative AI-related issues society needs to overcome in the near future. Then, we’ll conclude with some final thoughts on what we can do to improve the odds that the creation of superintelligence will have a positive outcome.

#### Short-Term Concerns

The rise of an artificial superintelligence isn’t the only thing we have to worry about. According to Tegmark, it’s likely that rapid AI advancements will create numerous challenges that we as a society need to manage. Let’s discuss:

  * Concern #1: Economic inequality
  * Concern #2: Outdated laws
  * Concern #3: AI-enhanced weaponry



##### Concern #1: Economic Inequality

First, Tegmark argues that **AI threatens to increase economic inequality.** Generally, as researchers develop the technology to automate more types of labor, companies gain the ability to serve their customers while hiring fewer employees. The owners of these companies can then keep more profits for themselves while the working class suffers from fewer job opportunities and less demand for their skills. For example, in the past, the invention of the photocopier allowed companies to avoid paying typists to duplicate documents manually, saving the company owners money at the typists’ expense.

As AI becomes more intelligent and able to automate more kinds of human labor at lower cost, this asymmetrical distribution of wealth could increase.

(Shortform note: Some experts contend that new AI-enhanced technology doesn’t have to lead to automation and inequality. If AI developers create technology that expands what one worker can do, rather than just simulating their work, that technology could create new jobs and update old ones while creating value for companies. These experts implore AI developers to consider the impact of their inventions on the labor market and adjust their plans accordingly, just as they would consider any other ethical or safety concern.)

##### Concern #2: Outdated Laws

Second, Tegmark contends that **our legal system could become outdated and counterproductive in the face of sudden technological shifts.** For example, imagine a company releases thousands of AI-assisted self-driving cars that save thousands of lives by being (on average) safer drivers than humans. However, these self-driving cars still get into some fatal accidents that wouldn’t have occurred if the passengers were driving themselves. Who, if anyone, should be held liable for these fatalities? Our legal system needs to be ready to adapt to these kinds of situations to ensure just outcomes while technology evolves.

(Shortform note: Although Tegmark contends that the legal system will struggle to keep up with AI-driven changes, other experts note that advancements in AI will drastically increase the productivity and efficiency of legal professionals. This could potentially help our legal system adapt more quickly and mitigate the damage caused by rapid change. For instance, in a self-driving car liability case, an AI language model could quickly digest and summarize all the relevant documents from similar cases from the past (for instance, a hotel-cleaning robot that injured a guest), instantly collecting the context necessary for legislators to make well-informed decisions.)

##### Concern #3: AI-Enhanced Weaponry

Third, **AI advancements could drastically increase the killing potential of automated weapons systems** , argues Tegmark. AI-directed drones would have the ability to identify and attack specific people—or groups of people—without human guidance. This could allow governments, terrorist organizations, or lone actors to commit assassinations, mass killings, or even ethnic cleansing at low cost and minimal effort. If one military power develops AI-enhanced weaponry, other powers will likely do the same, creating a new technological arms race that could endanger countless people around the world.

(Shortform note: In 2017, the Future of Life Institute (Tegmark’s nonprofit organization) produced an eight-minute film dramatizing the potential dangers of this type of AI-enhanced weaponry. After this video went viral, some experts dismissed its vision of AI-directed drones as scaremongering, arguing that even if multiple military powers developed automated drones, such weapons wouldn’t be easily reconfigured to target civilians. However, in a rebuttal article, Tegmark and his colleagues pointed to an existing microdrone called the “Switchblade” that can be used to target civilians.)

#### Long-Term Concerns

How should we address the long-term concerns related to AI, including potential superintelligence creation? Because there’s little we know for sure about the future of AI, Tegmark contends that **one of humanity’s top priorities should be AI research.** The stakes are high, so we should try our best to discover ways to control or positively influence an artificial superintelligence.

> **The Current State of AI Research Funding**
> 
> It seems that many people agree with Tegmark, as major institutions around the world are already prioritizing AI research. The European Union is currently investing €1 billion a year in AI research and development, and it intends to increase that annual investment to €20 billion by the year 2030. Private companies are leading AI research in the United States—for instance, Meta plans to spend $33 billion on AI research in 2023 alone.
> 
> However, some experts worry that private companies may not act in line with public interest while researching AI, and they urge the US government to fund a cutting-edge AI research program of its own. It’s possible that state-controlled research would be less likely to unleash a dangerous superintelligence, as governments lack the profit motive to create a marketable AGI as quickly as possible.

##### What Can You Do to Prevent the AI Apocalypse?

Outside of AI research, Tegmark recommends cultivating _hope grounded in practical action_. **Before we can create a better future for humanity, we have to believe that a bright future is possible** _if_ we band together and responsibly address these technological risks.

After cultivating this optimistic attitude, **Tegmark urges readers to do everything they can to make the world more ethical and peaceful** —not just in the field of AI, but in every aspect of society. The more humans who are willing to empathize and cooperate with one another, the greater the chance that we’ll develop AI safely and with the intent to benefit all of humanity. This could involve organizing a fundraiser for a local homeless shelter, volunteering at a nursing home, or just being kinder to the people around you.

> **Determinate vs. Indeterminate Optimism**
> 
> The attitude Tegmark urges readers to adopt is what Peter Thiel in _Zero to One_ calls “determinate optimism”—you expect the future to be better than the present, and you believe that you can successfully predict and bring about specific positive outcomes. Thiel argues that in contrast to this perspective, most Americans today (or in 2014, when _Zero to One_ was written) think in terms of “indeterminate optimism”—they believe that things will get better in the future, but they assume that the future is too unpredictable for them to plan.
> 
> According to Thiel, this is a problem because indeterminate optimism encourages people to be passive and short-sighted: They think, “Why bother planning a better future? Things will turn out OK no matter what I do.” To combat this, Thiel urges optimists to _make long-term plans_ and stick to them. To apply this to Tegmark’s plea to make the world more ethical and peaceful: Don’t just become a generally cooperative person in life; instead, come up with a plan to bring people together and motivate them to treat others well.

[[book_md/life-30/preview|preview]]

[[book_md/life-30/exercise-reflect-on-artificial-superintelligence|exercise-reflect-on-artificial-superintelligence]]

##### Welcome!

Let’s go on a quick tour of a Shortform book guide. 

Start

##### 1-Page Summary

Every guide starts with a 1-Page Summary. This is a 5-10 minute overview of the book’s key points. 

Next

##### Finished!

If you ever need to see this tour again, click here. 

Close

Guided Tour

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=0cf9687d-c6ab-489d-b8bf-ae07873ae673&sid=49fff5b0636c11eeb9c611038afc8668&vid=4a005010636c11ee80c703d4c4a7acd5&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Flife-30%2F1-page-summary&r=&lt=275&evt=pageLoad&sv=1&rn=446570)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



