![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Rationality

Back to Discover

[[book_md/rationality/preview|preview]]

  * [[book_md/rationality|rationality]]
  * Full Book Guide

    * [[book_md/rationality/exercise-examine-your-myside-thinking|exercise-examine-your-myside-thinking]]
  * [[book_md/rationality/highlights|highlights]]
  * [[book_md/rationality/community|community]]



![](/img/tutorial-fonts.175b2111.svg)

##### Change text options

Here you can change the font, text size, and reading screen to just how you like it. 

Next

  *   *   *   *   * 


![](/img/tutorial-menu.4c76dd27.svg)

##### Table of contents

Here you’ll find everything else, including the full chapter-by-chapter guide, your highlights, PDF downloads, and book discussions. 

Next

  *   *   *   *   * 


![](/img/tutorial-player.d25b1afb.svg)

##### Audio

Every guide has an audio narration so you can listen on the go. 

Next

  *   *   *   *   * 


![](/img/tutorial-favorite.b948300a.svg)

##### Add to Favorite

Mark your favorite guides here. You can find your favorites on your homepage. 

Next

  *   *   *   *   * 


![](/img/tutorial-night.ddd7fb5c.svg)

##### Night Mode

Like a darker look when you read? Turn dark mode on here. 

Finish

  *   *   *   *   * 


Adding to Favorites 

Removing from Favorites 

## 1-Page Summary

What does it mean to be rational? Why is being rational important? How can you be a more rational person? In _Rationality_ , Steven Pinker argues that rationality and reason are essential for improving our world and society, but that people often misunderstand them, acting irrationally even when they think they’re not. He examines how you can be more rational and make better decisions by improving your critical thinking skills and by understanding—and thus avoiding—the logical fallacies and cognitive blunders that people often fall victim to.

Steven Pinker is a cognitive psychologist whose best-selling works include _The Stuff of Thought_ (2008), _The Better Angels of Our Nature_ (2011), and _Enlightenment Now_ (2018). He’s a member of the National Academy of Sciences and has taught at several universities including Harvard, Stanford, and MIT. As an experimental psychologist, he’s conducted research on visual cognition, psycholinguistics, and social relations.

In our guide, we’ll explore Pinker’s definition of rationality, his argument for why it matters, and his advice on how you can think more rationally about your choices. We’ll also go over the key reasons why humans, despite our ingenuity, often think and behave so irrationally. Along the way, we’ll look at how other experts and thinkers have explored the topic and how they’ve added to the conversation.

### What Is Rationality?

Pinker defines rationality as _the use of knowledge to attain goals_. Within this definition, _knowledge_ is _a belief that can be proven true_. Both of these aspects—** _goals_ and _true, provable knowledge_ —are critical aspects of rationality.** You’re not acting rationally if you act on false beliefs, nor are you acting rationally if you don’t apply your beliefs to a goal—a rational person must have a purpose for their thinking, whether that be to get around a physical object or to determine the truth of an idea.

> **The Cognitive Versus Emotional Aspect of Rationality**
> 
> Pinker’s definition of rationality aligns with that of most thinkers who have attempted to describe it—at its base, rationality holds that when you make a choice (working toward a goal), you’ll choose the thing you’ve determined will be most advantageous to you. This definition typically focuses on how we _cognitively_ assess that advantage, and it usually assumes that thinking rationally is inherently at odds with thinking emotionally.
> 
> However, some point out that if you make a choice based on what you like best, you might, in doing so, follow your emotions instead of your mind, but because the result is that you choose the option you best prefer, it still satisfies the definition of rationality. This points to the difficulty of nailing down a definition of rationality that eliminates any influence of irrationality, since emotions are such an integral part of human decision-making, thus blurring the line between rational and irrational thought and action.

### Why Rationality Matters

Pinker contends we as **humans have a moral obligation to think rationally** , for two reasons.

First, rationality is moral because it’s through rationality that we build a better society. Reason has enabled us to make valuable discoveries, inventions, and social progress that have allowed humans to live better lives. Because we have a moral imperative to improve the world and the lives of others, and since rationality is a key aspect of our ability to do so, we have a moral obligation to think rationally.

Second, morality hinges on agreement on right and wrong. **Reason allows us to think impartially, consider conflicting interests, and determine what contributes to the common good**. Because rationality is the only thing that allows us to come to a collective consensus on anything, it’s the only way to prevent one group from deciding what’s right and forcing their ideas on others, which is inherently immoral.

> **Carl Sagan and the Morality of Science**
> 
> Scientist Carl Sagan has also argued that rationality is moral, specifically in terms of how it leads to scientific advancements. In _The Demon-Haunted World_ , he notes that some people consider science immoral because it has led to negative consequences that have decreased the quality of life for many, such as nuclear weapons, climate change, and the adverse effects of technology. However, Sagan argues that the benefits of science to society outweigh these negative outcomes—improvements to agriculture, medicine, and communication, for example, have markedly raised the standard of living for billions of people. He therefore believes that adhering to science is moral.
> 
> Sagan also weighs in on how science can promote ethical thinking in general. He argues that science can counter bigotry and intolerance,—for instance, when it dispels racial and misogynistic stereotypes that lie at the basis of much inequality. He also notes, in alignment with Pinker’s argument, that a society that thinks critically can better fight back against misleading propaganda produced by authoritarian governments, which rationality leads to more peaceful, equitable governance.

### Rational Choice Theory

Pinker explains that economists, philosophers, and others have defined rationality using the _rational choice theory_ , which states that **people will choose between options with the aim of maximizing the rewards of their decision**.

More specifically, when making a rational decision, a person considers the possible outcomes of each option, judges how much they want (or don’t want) each outcome, and considers how likely each outcome is to happen. **This analysis yields an option’s “expected utility”: its potential rewards weighted by both desirability and probability**. A rational person then chooses the option with the highest expected utility.

For example, if someone is deciding if they should pack a warm coat for a trip, they’ll first weigh the possible outcomes of each option by desirability. If the weather gets cold, they’ll be happy they packed the coat. But if the weather ends up being warm, they’ll be annoyed that they wasted space in their suitcase on such a bulky and heavy item. They’ll then weigh the likelihood of each option: Is it more likely the weather will be cold or warm? And from there, they’ll decide which option has the highest expected utility, and they’ll make that choice.

Pinker notes that one person's judgment of expected utility might differ from someone else’s if their values are different. This doesn’t mean that either choice is more or less rational than the other—they can be different but still be equally rational. If Joe is sensitive to cold, he might opt to bring a coat even if it might mean overpacking, while Jenny, who doesn’t mind the cold as much but hates overpacking, might leave her coat at home even if it might mean being chilly. Both choices are rational because they each align with the expected utility as determined by each chooser.

> **Rational Choice Theory Through the Ages**
> 
> Rational choice theory was first proposed by 18th-century economist Adam Smith, who argued that people perform a cost-benefit analysis on every decision to maximize their individual rewards. Though Smith didn’t detail how a person would weigh desirability versus probability like Pinker, his overall theory aligns with the underlying theme of Pinker’s argument: People make decisions according to what they think will benefit them.
> 
> Smith primarily focused on measurable, financial rewards, but in the mid-20th century, thinkers in other fields, including social behaviorists, political scientists, and criminologists, adopted the theory and broadened it to account for how people navigate not just economic decisions but also relationships, using the theory to describe how people make decisions when interacting with others. In this way, the theory has been adapted to describe and predict human behavior in any number of situations, not just economic circumstances as originally intended.

Pinker notes that while people may not have the time nor inclination to thoroughly calculate the expected utility of every decision, if they think in these terms more often, they’d make better decisions overall. In the following sections, we’ll look at reasons people fail to think rationally and techniques people can use to think critically more often.

(Shortform note: In _The Paradox of Choice_ , Barry Schwartz argues that not only is it not feasible to fully weigh the pros and cons of every decision (as Pinker notes), but it’s also not desirable. He distinguishes between _maximizers_ , who weigh each choice (even small ones) carefully for the best possible option, and _satisficers_ , who more quickly accept options that meet their standards even if not perfect. He argues that maximizing takes up more time than it’s worth and leads to stress, while satisficing leads to decent enough results without the need to carefully think through the nuances of every option. In this way he and Pinker align in their realistic approach to how far a person can push rationality.)

### Logic and Critical Thinking

Pinker writes that **one of the main reasons people think irrationally is that they use logic and critical thinking incorrectly**. Critical thinking (also known as deductive reasoning) is the ability to accurately assess logic—to judge whether a conclusion is true based on its premises.

(Shortform note: In _Blink_ , Malcolm Gladwell differentiates between two systems of thought that we use to arrive at decisions: conscious and unconscious. Our conscious minds think critically and weigh logical arguments, and our unconscious minds produce snap judgments that we sometimes can’t fully explain. While Pinker argues that deductive reasoning, which comes from our conscious mind, is the superior method of decision-making, Gladwell contends that our snap judgments can often produce better decisions because our subconscious brains are remarkably effective at sorting through and ignoring irrelevant information—details that can trip us up when we try to consider all possible premises logically when arriving at a conclusion.)

People often misinterpret logic and thus engage in fallacious arguments that lead to irrational conclusions. Pinker outlines two kinds of fallacies that people tend to succumb to: _formal_ (where the conclusion is wrong) and _informal_ (where the premises are wrong).

#### Formal Fallacies

**A _formal fallacy_ is one where the conclusion doesn’t follow logically from the premise—thus violating the _form_ (the structure) of a logical statement**. This means the premises are valid and true, but the conclusion drawn from them doesn’t follow logically and therefore isn’t valid.

One common formal fallacy is called _denying the antecedent_(in other words, saying the first premise isn’t true, and then drawing a false conclusion):

  * Premise: A equals B.
  * Premise: Not A.
  * Conclusion: Therefore, not B.



Here, the conclusion is fallacious because it might not always be the case. For example, you might say:

  * If a creature is a fish, it can swim. 
  * A human creature is not a fish. 
  * Therefore, a human cannot swim. 



In this case, both premises are valid, but the conclusion is false.

Another common formal fallacy is called _affirming the consequent_ (or, saying the second premise is true and then drawing a false conclusion):

  * Premise: A equals B.
  * Premise: B.
  * Conclusion: Therefore, A.



This might play out as:

  * If a creature is a fish, it can swim.
  * Humans can swim.
  * Therefore, humans are fish.



This type of fallacy, affirming the consequent, is a common one people fall for because it implies a reciprocity that seems straightforward but is often not true: Just because _A equals B_ , we can’t necessarily say _B equals A_. However, without deeper reflection, that conclusion sometimes does seem valid.

This fallacy can lead us to poor decisions. For example, “Groundbreaking, blockbuster products are always ones that are new to the market” does not mean “Products that are new to the market are always groundbreaking blockbusters.” But, if an entrepreneur convinces us her new product is guaranteed to be successful simply because no one’s ever seen it before (relying on this fallacy), we might lose money on a poor investment.

> **The Psychological Bias Underpinning Formal Fallacies**
> 
> In _Poor Charlie’s Almanack_ , Charlie Munger credits a psychological bias called the _reason-respecting tendency_ for causing people to fall for formal fallacies. This tendency makes us seek reasons behind outcomes (such as an event, behavior, thought, and so on), but it leads us to look for, and accept, _any_ reason, even if it’s a _poor_ reason (an illogical one).
> 
> In one experiment demonstrating this tendency, people lined up at a copy machine would allow someone to cut in front of them regardless of whether that person gave a logical reason (“I’m in a rush”) or an illogical reason (“I need to make copies”).
> 
> Reasons that lead to outcomes are essentially premises leading to conclusions, and our instinct to accept faulty connections between reasons and outcomes explains our tendency to fall for both types of fallacies outlined above—we focus on the fact that premises have been presented to us, and we ignore the lack of a solid, logical connection to the conclusion they supposedly lead to. The mere existence of a premise—any premise—feels convincing enough that we don’t examine it more deeply.

#### Informal Fallacies

Unlike formal fallacies, _informal fallacies_ aren’t invalid. That is, **the conclusion logically follows from its premises. However, the _premises_ are faulty or irrelevant**.

One example is the _straw man fallacy_ , in which an argument is intentionally simplified, falsified, or otherwise misrepresented. For example, if a school principal says, “We must introduce more science into our curriculum,” someone using a straw man fallacy might retort, “So, you don’t care about the arts?” This conclusion relies on the premise that if a principal cares about math, she doesn’t care about the arts—a faulty premise.

Pinker writes that **people create informal fallacies because they like to win arguments and they’ll take shortcuts to do so** : Instead of building solid arguments based on true premises, they’ll make arguments that _sound_ logical and will hope no one examines their premises too closely.

> **Implicit Premises Drive Many Informal Fallacies**
> 
> Sometimes informal fallacies, such as the straw man fallacy outlined above, rely on _implicit premises_ —unstated assumptions that the person making an argument expects the other person to accept as true without reflection. There are always some implicit premises in any argument because people don’t spell out every single commonly accepted truth of a situation—if you say a river isn’t poisoned because the fish aren’t dead, you’re implying, but not stating, that fish would be killed by poison.
> 
> But implicit premises can be incorrect. When an implicit premise is close to a commonly accepted truth, it becomes easy to pass it off as true, and in doing so, to make an informal fallacy. In the example above, the assumption that a principal who cares about math doesn’t care about art feels close to a possible truth because very often, a person who has an affinity for either math or the arts doesn’t have an affinity for the other. This makes it easy to pass off an illogical argument as logical.

### Probability

Pinker writes that an important aspect of rationality is understanding how probability works and using it to make better decisions.

**Probability can be best understood as the chance that an event will happen given the opportunity**. When someone says there is a 50% chance of an event occurring, this means it will happen 50 out of 100 times, on average. If you flip a coin 100 times, the head-to-tail ratio may not be exactly 50-50, but there’s still a 50% chance of either heads or tails with every flip.

(Shortform note: The authors of _Superforecasting_ note that probabilities are only estimates, and predicting the occurrence of a real-world event is nearly impossible since it would require rewinding history and replaying the event multiple times to see all the different possible outcomes. Thus it’s important to keep in mind that the probabilities that most forecasters are concerned with—for example, election outcomes—are essentially their best guesses, not objective facts.)

People often miscalculate the probability of an event occurring and then make poor decisions based on that miscalculation. In this section, we’ll cover a few key reasons why this happens—how certain cognitive biases (or, _heuristics_) that we hold misdirect us, and what we can do to prevent that.

#### The Availability Heuristic

According to the availability heuristic, people judge the likelihood of an event based on how readily they remember it happening before, rather than by rationally calculating the probability based on how often the event actually happens. That is, they rely on what information is most _available_ , rather than what is most representative of the truth.

A commonly cited example is when people are more afraid of flying on an airplane than driving in a car. The probability of getting injured in a car is higher than in a plane, but people remember plane crash headlines better. They therefore falsely believe plane crashes are more probable. This false belief can be harmful: Pinker notes that by irrationally preferring to drive, many have likely driven to their deaths rather than fly on a safer aircraft.

> **Availability Is Caused by More Than Headlines**
> 
> The availability heuristic not only favors memorable ideas such as plane crashes but also favors ideas that are easily visualized as they capture our attention more readily.
> 
> The availability heuristic drives us to also believe things we can easily _visualize_ are more probable because they capture our attention more readily. In one study demonstrating this, some participants were asked to guess the chances of a massive flood happening anywhere in North America while others were asked to guess the chances of a massive flood happening in California due to an earthquake.
> 
> By definition, the chances of a flood occurring in California are smaller than in North America, since California is a small section of North America. But, participants rated the likelihood of a California flood far higher. Researchers posited that they did so because they could more readily picture a California earthquake-caused flood, being more familiar with that type of event, whereas a vague notion of a flood “anywhere” in North America didn’t leave them with a concrete mental image, and thus they underestimated its likelihood.

#### Post Hoc Probability Fallacies

Another common probability blunder Pinker discusses is the _post hoc probability fallacy_. This is when, **after something statistically unlikely occurs, people believe that because it happened, it was _likely_ to happen**. They fail to account for the number of times the event _could_ have occurred but didn’t, and the probability that, given an enormous data set (an almost infinite number of events), coincidences are going to happen.

Post hoc probability fallacies are driven by the human tendency to seek patterns and ascribe meaning to otherwise meaningless or random events. It leads to superstitious beliefs like astrology, psychic powers, and other irrational beliefs about the world. It’s why, for example, if a tragedy occurs on a Friday the 13th, some will believe it’s because that day is cursed, ignoring the many Friday-the-13th dates that have passed with no tragedy, or the many tragedies that have occurred on dates other than Friday the 13th.

(Shortform note: In _The Demon-Haunted World_ , Carl Sagan argues that supernatural and superstitious beliefs—often brought about by post hoc probability fallacies—can cause considerable societal harm because a society that holds minor irrational beliefs is more likely to hold major irrational beliefs. So, even seemingly innocuous beliefs like astrology or believing in guardian angels are potentially harmful because they lead to a more credulous and less critical society.)

#### Using Bayesian Reasoning to Counter Probability Fallacies

Pinker says that to prevent falling for fallacies like these, we can use _Bayesian reasoning_ , which is a mathematical theorem named after an eighteenth-century thinker that calculates how we can base our judgments of probability on _evidence_ (information showing the actual occurrences of an event). We won’t detail the full mathematical equations of the Bayesian theorem here, but essentially,**it helps you consider all the relevant probabilities associated with a possible outcome to determine its true likelihood** , which can sometimes differ greatly from the likelihood that _seems_ most accurate.

One common use of this theorem is in determining the probability of a medical diagnosis being correct, which Pinker says is an archetypal example where Bayesian reasoning can aid in accurate assessments of probability. Let’s say you test positive for cancer. Most people (including many medical professionals) might believe that because the test came back positive, there’s an 80-90% chance you have the disease.

However, once other relevant probabilities are taken into account, the true risk is revealed to be much less than that. In this case, if the cancer in question occurs in 1% of the population (this is the evidence of the event), and the test’s false positive rate runs at 9%, then the true likelihood that you have cancer, after receiving a positive test, is just 9%.

In everyday life, we can use Bayesian reasoning without resorting to plugging in numbers by following three general rules:

  1. **Give more credence to things that are more likely to be true.** If a child has blue lips on a summer day just after eating a blue popsicle, it’s more likely that the popsicle stained her lips than that she has a rare disease causing the discoloration.
  2. **Give more credence to things if the evidence is rare and associated closely with a particular event.** If that child with blue lips has not been eating popsicles, but instead also presents with a rash and fever, the probability that the discoloration is caused by a disease increases.
  3. **Give _less_ credence to things when evidence is common and _not_ closely associated with a particular event.** If a child _does not_ have discolored lips or any other symptoms of illness, there’s no rational reason to think she has the rare disease, _even if some people with that disease have no symptoms_. 



> **We Use Bayesian Reasoning Naturally**
> 
> In _Smarter Faster Better_ , Charles Duhigg also recommends using Bayesian reasoning to assess the likelihood of events, and he argues that we often do it naturally by subconsciously detecting patterns in the world, which helps us form assumptions about the likelihood of events happening—we unconsciously detect, for example, when the color of a child’s lips are either more or less associated with a pattern of rare disease. These assumptions then guide us when we make predictions.
> 
> Duhigg points out, however, that our predictions will only be good if our assumptions are accurate, which is why it’s important to regularly update our assumptions based on new information. In other words, it’s important to _continuously_ apply Bayesian reasoning; otherwise, we may make decisions based on faulty, outdated assumptions.)

### Correlation Versus Causation

Pinker next turns his attention to problems that people run into when considering _causation_ and _correlation_ , which drive them to make irrational decisions.

**A common mistake people make is thinking that events that are _correlated_ (they often happen at the same time) are _causing_ each other, when in fact they might be linked simply by coincidence or by a third factor.** This can lead people to make poor decisions—when they think the wrong event causes another, they incorrectly predict the future.

For example, if the stock price of a company always rises in November, a person might think the arrival of November causes the price to rise, and they might then buy stock in October in anticipation of that rise. However, if the true reason behind the price increase is that the stock rises when the company offers a huge sale on their goods, which they happen to always offer in November, then the person buying stock in October might lose money if the company decides not to offer that sale this particular November. If the person had correctly identified the causal link (between the sale and the price rise, instead of between the month and the price rise), they might have purchased their stock at a better time.

Pinker notes that it can be difficult to determine causation, especially when there are multiple events or characteristics to account for. Complicating matters is that, very often, **correlation does imply _some sort_ of causation: If two events are commonly linked, they likely have a common source** (as in the stock price example above).

> **How Our Search for Meaning Misleads Us**
> 
> In _Fooled by Randomness_ , Nassim Nicholas Taleb argues that one reason people confuse correlation with causation is that humans are wired to seek meaning, which drives us to invent meaning when we see patterns of events. This can lead us to mistake correlations that are due to either random chance or a third factor that’s not as easy to detect.
> 
> This can be a particularly easy trap to fall into if the events we’re studying are closely related in theme as well as in timing, as this often indicates a common source. For example, eating ice cream and getting sunburned are both related to hot afternoons. However, if you get a sunburn every time you eat ice cream, it’s not the ice cream that’s causing your burn—it’s the common source of both events: the hot sun.

When determining what factors cause other factors and which are merely correlated, Pinker notes that you can do one of two things:

  * Run experiments. 
  * Analyze data. 



#### Experiment to Determine Causation

**Pinker recommends running a “natural experiment” to identify which events cause other events**. To do so, you’d divide a sample population into two groups, change some characteristics in one group, and see how (or if) those changes affect the situation for that group. Such experiments are excellent ways to measure precisely how a factor might affect change and to determine which might only be correlated with other factors.

There are limits to these experiments, though. You might fail to account for variables that affect your results (if you’re studying mostly young adults, for example, you might miss how a change would affect a broader population), and there are ethical limits to how much you can change real-world elements. You can’t, for example, force two countries to go to war just so you can examine the effects on food pricing.

> **Natural, Field, and Lab Experiments**
> 
> Pinker calls these “natural experiments,” but some psychologists call this type of experiment a “field experiment,” and they have a different meaning for the term “natural” experiment.”
> 
> A field experiment is conducted by changing some aspect of a natural setting, in the way Pinker describes, but a “natural” experiment, as defined by other psychologists, doesn’t deliberately manipulate a variable, but instead tracks the effects of a change that’s already happening in a natural setting. For example, you might track how a change to minimum wage influences retail prices in one state versus another, where the wage didn’t increase. This differs from Pinker’s description of purposefully changing a variable and seeing its effects.
> 
> Psychologists also name a third type of experiment that Pinker doesn’t discuss—a _lab_ experiment, which is conducted in a controlled environment rather than in a real-world setting like a field experiment or natural experiment.
> 
> Each of these three types of experiments gives a researcher different levels of control as well as different levels of accuracy—in a lab experiment, a researcher has more control over the variables but might end up with a less accurate reflection of how a change would affect a group of people in the real world. Natural and field experiments might produce more accurate results, but they give the researcher less control over variables and changes.

#### Analyze Data to Determine Causation

When you can’t run an experiment, you can look for patterns in existing sets of data that might shed light on how one factor affects another. **Pinker mentions two factors in particular that you should analyze data for when determining causation:_chronology_ and _nuisance variables_**.

**Chronology:** You can often judge which factors affect others, and not in reverse, by noting which factors occurred first. For example, in economic data, if prices across multiple countries rise before wages rise, but wages never rise before prices rise, that indicates that price increases drive wage increases and not the reverse.

(Shortform note: Chronology can help identify causation but isn’t foolproof. In _Fooled by Randomness_ , Taleb notes the danger of analyzing data for patterns like chronology, arguing that given a large data set, you can always find some patterns if you look hard enough. He calls this “data mining,” and writes that we can ascribe meaning to such coincidences even though their true cause is chance. He cites, as an example, books that claim to show that the Bible predicted events—if you search among all events that have happened since the publication of the Bible, you’re sure to find ones that have some discernible connection to Bible verses, but the chronology of the Bible preceding these events doesn’t prove it predicted them.)

**Nuisance variables:** You can control for factors that are associated with events but don’t cause them (nuisance variables) by matching such factors in different contexts and looking at how other data changes within those matched sets. For example, if you’re examining how alcohol consumption affects longevity, you might want to consider how exercise might skew your results. You could examine two groups of people, one that drinks and one that doesn’t, and match up individuals from both groups who also exercise. Any differences in longevity would then not be due to differences in exercise but would be more closely related to alcohol consumption.

(Shortform note: Nuisance variables can create what Daniel Kahneman, Olivier Sibony, and Cass Sunstein call “noise.” In _Noise_ , they define the phenomenon as unwanted variations in events that can impair our judgments—for example, stock prices that jump higher and lower in small amounts and encourage people to buy and sell repeatedly, and that obscure the longer-term trajectory of the price. Nuisance variables, as tangential factors that introduce variation into events, can create such noise and cause people to miss important data they’d be better off paying attention to.)

### Rationality and Game Theory

Thus far, we’ve examined how people make rational decisions at an individual level. We’ll now look at how people make decisions as part of a group. This brings us to **_game theory_ , which examines how rationality is affected when the needs of an individual are pitted against the needs of others**.

We’ll first look at how game theory shows that sometimes, acting irrationally can be the most rational choice, and we’ll then examine how people can be convinced to make rational choices when they’ll only see benefits from those choices if everyone else chooses rationally as well.

(Shortform note: In _The Undercover Economist_ , Tim Harford defines a _game_ as an activity in which predicting another’s actions affects your decisions. With this definition, many situations in our lives can be considered games, like driving, where how you drive depends on how others are driving.)

#### The Zero-Sum Game

Pinker first examines how **game theory shows that sometimes, a rational person must make choices that are, on their face, irrational** , such as when opposing another person in a competition. This happens in a **_zero-sum game_ —a match-up that produces one winner and one loser **(so that the “positive” win and “negative” loss add up to a sum of “zero”). In such a contest, unpredictability has an advantage, as it prevents the other person from preparing a response.

This is why, for example, a tennis player will try to serve the ball unpredictably, even if, say, her strongest serve is to the right side of the court. If she acts “rationally” and always serves her strongest serve (to the right), her opponent will predict it and prepare to meet it. Thus, her most rational move is to act randomly and irrationally.

(Shortform note: Some posit that the advantages of unpredictability are so important to human interactions that they’ve even influenced our evolution—and it might explain why some people are left-handed. In the same way that an irrational choice brings an advantage of surprise in a zero-sum game, left-handedness brings an advantage of surprise in hand-to-hand combat, favoring the person throwing the unexpected punch. Scientists theorize that the reason left-handedness is not _more_ common, given this advantage, is that were it more common, it would no longer have the advantage of surprise. Thus it might be a somewhat self-limiting evolutionary adaptation.)

#### Volunteer’s Dilemma

A _volunteer’s dilemma_ is another situation in which a person’s best choice might be an irrational one. **In this dilemma, one person must do something dangerous to help the group as a whole**. If they’re successful, they’ll save everyone (including themselves), but if they fail, everyone will suffer—and they’ll suffer most of all.

For example, let’s say you’re marooned with a group of friends on an island and to get off the island, one of you must swim across shark-infested waters to get help. If you succeed, everyone will be saved, but if you fail, the rescue boats won’t know where the group is—and _you’ll_ be eaten by sharks. The question becomes, who will volunteer for such a task?

A volunteer’s dilemma is similar to a zero-sum game in that the incentives of the individuals are in conflict—no one wants to be the one entering the water. However, the end result of this dilemma is not zero-sum: If the volunteer succeeds, everyone wins. If the volunteer fails, everyone loses.

In such a situation, everyone’s individual rational choice is to let someone else volunteer and put themselves in danger. However, if no one volunteers, everyone loses. Thus, **in order to ultimately choose rationally so that _everyone_ has a chance of survival, someone will have to irrationally put themselves in danger**.

(Shortform note: Psychologists point to the similarities between the volunteer’s dilemma and the bystander effect, in which people are less likely to help someone when other people are present. In both the volunteer’s dilemma and the bystander effect, people are less likely to act because of a “diffusion of responsibility”: when there are more people, each individual assumes someone else will take on the responsibility to carry out the necessary action. In large groups of people, the volunteer’s dilemma can be especially dangerous because it leads to no one stepping up. If you were on the hypothetical island with a hundred other people. You might ask, “Why should I be the one to put my life on the line?”)

#### The Tragedy of the Commons

The _tragedy of the commons_ is a dynamic that applies to situations involving shared resources, where **everyone in a group has an individual incentive to take as much of that resource for themselves as possible and contribute as little as possible, which ultimately harms everyone**. For example, each fisher in a village will be incentivized to catch as many fish as they can, so that others don’t take them first. Unfortunately, if everyone is fishing aggressively, the stock is soon depleted and then no one has enough.

The same dynamic shows up in any situation where a public good is shared, be it roads, schools, or a military force—**everyone benefits from using these things, but each individual benefits _more_ if _others_ pay for them**. This dynamic also affects how the world’s environmental crisis plays out, as each individual or country is incentivized to consume energy and resources as they wish, hoping that others will curtail _their_ own use. However, those others have the same incentives to use as much as they want to, too.

Pinker writes that the most effective way to manage this dilemma is to remove the choice from individuals and instead have an outsider regulate people’s decisions—specifically, a government or organization that oversees how much each individual can take from the shared resource and establishes rules or contracts that individuals must abide by. When “free riders” are punished for taking too much or not contributing enough (through fines, for example, for failing to pay taxes), everyone is more likely to refrain from the self-benefiting behavior that can drain a public resource because they can trust that others are also refraining.

> **Scarcity and the Tragedy of the Commons**
> 
> Ironically, the tragedy of the commons can come about either because people are hoping for the best from others or because they assume the worst of others. In many of the examples Pinker mentions, like unwillingness to pay for schools, the military, and so on, or in the way both individuals and countries fail to curb their environmental waste, people are hoping for the best from others: They’re hoping that others will step up and do what needs to be done to prevent future problems. But there’s another instinct that can drive the tragedy of the commons, one that comes from assuming the worst of others: a fear of scarcity, which can motivate people to grab what they can, while they can. This is the instinct underpinning the situation of fishers taking more than they should.
> 
> An example of how this can play out was demonstrated by the efforts of the park management of the Petrified Forest in Arizona to discourage people from taking pieces of petrified wood as souvenirs. In the mid-20th century, the park posted signs warning that if theft of rocks samples continued at current rates, the forest would disappear within decades. Unfortunately, the signs had the opposite of their intended effect: They led people to believe that the rocks were scarce, making them want to grab some before they were all gone—especially since it seemed lots of other people were doing the same. Theft rates increased, rather than decreased, as people who fear a resource will be depleted by others are more likely to deplete it themselves.
> 
> In either case, whether people are hoping for the best or fearing the worst in other people, the most effective solution often is, as Pinker suggests, regulation by an outside party, as that’s the only way to establish the trust that will ameliorate either extreme.

### Why Humans Are Irrational

So, Pinker asks, **given that most people agree on the importance of rationality and its basic characteristics, why do people often act irrationally?** Why do people hold irrational beliefs—-such as paranormal phenomena or conspiracy theories?

Pinker notes that social media has allowed people to express their irrational beliefs loudly, which makes it seem that irrational thought is a growing and recent phenomenon, but, he argues that people have believed these types of ideas for millennia. Aside from all the fallacies and biases that we just covered, Pinker discusses **two additional causes of such human irrationality in the modern world:_motivated reasoning_ and _myside thinking_**.

(Shortform note: The examples of irrational beliefs that Pinker mentions are, indeed, only recent examples of a phenomenon dating back to ancient times. For thousands of years, for example, people have believed in shadowy groups controlling the world, right back to the ancient Egyptians complaining of Grecian priests who held secret knowledge and the 18th-century belief in the Order of the Illuminati. Still, despite scientific studies showing that conspiracy beliefs have not become more common in recent years, many people feel, as Pinker points out, that they _have_ increased in frequency—an example, ironically, of more irrational thinking, in this case, based on the availability heuristic: What people notice today, they assume is more likely.)

#### Motivated Reasoning

Pinker writes that rationality, by itself, is _unmotivated_. That is, a rational line of thought doesn’t desire to end in a certain place, but instead follows its logic to wherever its premises and conclusions lead it. However, sometimes a rational line of thought points to an end that the thinker doesn’t desire, like when it’s clear that the fair thing to do in a situation requires the reasoner to do something unpleasant. When this happens, a person might fall back on **_motivated reasoning_ —the use of faulty logic to arrive at a desired conclusion**.

We see people engaging in motivated reasoning when, for example, they justify purchasing an extravagant car by saying they like its fuel efficiency. In such a case, their true motivation is that they simply want the car, and they find a reason to justify that desire. We also see motivated reasoning when people choose to ignore certain facts that don’t support their worldview—like when a favored politician does something wrong. And, it’s behind many conspiracy theories, such as when someone who doesn’t _want_ to believe in climate change dismisses scientific data as manipulated despite a lack of evidence to that effect.

Pinker says that people engage in motivated reasoning so frequently, it suggests that our instinct to try to win arguments developed in tandem with our ability to reason. We’ve evolved not only to think logically but equally, to convince others of our logic, even when flawed. According to this theory, the evolutionary advantage of this instinct is that it leads to stronger _collective_ conclusions—**people are eager to pass off weak arguments of their own but are quick to point out flaws in the arguments of others, and in doing so, the group as a whole ends up at the right answer**. He points to studies that show small groups are better able to arrive at a correct answer than individuals are—as long as one group member can spot the right argument, the others are quickly convinced.

(Shortform note: Some psychologists take Pinker’s theory even further, suggesting that arguments—attempts to convince others of your point of view, even when flawed—aren’t just a byproduct of reason, but in fact, are the purpose of reason itself. In this view, reason didn’t evolve as a way to make better decisions but instead, to allow people to evaluate the arguments of others. This allowed for communication to be reliable, which in turn, made possible the development of human societies. This theory suggests, then, that when people bandy around some of the irrational ideas Pinker refers to, such as support for flawed politicians and conspiracy theories, eventually, more rational takes will win out.)

#### Myside Thinking

**_Myside thinking_ is the tendency to irrationally favor information or conclusions that support your group**. It’s largely driven by our desire to be part of a collective, and in this way, is rational. If your goal is to be respected and valued by your peers, it makes sense to express opinions and even see things in a way that earns this respect. But, the result is that you’ll likely think and behave irrationally, overlooking logical flaws of arguments that support your own side while fixating on flaws of the other side.

Pinker writes that **virtually everyone is susceptible to myside thinking** no matter their political affiliation, race, class, gender, education level, or awareness of cognitive biases and fallacies, and that **myside thinking is driving the heated political climate of recent years**. He points to studies that show liberals and conservatives will accept or refute a conclusion or scientific evidence based on whether or not it supports their predetermined notions (not based on whether or not it’s well-argued or supported). Additionally, if an invalid logical statement supports a liberal idea, a conservative will be more likely to spot the fallacy, and vice versa.

(Shortform note: Though it’s important to be aware of your myside thinking and to try to minimize it to avoid irrational thinking, scientists suggest that tribal bias is human nature and is thus impossible to eradicate entirely. Humans are tribal because we evolved as members of small groups in intense competition with each other—to survive, group loyalty was a must. Because this group dynamic has been so important to human survival, we’ve evolved to favor people who support our group’s thinking and behaviors, and thus people who align with a group’s values often have a higher social status within that group. This underpins the instinct to—sometimes blindly—support or parrot a group’s thinking.)

[[book_md/rationality/preview|preview]]

[[book_md/rationality/exercise-examine-your-myside-thinking|exercise-examine-your-myside-thinking]]

##### Welcome!

Let’s go on a quick tour of a Shortform book guide. 

Start

##### 1-Page Summary

Every guide starts with a 1-Page Summary. This is a 5-10 minute overview of the book’s key points. 

Next

##### Finished!

If you ever need to see this tour again, click here. 

Close

Guided Tour

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=28298b42-304a-497a-a0dc-857ac5f138e5&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Frationality%2F1-page-summary&r=&lt=337&evt=pageLoad&sv=1&rn=171726)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



