![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Superforecasting

Back to Discover

[[book_md/superforecasting/preview|preview]]

  * [[book_md/superforecasting|superforecasting]]
  * Full Book Guide

    * [[book_md/superforecasting/shortform-introduction|shortform-introduction]]
    * [[book_md/superforecasting/part-1|part-1]]
    * [[book_md/superforecasting/chapter-3|chapter-3]]
    * [[book_md/superforecasting/chapter-4|chapter-4]]
    * [[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]
    * [[book_md/superforecasting/part-2|part-2]]
    * [[book_md/superforecasting/exercise-answer-the-ball-and-bat-problem|exercise-answer-the-ball-and-bat-problem]]
    * [[book_md/superforecasting/exercise-generate-new-perspectives|exercise-generate-new-perspectives]]
    * [[book_md/superforecasting/chapter-6|chapter-6]]
    * [[book_md/superforecasting/exercise-embrace-probabilistic-thinking|exercise-embrace-probabilistic-thinking]]
    * [[book_md/superforecasting/chapter-7|chapter-7]]
    * [[book_md/superforecasting/chapter-8|chapter-8]]
    * [[book_md/superforecasting/exercise-develop-a-growth-mindset|exercise-develop-a-growth-mindset]]
    * [[book_md/superforecasting/chapters-9-10|chapters-9-10]]
    * [[book_md/superforecasting/exercise-identify-psychological-safety|exercise-identify-psychological-safety]]
    * [[book_md/superforecasting/chapter-11|chapter-11]]
    * [[book_md/superforecasting/exercise-weigh-the-impact-of-black-swan-events|exercise-weigh-the-impact-of-black-swan-events]]
    * [[book_md/superforecasting/chapter-12|chapter-12]]
  * [[book_md/superforecasting/highlights|highlights]]
  * [[book_md/superforecasting/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 4: The History of Superforecasting

Tetlock first discovered that some forecasters are more accurate than others thanks to a decades-long study called “Expert Political Judgment” (EPJ). As the authors describe, the results of the EPJ revealed that **overall, the average expert’s predictions were no more accurate than chance.** But a closer look at the data revealed two subgroups of forecasters: one that did no better (and sometimes much worse) than chance, and another that did slightly better.

The second group just barely surpassed the rate of chance, but even that slight edge statistically differentiated them from the first group. **Tetlock named the first group “Hedgehogs'' and the second group “Foxes,"** based on Isaiah Berlin’s classic philosophy essay entitled “The Hedgehog and the Fox” (the title comes from a line from an ancient Greek poem: “The fox knows many things but the hedgehog knows one big thing”). (Shortform note: “Knowing many things'' is similar to the psychological concept of active open-mindedness. Actively open-minded thinkers are willing to consider other people’s ideas and opinions instead of clinging to their own point of view. As we’ll see in Chapter 9, superforecasters tend to be more actively open-minded than most people.)

On average, these two groups were equally intelligent and experienced. So what distinguishes them?

#### Hedgehogs

According to the authors, forecasters in the “hedgehog” group are passionately ideological thinkers who see the world through the lens of a Big Idea. They organize new information to fit their Big Idea, and they ignore any information that doesn’t fit that paradigm. The “Big Ideas” themselves vary widely from liberal to conservative and everything in between.

The hedgehog’s preoccupation with one Big Idea biases their predictions. Because hedgehogs are so passionate, they’re more likely to make bold predictions with probabilities closer to 0% or 100% rather than stick close to a safe guess of 50%. They’re convinced that their Big Idea is “right,” and all other ideas are “wrong,” and their forecasts reflect that level of certainty. Unfortunately, Tetlock and Gardner argue that most forecasting requires a more nuanced approach, so hedgehogs’ bold predictions tend to overshoot the mark. In the EPJ project, hedgehogs did worse than foxes in both calibration and resolution. (Shortform note: In an interview, Tetlock argued that hedgehogs are more likely to embrace the intuitive, snap-judgment-based prediction style that Malcolm Gladwell describes in _Blink_. This is because their theories about the world function as predictive models, and they easily forget that the real world doesn’t always conform to any given model. For example, as Tetlock notes in the interview, if a hedgehog believes that rising global powers always come into conflict with reigning powers, they’re more likely to predict that the United States and China will go to war because that prediction fits their model.)

#### Foxes

Foxes, on the other hand, are “eclectic experts” who have a wide range of analytical tools at their disposal rather than a single Big Idea. The authors argue that this allows foxes to be more flexible, changing their approach based on the particular problem.

Foxes approach new information with a blank slate, allowing the data to shape their interpretation rather than the other way around. Because they are less clouded by bias, foxes tend to seek out information about a situation from _all_ possible sources, including those they don’t personally agree with. This allows them to consider the problem from all angles, creating a more holistic picture of the situation and reducing the likelihood that they’ll fall back on reflexive biases to inform their predictions.

> **Foxes Make Better Forecasters, Hedgehogs Make Better CEOs**
> 
> A fox mentality is not always preferable to a hedgehog mentality. In fact, in some fields, hedgehogs have a distinctive advantage. For example, while researching for the book _Good to Great, _author Jim Collins and his research team interviewed leaders of companies that vastly outperform other companies in their respective industries. Collins found that the CEOs of great companies were overwhelmingly hedgehogs, not foxes. In fact, the hedgehog mentality was so crucial to success that these companies tended to organize their entire business models around the hedgehog’s one Big Idea.
> 
> Based on what we’ve learned so far about foxes and hedgehogs, this might be somewhat counterintuitive. If foxes make better forecasters, and CEOs need to be able to predict the outcomes of their business decisions, shouldn’t CEOs strive to be foxes? Collins argues that a fox mentality considers too many variables, which ultimately distracts leaders from pursuing the company’s core mission (or, as Collins calls it, their “Hedgehog Concept”). Instead, strong business leaders should boil down the answers to three questions: What can I do better than anyone else? What is my financial engine? And what am I most passionate about? The combined answers to these questions form the core of the Hedgehog Concept, and pursuing that central concept—without getting distracted by outside factors—is what takes a company from good to great.

### The Need for Reliable Forecasts

Tetlock’s discovery of “foxes” in the EPJ had the potential to revolutionize forecasting, which would be especially useful for the intelligence community (IC). According to the authors, the IC previously resisted efforts to quantify forecasts or train forecasters in new methods. Their forecasters were professionals with impressive resumes and top security clearance—in the eyes of IC officials, interfering in their methods would amount to fixing something that wasn’t broken.

Except that it _was_ broken. An example of this is how the IC handled the 2002 proposal that the United States invade Iraq. Top officials believed the Iraqi government was stockpiling weapons of mass destruction (WMDs), and the IC was tasked with evaluating the evidence. Their report corroborated the Bush administration’s claims: The Iraqi government was producing and storing WMDs. However, when the United States invaded Iraq in 2003, they found no WMDs; the IC had made a mistake.

Tetlock and Gardner argue that **the mistake was not in the conclusion but in the level of certainty.** All the evidence pointed to the presence of WMDs in Iraq—but none of it conclusively. Somehow, experienced analysts making an incredibly high-stakes decision failed to realize they were jumping to conclusions and that the evidence _could_ be interpreted another way. If they had, they likely still would have come to the same conclusion, but certainly not “beyond a reasonable doubt." We can’t know for sure, but it’s possible that a degree of reasonable doubt would have been enough to stop Congress from authorizing the invasion.

> **Did Survivorship Bias Contribute to the Iraq War?**
> 
> Tetlock and Gardner attribute the IC’s 2003 failure to overconfidence in their own predictions. However, it’s possible that the IC also fell victim to another common cognitive shortcoming: survivorship bias. As Nassim Nicholas Taleb argues in _Fooled by Randomness_ , survivorship bias is the tendency to overvalue available evidence and undervalue missing evidence. In this case, the authors of the October 2002 National Intelligence Estimate (the same report that concluded Iraq must be stockpiling WMDs) also said, “We lack specific information on many key aspects of Iraq’s WMD programs.” While the IC analysts were aware that they didn’t have all the evidence and therefore couldn’t see the whole picture, they implicitly assumed that the missing evidence would not change their ultimate conclusion.
> 
> Additionally, when the IC analysts looked at the information they _did_ have, they concluded that Iraq having a strong WMD program was the most plausible explanation to explain the facts. It’s possible that they mistook “absence of evidence” of other viable explanations as “evidence of absence” and therefore concluded that their story must be the correct one.

#### IARPA

As the authors describe it, the magnitude of the Iraq failure rocked the IC to its core. Clearly, they needed to generate more accurate forecasts—but how? To answer that question, the IC created a research arm called the Intelligence Advanced Research Projects Activity (IARPA). But IARPA quickly hit a snag: Useful research requires data, and they had no way to measure forecaster accuracy or track their methods.

To address this, IARPA officials approached Tetlock and his partner, Barbara Mellers, for help creating a forecasting tournament that would identify superforecasters and give researchers insight into their methods. Unlike the earlier EPJ tournament, forecasters would make predictions about events months into the future, not years, since forecasting accurately more than a year out is almost impossible. (Shortform note: Tetlock and Gardner assert that the one year time limit on accurate forecasting is scientifically well-established; however, in recent years, official superforecasters have begun predicting events up to 10 years into the future.)

The IARPA tournament was designed for researchers to take advantage of the statistical tools discussed in Chapter 3. Armed with a much larger sample size, they could finally evaluate forecaster accuracy _and_ compare forecasters to each other. (Shortform note: Daniel Kahneman, author of _Thinking, Fast and Slow_ , has since implied that IARPA’s interest in forecasting was sparked, at least in part, by Tetlock’s work in _Expert Political Judgment_.)

##### Regression to the Mean

Over the first two years of the tournament, Tetlock identified more “foxes” who scored better than 98% of the group. He dubbed them “superforecasters.” These superforecasters’ skills not only held up from one year to the next; they actually _improved_. This is a surprising result because of a statistical concept called “regression to the mean” (sometimes called “reversion” to the mean), which is the idea that, **with enough trials of a task, outliers will shift toward the mean.** In other words, if someone performs extremely well or extremely poorly on a task compared to other people, their scores will probably inch toward the mean score if they try the task again.

The fact that most superforecasters’ scores _didn’t_ regress toward the mean suggests that something is skewing the data. **The authors think this happened because forecasters who did exceptionally well in year one were given the “super” designation and put on teams of other superforecasters for year two**. It’s possible that this recognition provided a sense of accomplishment that inspired forecasters to work even harder—in the second year, superforecasters raised their individual scores enough to offset the usual regression.

The authors argue that measuring regression to the mean is important for another reason: It allows us to measure how much of superforecasters’ success is due to luck and how much is due to skill. That’s because **skill-based scores regress slowly, but luck-based scores regress quickly**. After several years in the tournament, researchers determined that the scores of about 30% of superforecasters regress toward the mean each year, while the other 70% remain “super." If forecasting accuracy were purely a matter of luck, we’d expect 100% of superforecasters to regress to the mean over time; therefore, the authors conclude that superforecasting involves a fair amount of skill. (Shortform note: Tetlock has argued that, depending on the specific question, forecasting can fall anywhere between chess (where success is all about skill) and roulette (where success is all about luck) on the skill/luck continuum.)

> **Calling Forecasters “Super” Improves Their Performance**
> 
> The fact that superforecasters perform better after being given the “super” label is an example of another form of cognitive bias: the Pygmalion effect, which is the idea that having high expectations for someone can actually improve their performance. You may have noticed this yourself—when a teacher or boss expresses their belief in your skill, you might feel extra motivated to live up to that impression and work harder than you would otherwise. On top of that, their belief in you might cause your teacher or boss to provide you with more support and resources. The combination of these factors ultimately leads to better performance.
> 
> However, high expectations don’t always lead to improved performance. In fact, as psychologist Carol Dweck explains in _Mindset, _praising someone for their ability can often cause them to perform worse on subsequent tasks. The fact that superforecasters improved after being labeled “super” may actually have to do with the fact that superforecasters typically have a growth mindset, which we’ll explore in detail in Chapter 8.

[[book_md/superforecasting/chapter-3|chapter-3]]

[[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=8b30f829-58bf-40ef-9f0b-30aff65c1766&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fsuperforecasting%2Fchapter-4&r=&lt=631&evt=pageLoad&sv=1&rn=903506)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



