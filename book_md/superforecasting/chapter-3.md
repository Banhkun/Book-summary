![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Superforecasting

Back to Discover

[[book_md/superforecasting/preview|preview]]

  * [[book_md/superforecasting|superforecasting]]
  * Full Book Guide

    * [[book_md/superforecasting/shortform-introduction|shortform-introduction]]
    * [[book_md/superforecasting/part-1|part-1]]
    * [[book_md/superforecasting/chapter-3|chapter-3]]
    * [[book_md/superforecasting/chapter-4|chapter-4]]
    * [[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]
    * [[book_md/superforecasting/part-2|part-2]]
    * [[book_md/superforecasting/exercise-answer-the-ball-and-bat-problem|exercise-answer-the-ball-and-bat-problem]]
    * [[book_md/superforecasting/exercise-generate-new-perspectives|exercise-generate-new-perspectives]]
    * [[book_md/superforecasting/chapter-6|chapter-6]]
    * [[book_md/superforecasting/exercise-embrace-probabilistic-thinking|exercise-embrace-probabilistic-thinking]]
    * [[book_md/superforecasting/chapter-7|chapter-7]]
    * [[book_md/superforecasting/chapter-8|chapter-8]]
    * [[book_md/superforecasting/exercise-develop-a-growth-mindset|exercise-develop-a-growth-mindset]]
    * [[book_md/superforecasting/chapters-9-10|chapters-9-10]]
    * [[book_md/superforecasting/exercise-identify-psychological-safety|exercise-identify-psychological-safety]]
    * [[book_md/superforecasting/chapter-11|chapter-11]]
    * [[book_md/superforecasting/exercise-weigh-the-impact-of-black-swan-events|exercise-weigh-the-impact-of-black-swan-events]]
    * [[book_md/superforecasting/chapter-12|chapter-12]]
  * [[book_md/superforecasting/highlights|highlights]]
  * [[book_md/superforecasting/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 3: Measuring Forecasts

Given all the ways our brains can work against us, forecasting accurately is incredibly difficult. But determining whether a forecast is accurate in the first place presents difficulties of its own.**** According to the authors, **a forecast judged by different standards than the forecaster intended will be deemed a failure, even if it’s not**.

For example, in 2007, Steve Ballmer, then-CEO of Microsoft, claimed that there was “no chance” that Apple’s iPhone would get “any significant market share." In hindsight, this prediction looks spectacularly wrong, but the wording is too vague to truly judge. What did he mean by “significant”? And was he referring to the US market or the global market?

According to the authors, these questions matter because the answers lead us to very different conclusions. Judged against the US smartphone market (where the iPhone commands 42% of the market share), Ballmer is laughably wrong. But in the global mobile phone market (not just smartphones), that number falls to 6%—far from significant. (Shortform note: In 2009, Ballmer admitted to seriously underestimating the iPhone, in effect contradicting the authors: Even Ballmer thinks that the prediction was bad after all.)

Although Ballmer’s infamous iPhone forecast _seems_ clear at first, the authors argue that it’s actually ambiguous. Certain words can be interpreted differently by different people, and forecasts tend to be full of these words (like “significant” and “slight”). This ambiguity makes the prediction complicated to judge.

The authors note that lack of timelines is another common problem in forecasts. If someone says “the world will end tomorrow," that has a clear end date—tomorrow, if the world has not ended, we can safely say they were wrong. But if someone says “the world will end," any arguments to the contrary can be met with “just wait and see." **We can’t prove the forecaster wrong.**

> **Judging the “Worst Tech Predictions” of All Time**
> 
> Hero Labs, a technology company, compiled a list of 22 of the “worst tech predictions of all time,” including Ballmer’s infamous quip. However, unlike Ballmer’s forecast, most of the other predictions on the list _are_ specific enough to judge. Here’s why:
> 
>   * **They use unequivocal language.** For example: In 1946, Darryl Zanuck of 20th Century Fox said, “Television will never hold onto an audience.” His use of the word “never” makes it easy to judge this forecast as completely false—in 2019, the television industry was worth $243 billion (and that’s _only_ traditional network television, not including television streaming services like Netflix or Hulu).
> 
>   * **They provide a time frame.** For example: In 1998, economist Paul Krugman said, “By 2005, it will be clear that the internet’s impact on the global economy has been no greater than the fax machine.” By the end of 2005, Amazon alone was already worth over $18 billion, and as of 2021, the combined market capitalization of the largest 100 internet companies was over $7 trillion. To contrast, experts predict the global fax machine market will be worth around $733 million by 2026.
> 
> 


### Probabilities Are Useful Estimates, Not Facts

**Probability is one of the biggest obstacles to judging the accuracy of forecasts.** Calculating the probability of pulling a blue ball out of a bag is fairly easy—even if you don’t know any probability formulas, you can just keep blindly pulling a ball out of the bag, recording its color, then putting it back and repeating the process. After enough trials, it would be easy to say which color ball you’re most likely to draw and about how much more likely you are to draw that color than the other.

However, according to the authors, attaching an accurate number to the probability of a real-world event is almost impossible. To do so, we’d need to be able to rerun history over and over again, accounting for all the different possible outcomes of a given scenario. This means that for most events that forecasters are concerned with, it is impossible to know for sure that there is a specific probability of the event happening. **Therefore, any probability attached to an event in a forecast is only the forecaster’s best guess, not an objective fact.**

> **Some Probabilities Are More Accurate Than Others**
> 
> Assigning accurate numerical probabilities is easier for certain types of forecasts than others. For example, we learned earlier that history is a nonlinear system, which means that there are so many possible variables that could influence the outcome of a given event that it’s impossible to assign an accurate probability.
> 
> However, some situations have happened so many times already that it’s possible to track the outcomes and use those numbers to generate relatively accurate probabilities for future outcomes. This is the case for college admissions rates. In _Smarter Faster Better_ , author Charles Duhigg tells the story of a high school student anxiously calculating the odds of getting into college. He chose 12 schools to apply to and researched the admissions rates for each, then added those probabilities together. He realized that, while the odds of being accepted to his top-choice school were fairly low, the odds of being accepted to _any_ school were high. Therefore, while he couldn’t predict _which_ college he’d attend with any certainty, he could at least be fairly certain that he’d be in college _somewhere_ the following year, which helped to ease his anxiety.
> 
> In this case, the student can be more confident in the numerical odds he calculated because the admissions rate data he used is based on hundreds of thousands of applicants. The aggregate of all of those decisions creates a more accurate base rate to predict future decisions.

#### Estimated Probabilities Still Have Value

While the fact that estimated probabilities are only “best guesses” can be misleading, it doesn’t mean that these probabilities are useless. In fact, the authors argue that using numerical probability estimates in forecasts is critical because miscommunicating the odds in a forecast can have serious, global consequences.

That claim may sound dramatic, but it’s exactly what happened in 1961 when President Kennedy commissioned the Joint Chiefs of Staff to report on his plan to invade Cuba. The final report predicted a “fair chance” of success, and the government went ahead with what became the Bay of Pigs disaster. After the fact, it was clarified that “fair chance” meant three to one odds _against_ success, but President Kennedy interpreted the phrase more positively and acted accordingly. (Shortform note: The vague language of the report may not have been the only reason the government went ahead with such a flawed plan. In _Mindset_ , psychologist Carol Dweck argues that Kennedy’s team may have been so blinded by his charisma as a leader that they accepted his ideas uncritically.)

The authors note that in the aftermath of the failed Bay of Pigs invasion, Sherman Kent, head of forecasting for the CIA, proposed a universal standard for official forecasts that would eliminate ambiguity by assigning numerical probabilities to particular words (so “almost certain” described events to which forecasters assigned 87-99% probability, for example). His proposal was rejected outright by the intelligence community, who felt that expressing probabilities numerically was crude and misleading. They feared readers would fall into the common trap of interpreting numbers to mean something _is_ X percentage likely to happen, not that the forecaster _believes_ that to be the likelihood. As a result, intelligence reports tend to exclude any specific probabilities.

> **Vague Forecasts Lead to Misguided Decisions**
> 
> As Tetlock and Gardner argue, the intelligence community’s resistance to using numerical probabilities in forecasts stems from the fear that numbers inspire overconfidence in policymakers. However, research shows that the opposite is true: Using numerical probabilities in forecasts actually makes policymakers more cautious, not less. They’re also more likely to seek out more information before committing to a decision. Furthermore, in the Bay of Pigs example, it was a _vague_ probability that inspired overconfidence in Kennedy and his team—not a specific, numerical one.

### Brier Scores Measure Forecaster Accuracy

According to the authors, when a forecast _is_ clear enough to judge conclusively, we can use that prediction to measure how accurate a particular forecaster is in general by measuring the distance between their forecast and whether or not the event actually occurred. Over the course of several forecasts, this process yields a measure called a **Brier score**. Brier scores range between 0 and 2, where zero is an absolutely perfect forecast and two is a forecast that is wrong in every possible way. Random guessing, over time, produces a score of .5.

The authors explain that there are two components of a Brier score: **calibration** , or how accurate a forecast is, and **resolution** , or how confident the prediction was. A forecaster who always predicts probabilities near the level of chance (50%) will be fairly well-calibrated, but the information isn’t helpful—it’s the mathematical equivalent of a shrug. According to the authors, stronger forecasters are accurate outside the range of chance—they’re confident enough to assign much higher or lower odds to a particular event (such as 90% or 10%), despite the increased risk of being wrong. These forecasters will be well-calibrated _and_ have high resolution.

The authors caution that a forecaster’s Brier score is only meaningful in the context of the types of forecasts they make. For example, a forecaster who predicts the outcomes of basketball games might have a Brier score of .2, which looks quite impressive. However, basketball game outcomes are relatively easy to predict compared to other sports because the scores are much higher and players score frequently, so it’s easier to come from behind than in sports with less frequent scoring. Thus, in context, that .2 score isn’t as impressive as it looks.

The authors note that Brier scores also give us a way to compare one forecaster to another—we can say that a forecaster with an overall Brier score of .2 is a more accurate forecaster than someone with a score of .4. But context is important here, too, because **Brier scores don’t account for the difficulty of each prediction.** For example, it’s notoriously difficult to predict the outcome of baseball games because chance plays such an important role. Therefore, a forecaster who successfully predicts the outcomes of a series of baseball games is more impressive than one who successfully predicts the outcomes of a series of basketball games. Even if the baseball forecaster’s score is slightly higher (and thus less accurate), earning that score in more volatile circumstances is still more impressive than a better score for predicting basketball.

> **Brier Scores vs Net Brier Points**
> 
> While Brier scores are an effective way to measure a forecaster’s skill, both individually and compared to other forecasters, they don’t take the number of forecasts made into account. That can skew the results because Brier scores are calculated by averaging a forecaster’s accuracy across every forecast they make for a particular question.
> 
> We can illustrate this with an example. Let’s say that every day from Sunday to Friday, Judy predicts the likelihood that it will rain on Saturday (so she makes six forecasts in all). Judy’s friend Jen also predicts the likelihood that it will rain on Saturday, but she only participates on Thursday and Friday (so she makes only two forecasts). Judy’s task was much harder—she made predictions early in the week while Jen waited until the clouds gathered on Thursday and it was very obvious that it would rain that weekend. However, Jen’s Brier score is much better because her accuracy is only divided across two forecasts, not six.
> 
> To combat this, some forecasting tournaments use something called “net Brier points” rather than simple Brier scores. Net Brier points are calculated by comparing each forecaster’s score to the median score for all forecasters, then averaging that across _every day the question was open_ , not just the days that particular forecaster participated. With this system, Judy would be rewarded for making earlier, more difficult forecasts and would end up with a slightly better Brier score than Jen.

[[book_md/superforecasting/part-1|part-1]]

[[book_md/superforecasting/chapter-4|chapter-4]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=a02e816e-ab93-4723-86a1-cf0239c6ae95&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fsuperforecasting%2Fchapter-3&r=&lt=463&evt=pageLoad&sv=1&rn=296172)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



