![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Black Box Thinking

Back to Discover

[[book_md/black-box-thinking/preview|preview]]

  * [[book_md/black-box-thinking|black-box-thinking]]
  * Full Book Guide

    * [[book_md/black-box-thinking/shortform-introduction|shortform-introduction]]
    * [[book_md/black-box-thinking/part-1|part-1]]
    * [[book_md/black-box-thinking/part-2|part-2]]
    * [[book_md/black-box-thinking/exercise-reflect-on-a-recent-failure|exercise-reflect-on-a-recent-failure]]
    * [[book_md/black-box-thinking/part-3|part-3]]
    * [[book_md/black-box-thinking/exercise-practice-engaging-with-complexity|exercise-practice-engaging-with-complexity]]
    * [[book_md/black-box-thinking/part-4|part-4]]
    * [[book_md/black-box-thinking/exercise-practice-viewing-failure-as-positive|exercise-practice-viewing-failure-as-positive]]
  * [[book_md/black-box-thinking/highlights|highlights]]
  * [[book_md/black-box-thinking/community|community]]



Adding to Favorites 

Removing from Favorites 

## Part 3: Neglecting Failure Is Risky

So far, we’ve looked at how we develop through failure and examined the institutions that learn from their errors. Now, we’ll discuss the institutions that _neglect_ to learn from failure.

In contrast to the learning-oriented cultures and systems in Part 2, **failure-averse institutions have cultures and systems that prevent them from learning.**(Shortform note: While Syed described these as “closed loop” systems, we describe them as “failure-averse” institutions to reduce ambiguity. For example, a “closed loop” is often a good thing, like a task or project completed, and feedback processed.)

In Part 3, we’ll explore these two barriers—cultural and systemic—and we’ll illustrate the problem using Syed’s description of failure-aversion in the US health care system.

### Element #1: The Cultural Barrier

Syed explains that**the cultures of failure-averse institutions stigmatize failure**. In other words, they treat error as a terrible thing. In these cultures, you should feel deeply ashamed of mistakes—and you’ll be judged harshly by your peers.

This is because they’re cultures of expertise: **The employees are well-educated professionals** —such as doctors, nurses, lawyers, and judges—**who believe that experts don’t make mistakes**. As a result, any error calls your professional worth into question on two levels: the personal, and the social.

**On the personal level, mistakes threaten your self-esteem.** Say you’ve built your identity around your skill as a doctor, and then a surgery patient dies on your watch. Acknowledging this as _your failure_ could decimate your identity and burden you with guilt.

(Shortform note: In _Tribal Leadership_ , Dave Logan explains that this issue is common in a Stage 3 culture, where individuals often exhibit self-focused behaviors. Doctors, he says, often exemplify the Stage 3 “shark”—an accomplished, top-of-the-class individual who’s used to pursuing their own success. These individuals use ego-focused language (“I, me, my”), which helps to explain why they’ll cover up their mistakes: When everything revolves around him, the doctor unthinkingly puts patient concerns second.)

**On the social level, mistakes damage your reputation.** Since you’re working in a culture of expertise, any error you commit calls your competency into question. Make too large of a mistake, and you may lose all credibility overnight. For example, the above doctor might become known for his failure to save that patient in surgery.

(Shortform note: Individuals at _Tribal Leadership_ ’s Stage 3 culture also exhibit a particular relationship style that precludes collaborating to learn from failure. Dave Logan explains that they tend to view colleagues as less competent than them, and they are more interested in controlling the flow of information through one-on-one relationships. Because knowledge is power in a competitive environment, this helps them maintain their image and keep others from knowing too much about them. Unfortunately, this reputation management can come at the expense of the patient.)

Syed explains how these cultures believe that to reduce mistakes, you need to be tough on the person who committed them. In other words, proper punishment should enhance performance. But in reality, **unjust punishment causes a number of other problems, from scapegoating to cover-ups**.

#### We Instinctively Scapegoat

When something goes wrong, we instinctively scapegoat others. We find someone to throw under the bus—usually someone close to the failure—without properly examining the situation.

The problem is always more complicated than it seems. But **scapegoating oversimplifies the story and exacerbates future problems by creating a climate of fear**. In organizations, nobody wants to be the scapegoat—they could lose their reputation and their livelihood. So mistakes go unreported, unexamined, and unlearned from.

For example, the No Child Left Behind Act, an American educational policy signed into law by President George W. Bush in 2002, failed to achieve its goals on multiple counts. Casual observers might scapegoat Bush, saying that his policy forced teachers to teach to the test and crushed student creativity.

But the situation was more complicated: Policy makers tried to satisfy multiple parties—from civil rights groups to businesses to educators—and aimed to make American schools competitive in a globalizing world. But funding goals weren’t met; many states and districts ignored or partially implemented the policies; and legislative updates floundered in Congress. Knowing all this, it’s difficult to point to a single reason the act failed.

> **Cancel Culture Is Mass-Scale Scapegoating**
> 
> Cancel culture is the recent internet phenomenon wherein individuals deemed problematic by public opinion are “canceled,” or boycotted and removed from their online platforms. While some see this as a positive force for removing genuinely harmful individuals from power, others call it censorship.
> 
> In light of Syed’s description of scapegoating, we can see that cancel culture can function as large-scale scapegoating: We throw someone under the bus without examining the full complexity of the situation.
> 
> Further, well-targeted cancellations may remove genuinely harmful individuals from power, like former producer Harvey Weinstein, but they don’t address the systemic problem. Creating effective discourse is one key step toward systemic reform, and the “Rule Omega” can help prevent scapegoating in discourse: In short, any perspective contains some signal (meaningful information) and some noise (meaningless information). When you’re listening to another person speak, withhold judgment when you disagree with them and aim to find the signal—whichever point feels meaningful to you. By finding that common ground, you can empathize with one another and engage in constructive discourse, even if you mostly disagree.

#### We Oversimplify the Story

Syed explains that when scapegoating causes us to gloss over the full complexity of mistakes, the “narrative fallacy” is at work. In short, **we make sense of complex situations by telling simple, appealing stories**. Continuing the above example on No Child Left Behind, observers might assume, “Big government wanted to crush student diversity!”

**Simple stories help us feel like we understand, but they’re usually wrong**. We emphasize the major “story beats,” like the signing of the act and the negative impact of standardized testing, but we discount the details. Because of this, we fail to grapple with the full complexity of the failure.

For example, it’s much easier to say “No Child Left Behind was callous and poorly thought-out,” than to confront the complexity of the situation. Maybe law makers implemented ineffective policies but genuinely meant well. Maybe they acted on the best pedagogical knowledge at the time, with as much money as they could secure, and yet couldn’t get states to follow the policies faithfully enough for them to work.

(Shortform note: In _Antifragile_ , Nassim Nicholas Taleb explains that thought leaders often engage (knowingly or unknowingly) in “narrative discipline,” or the process of creating stories that sound great and seem to fit the data, but which are false or incomplete. This involves cherry-picking data from statistical studies to fit the narrative, since statistics can seem objective and convincing without contextualization. To avoid this, Taleb suggests running controlled experiments, which can’t be cherry-picked as easily.)

#### We Commit Cognitive Distortions

When a story turns out to be more complex than we thought, cognitive dissonance—unconscious mental distortion of the facts—comes into play.

As Syed explains, nobody likes to be wrong—it’s a threat to our egos. So to defend our versions of events, our brains distort the information to conform to our beliefs. And the more invested we are, the worse the distortions.

(Shortform note: The notion of cognitive dissonance originates from a 1957 theory described by Leon Festinger, who claimed that we have an innate drive to maintain harmony between our attitudes (thoughts, beliefs) and behaviors. When we face any decision, every option has pros and cons, so we’ll always lose out on something—for example, accepting a job with great pay but a boring office in an average town. This causes dissonance, which we tend to overcome by rationalizing: “ _It isn’t so bad, really._ ” While this may seem _irrational_ , it creates cognitive consistency, which is rational in that it reduces the anxiety of dissonance and helps you get on with your life.)

Syed explains that we’re all prone to three types of cognitive dissonance. We’ll illustrate each type with examples of real-world incidents surrounding Chogyam Trungpa Rinpoche, the spiritual leader of Shambhala International (a Buddhist organization) who faced controversy related to substance use and sexual abuses:

  1. **We reframe the evidence.** In other words, we accept a piece of evidence but tell a story to conform it to our beliefs. For example, Rinpoche’s accusers cited stories of sexual abuse from ex-devotees, but devout followers answered that his sexual abuses were spiritual lessons, and his victims just weren’t ready to learn.
  2. **We invent justifications.** Here, we accept the evidence but find some way to excuse it. So a devout follower might justify Rinpoche’s “seven spiritual wives” by saying that he earned it with his spiritual dedication. 
  3. **We ignore the facts outright.** Sometimes, cognitive dissonance causes point-blank denial of the facts. After Rinpoche died of liver poisoning from his drinking problem, followers propped his body into a sitting meditation position and proclaimed that he’d reached _parinirvana_ (enlightenment after death), as if his failings had never happened. 



> **How to Combat Cognitive Dissonance**
> 
> Though cognitive dissonance often occurs automatically, we can learn to recognize it. According to psychologists, there are various signs of cognitive dissonance:
> 
>   * Feeling discomfort prior to making an important decision may indicate conflicting beliefs, a form of dissonance.
> 
>   * Excessive justification or qualification of a decision can indicate that you know it wasn’t quite right.
> 
>   * Doing something while hiding it from others, like smoking in a private place, indicates dissonance between your desire to improve and your existing habit.
> 
> 

> 
> The best way to resolve cognitive dissonance is to honestly examine and adjust your beliefs. While this is difficult, our only other options are to rationalize or ignore the conflicting information, which doesn’t help. To examine your beliefs, try using mindfulness meditation to develop space between your reception and response to new information. This allows you to observe and change your actions, instead of simply reacting from habit.

### Element #2: The Systemic Barrier

In addition to the above cultural barriers, **failure-averse organizations also lack systems that enable them to learn from failure.** Unlike learning-oriented organizations, **they don’t gather data, conduct investigations, analyze what went wrong, or implement changes.** Often, they don’t even record their mistakes.

#### No Feedback Means No Improvement

In short, **these organizations lack a learning mechanism**. Specifically, Syed explains that they lack a feedback loop that enables information to flow from _failure_ to _analysis_ to _adjustment_ to _implementation_.

An individual or organization can’t improve without feedback, since feedback tells you what’s going wrong. **If you don’t know where things aren’t working, you can’t improve**.

For example, raising children involves ambiguous feedback that makes it difficult to know how you’re doing. Since there’s no clear and obvious benchmark for what a “good” job raising a child is, and there’s no clear and obvious feedback, it’s difficult to improve on a measurable scale.

(Shortform note: In _The Art of Learning_ , Josh Waitzkin advocates for a particular kind of feedback: examining the theme of your errors. In short, gather data by noting the mistakes you repeatedly make. Then, examine your notes and identify any patterns. Look for both the _emotional_ and _technical_ aspects of your error, and work to correct them. For example, a parent who gets impatient with their toddler might work to change their mindset while also studying better childcare techniques. To maintain the feedback loop, continue to note and correct your patterns of error.)

By contrast, if you’re learning piano, it’s obvious when you play the wrong notes. **With clear and immediate feedback, you can effectively learn**.

In professions that lack readily available feedback, professionals do their best with the skills they’ve developed, but they often fail to improve over time. Emergency surgeons, for example, don’t gather specific data about how _well_ they set a broken bone, implanted metal pins, or whether they make the right choices under pressure. To do so would require setting up mechanisms, like regular patient check-ins, to gather feedback over the long term.

(Shortform note: In _The Bullet Journal Method_ , Ryder Carroll suggests using a journal to conduct daily reviews. In your morning review, look back over previous journal entries to get a sense for where you were and where you are now. In the nightly review, look over the day’s tasks and ideas to determine how well you’ve done. This tangible feedback gives you direct information about how to improve—for example, you might learn that setting 11 tasks is too many, so you adjust your to-do list until you learn what you can handle.)

#### No Feedback Means the System Stagnates

Syed argues that the absence of a feedback mechanism is a systemic problem, not a cultural one. The employees are intelligent, capable professionals, but **workers can only perform as well as the system enables them to**. If an organization’s systems aren’t designed to learn from failures, they simply won’t get better—and neither will the employees.

Imagine a hospital that neglects to optimize its triage system for the emergency room’s busiest night. They know they’ll be overburdened, yet make no adjustments, allowing employees to continue struggling.

Compare this to a hospital that had faced the same problem, but took the opportunity to learn and implement changes. By learning from their overburdened ER, they remove the chance to repeat past mistakes. For example, say the problem was a disorganized waiting space. To fix this, the hospital restructures seating areas for each type of patient emergency.

Now staff can find patients at a glance—the system itself ensures better performance. Put another way, **you can’t make mistakes that the system has already corrected for.**

**(** Shortform note:**** In his 3-2-1 newsletter, James Clear explains that we don’t “rise to the level of our goals,” we “fall to the level of our systems.” He defines your personal system as the network of habits you’ve built, and we can extend this to organizations: An organization’s system is the network of habits and habit-enabling tools (like error reporting mechanisms) that all its people engage in. If part of the system is to report errors and openly admit fault, any new employees will default to that behavior, “falling” to the level of the system they’ve stepped into.)

#### Believing in False Dichotomies Prevents Change

Syed argues that failure-averse institutions believe in a false dichotomy that prevents improvement: either we do A and lose out on B, or do B and lose out on A. But in fact, it’s often possible to improve both:

  * In health care, it’s commonly believed that more processes (like error reporting) means more administrative spending, so you can either save money _or_ reduce mistakes. But hospitals that implemented error reporting mechanisms actually saved money _and_ reduced mistakes, because fewer errors meant safer patients, thus lower insurance/liability premiums. 
  * In the criminal justice system, professionals argue that wrongful convictions guarantee a strong enough system to catch every actual criminal and if you weaken the burden of evidence, you risk letting wrongdoers get away. But Syed argues that we can reduce wrongful convictions _and_ strengthen the system—we just need to find the flaws that cause wrongful convictions, like unreliable forensic techniques, and improve them _without_ weakening the demands of evidence.



(Shortform note: The above situations demonstrate the false dilemma fallacy, wherein reasoning from an either-or mindset obscures other possibilities. But there are nearly always other possibilities. In contrast to either-or thinking, look for opportunities to see when _both-and_ is true. For example, rationality and empiricism each have unique strengths, so it’s better to think “both-and,” rather than fight for one or the other.)

### Case Study: Failure Aversion in the Health Care System

Syed argues that in the health care system, hospitals have numerous opportunities to learn from failure but often disregard them. This is because they operate according to top-down systems that go unquestioned, and they have cultures that stigmatize failure.

#### Problem #1: Health Care Cultures Avoid Acknowledging Mistakes

Doctors, nurses, and other health care professionals train for years, so they expect perfection from themselves and their peers. **Mistakes carry a strong stigma—if you mess up as a surgeon or nurse, you’ll be looked down upon—and the more senior your role, the stronger the blame.**

Because of this, Syed argues, many health care workers fear reporting mistakes. Reporting their own errors leads to consequences, and reporting their superiors’ might provoke retaliation. As we explained above, cognitive dissonance compounds the problem: A doctor can unwittingly suppress his memory of a failure, justify the mistakes, or outright deny that it happened. By doing so, he preserves his reputation at the expense of continued patient harm.

(Shortform note: In a January 2022 article, emergency room nurse Sally Ersun details her gut-wrenching last day on the job, highlighting the systemic issues Syed refers to. Understaffed and overburdened, her department had too few supplies and could not provide blood for a dying man; another patient was left unattended and fell and hit her head—and there was no time to report the incident. Ersun explains that she’s been “threatened” by superiors, and ultimately quit due to physical and emotional exhaustion. A nurse for 10 years, she argues that health care’s for-profit model “prioritizes finances over lives,” which stoutly corroborates Syed’s analysis of health care.)

#### Problem #2: Health Care Systems Don’t Analyze Failures

Many hospitals also lack systems for reporting, investigating, and improving upon errors. Syed cites a report showing that fewer than 20 US states require error reporting mechanisms in hospitals. Of those 20, few consistently investigate errors and enforce changes. Another study found that of 273 hospitalizations, hospitals “missed or ignored” 93% of preventable errors.

In health care, investigating errors simply isn’t the convention. Because of this, numerous mistakes—including hundreds of thousands of preventable deaths annually—go unexamined. Many deaths from surgery, medication error, and neglect are written off as “inevitable” or “one-off” tragedies.

(Shortform note: One key to effective investigations is to work with an independent investigator. In 2016, a National Health Service investigation demonstrated the need for this, showing that hospitals consistently treated family members of the deceased with little courtesy and often ignored or blocked their requests for information. The Care Quality Commission (CQC), a care watchdog based in England, is attempting to establish an independent review process to enforce accountability. Their first goal is to secure better treatment for the families, which the CQC determined hospital workers view as “antagonistic.”)

**Since they don’t learn from their mistakes, hospitals also lack “institutional memory,” a shared compendium of lessons learned.** Without this, the few lessons learned take years, even decades, to percolate through the broader health care system.

The infrequency of autopsies exemplifies the problem, according to Syed:

  * Autopsies are health care’s version of the black boxes used in aviation: They enable doctors to look clearly at the precise, objective details of what went wrong. 
  * Close to 80% of families give permission to perform the autopsy. Despite this, almost none are performed—fewer than 10%. 
  * Each autopsy is an opportunity for doctors to learn what went wrong and improve their processes. So each time they’re passed up, potentially life-saving learnings go down the drain.
  * Syed argues that this happens because of health care’s culture: The doctor fears the shame of knowing his failure, so he avoids ordering autopsies. This preserves his self-image and his reputation as a consummate professional, at the cost of important insights that could improve his systems.



(Shortform note: As recently as the 1950s, US hospitals performed autopsies on around 50% of all deaths. Virtually all medical experts agree that autopsies are invaluable for determining the cause of a death, yet they’re expensive, time-consuming, and must be performed at the hospital’s expense. Modern physicians argue that advanced imaging technologies and budgetary concerns render autopsies unnecessary, though one study of autopsies performed to confirm clinical diagnoses found a median error rate of 23.5%, showing that they remain relevant for determining cause of death and for learning from what happened.)

[[book_md/black-box-thinking/exercise-reflect-on-a-recent-failure|exercise-reflect-on-a-recent-failure]]

[[book_md/black-box-thinking/exercise-practice-engaging-with-complexity|exercise-practice-engaging-with-complexity]]

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__




![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=2f805a3c-9372-4bc1-bb6c-dfb727b0b71e&sid=201ffde0635411ee902411d77b750559&vid=20202bf0635411ee9ac03f2e618b0b9f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Black%20Box%20Thinking&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fblack-box-thinking%2Fpart-3&r=&lt=442&evt=pageLoad&sv=1&rn=63284)
