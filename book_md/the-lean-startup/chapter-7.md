![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# The Lean Startup

Back to Discover

[[book_md/the-lean-startup/preview|preview]]

  * [[book_md/the-lean-startup|the-lean-startup]]
  * Full Book Guide

    * [[book_md/the-lean-startup/introduction|introduction]]
    * [[book_md/the-lean-startup/part-1|part-1]]
    * [[book_md/the-lean-startup/chapter-2|chapter-2]]
    * [[book_md/the-lean-startup/chapter-3|chapter-3]]
    * [[book_md/the-lean-startup/chapter-4|chapter-4]]
    * [[book_md/the-lean-startup/exercise-your-startup-beginning|exercise-your-startup-beginning]]
    * [[book_md/the-lean-startup/part-2|part-2]]
    * [[book_md/the-lean-startup/chapter-6|chapter-6]]
    * [[book_md/the-lean-startup/exercise-design-your-mvp|exercise-design-your-mvp]]
    * [[book_md/the-lean-startup/chapter-7|chapter-7]]
    * [[book_md/the-lean-startup/exercise-choose-the-right-metrics|exercise-choose-the-right-metrics]]
    * [[book_md/the-lean-startup/chapter-8|chapter-8]]
    * [[book_md/the-lean-startup/part-3|part-3]]
    * [[book_md/the-lean-startup/chapter-10|chapter-10]]
    * [[book_md/the-lean-startup/chapter-11|chapter-11]]
    * [[book_md/the-lean-startup/exercise-try-the-five-whys|exercise-try-the-five-whys]]
    * [[book_md/the-lean-startup/chapter-12|chapter-12]]
    * [[book_md/the-lean-startup/chapter-13|chapter-13]]
  * [[book_md/the-lean-startup/highlights|highlights]]
  * [[book_md/the-lean-startup/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 7: Measure

The job of a startup is to: 

  1. Build an MVP to measure where it is now 
  2. Experiment to improve the metrics 
  3. Decide whether to continue in the same direction or pivot in a new direction. 



Defining the right metrics that actually matter to your business is critical. Before giving examples of good metrics, we’ll define common bad metrics startups choose. 

###  Avoiding Vanity Metrics 

The most insidious kind of metrics, vanity metrics, give you false optimism – it _seems_ like you’re making great progress, but in reality you’re actually stuck. 

Often, **vanity metrics are metrics that have no choice but to keep increasing over time** . One common example is total user count. Let’s say your app adds 1,000 users every week. At the end of 10 weeks of hard work, you have 10,000 users. This is a big number! 

Except you aren’t growing any faster – you’re still adding 1,000 users every week. Your hard work actually didn’t change your _growth rate_ . 

These vanity metrics are misleading because they keep increasing over time, making you feel good. To give an obviously silly example, think about a metric like number of hours worked. Every day, you and your team each add 10 hours to the total count. At the end of the month, you proudly announce to the team, “this month, we added 1,250 hours to our Total Hours Worked metric. This is great progress. Good job, team!” 

This may sound silly, but think about how many founders you’ve heard brag about how many hours they work. What really matters are the _results_ they achieve. 

Another type of vanity metric **tracks results that aren’t actually critical to business function** . For instance, early startups worry about racking up press mentions or making new hires. While these may help the business move, they’re not the actual core of the business – your company doesn’t exist to hire employees, it exists to create value for customers and create profits for shareholders. 

These common vanity metrics are all problematic: 

  * Total number of anything – users, sales, actions in product 
  * Money raised from investors 
  * Press articles written about your company 
  * Number of employees hired 
  * Number of features added to product 
  * Meetings scheduled 
  * Emails written 



###  Cohort Analysis 

Instead of looking at cumulative vanity metrics over time, **the more accurate analysis is to _separate_ users into groups based on the time they joined, then measure your metric for each group independently ** . Each group of users is called a **cohort** . 

For example, say we wanted to measure engagement in an app by number of photos sent. Each week, we take all the users who joined that week, and then look at the average number of photos each user sends in their first day. We work really hard for 4 weeks, and we hope to see this number rise. Instead we see this: 

|  Number of photos sent per user  |  Vanity metric – total photos sent   
---|---|---  
Week 1  |  5  |  100   
Week 2  |  5  |  200   
Week 3  |  5  |  350   
Week 4  |  5  |  500   
  
Notice how despite our hard work, the number of photos each new user sends has not risen – we haven’t made any real progress per cohort. However, the vanity metric on the right – total photos – has risen, due to the cumulative actions of all users. 

In contrast, here’s what progress would look like: 

|  Number of photos sent per user  |  Vanity metric – total photos sent   
---|---|---  
Week 1  |  5  |  100   
Week 2  |  6  |  250   
Week 3  |  8  |  400   
Week 4  |  10  |  600   
  
The key metric of photos sent per user is rising across cohorts – hard work is paying off and actually improving the metric! (Improvement in this metric also improves the vanity metric of total photos sent, since users are sending more photos.) 

###  Choosing Good Metrics 

**You need to define metrics that really matter to your business, and measure improvements to that metric.**

The metrics that really matter vary from business to business. Often, they reflect your Value Hypothesis and your Growth Hypothesis. 

For example, an app might aim for these metrics: 

**Value Hypothesis** : Each new user will upload an average of 10 photos in the first week of using the app. 

**Growth Hypothesis** : Every user who joins the app will refer 10 new visitors, 1 of whom will join as a new user (leading to virality). 

The important part is that you need to believe these numbers are vital to the success of your business. Eric Ries suggests **choosing the riskiest assumptions first** – the metrics that you have the least confidence in, yet have the most impact to your business. For example, if your company is going to be supported by advertising, advertising rates are probably not the riskiest assumption – getting user engagement will be. 

Here are examples of good metrics that reveal the health of your business (only some will apply to your startup): 

  * Engagement 
    * Time in product per user per week 
    * % of users who return in the first day / first week / first month 
  * Growth 
    * Viral factor 
    * Net Promoter Score 
    * Conversion rate of each major step – visiting to signup to payment 
    * New users gained per week 
  * Finances 
    * Customer acquisition cost 
    * Lifetime value per user 



Unlike vanity metrics, these metrics don’t improve unconditionally over time – if you don’t put in work, it’s unlikely that cohort metrics like revenue per user or engagement will increase. 

###  A/B Testing 

(aka Split Testing) 

Let’s say you develop a new feature to your product and you release it to all your users. Suddenly your metrics improve. But how do you know seasonal effects aren’t at play – that the users who joined later aren’t just naturally more engaged? Or that you got a burst of users from an unexpected news article? 

**An A/B test avoids bias by splitting users into seeing two different versions of your product.** By analyzing the metric resulting from both groups, you get strong quantitative evidence about which version users like more. 

For example, let’s say you have a landing page MVP listing your potential features and a signup form. You’re not sure which of two features your users will like more. So you set up an A/B test – half of visitors see feature A on your landing page; the other half sees feature B. You measure the difference in signup rate. If feature A gets a 5% signup rate, but feature B gets 2%, this is evidence that your users may prefer feature A! 

**A/B testing should be applied at critical stages of your progress.** You can test variations in your marketing pages, different signup and payment processes, different product interfaces, even turn features on and off in your product. 

One major benefit to A/B testing is **reducing time confounds** . A common way to test is to expose all users in one weak to page A, then users in the next week to page B. The issue with this is the users might not be similar - they could have come from different sources, or they might behave differently in that particular week for some reason (e.g. they’re on spring break). In A/B testing, since the same group of users are randomly assigned to two groups at once, you don’t need to worry that an earlier group of users is different from a later group. 

Another benefit of A/B testing is that **it lowers politics** . You don’t have to squabble with your team over which features are better – you can put it to the test with an MVP. A/B testing also lets you **assign credit where it’s due** . If you launch a bunch of marketing and product changes at once, and your metrics improve, who’s responsible? Without independent experiments, it’s difficult to tell. But by running separate A/B tests, you would be able to see that marketing’s redesigned pages caused the signup boost, not the new product feature. 

Finally, A/B testing lets you gauge the real effect your work is having on users. The product team may obsess over features that users absolutely don’t care about. A/B testing gets them to focus on results that actually make a difference. 

###  Useful Reports 

To make A/B testing, you need to have discipline around analyzing your experiments and planning the next step. Often this means producing reports, which should fit 3 A’s: 

**Actionable** : correctly designed experiments will show a clear cause and effect, and the right metrics will show whether you’re really making progress. From here, you can iterate through the Built-Measure-Learn loop. 

**Accessible** : simplify the metrics and help people understand what they mean and why they’re important. Consider making metrics publicly viewable by any member of your team. 

**Auditable** : people should be able to dig into the raw data and trace how the metrics are compiled. If someone doesn’t like the result of an experiment, she may be tempted to question technicalities of the data. You need to be able to prove that the analysis is faultless. 

[[book_md/the-lean-startup/exercise-design-your-mvp|exercise-design-your-mvp]]

[[book_md/the-lean-startup/exercise-choose-the-right-metrics|exercise-choose-the-right-metrics]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=9e461b93-9b50-47bd-beab-418d3036e264&sid=1711133063fa11eebdec89a8b8ae3bbc&vid=171147a063fa11eea7440fcfeb230d96&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fthe-lean-startup%2Fchapter-7&r=&lt=480&evt=pageLoad&sv=1&rn=971967)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



