![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# The Design of Everyday Things

Back to Discover

[[book_md/the-design-of-everyday-things/preview|preview]]

  * [[book_md/the-design-of-everyday-things|the-design-of-everyday-things]]
  * Full Book Guide

    * [[book_md/the-design-of-everyday-things/introduction|introduction]]
    * [[book_md/the-design-of-everyday-things/chapter-1|chapter-1]]
    * [[book_md/the-design-of-everyday-things/exercise-reexamine-everyday-objects|exercise-reexamine-everyday-objects]]
    * [[book_md/the-design-of-everyday-things/chapter-2-1|chapter-2-1]]
    * [[book_md/the-design-of-everyday-things/exercise-break-down-an-ordinary-action|exercise-break-down-an-ordinary-action]]
    * [[book_md/the-design-of-everyday-things/chapter-2-2|chapter-2-2]]
    * [[book_md/the-design-of-everyday-things/exercise-do-a-root-cause-analysis|exercise-do-a-root-cause-analysis]]
    * [[book_md/the-design-of-everyday-things/chapter-3-1|chapter-3-1]]
    * [[book_md/the-design-of-everyday-things/chapter-3-2|chapter-3-2]]
    * [[book_md/the-design-of-everyday-things/exercise-redesign-reminders|exercise-redesign-reminders]]
    * [[book_md/the-design-of-everyday-things/chapter-4|chapter-4]]
    * [[book_md/the-design-of-everyday-things/exercise-identify-physical-constraints|exercise-identify-physical-constraints]]
    * [[book_md/the-design-of-everyday-things/chapter-5|chapter-5]]
    * [[book_md/the-design-of-everyday-things/exercise-identify-system-errors|exercise-identify-system-errors]]
    * [[book_md/the-design-of-everyday-things/chapter-6|chapter-6]]
    * [[book_md/the-design-of-everyday-things/exercise-observe-the-right-users-in-the-right-settings|exercise-observe-the-right-users-in-the-right-settings]]
    * [[book_md/the-design-of-everyday-things/chapter-7|chapter-7]]
    * [[book_md/the-design-of-everyday-things/exercise-does-technology-make-us-smart|exercise-does-technology-make-us-smart]]
    * [[book_md/the-design-of-everyday-things/exercise-consider-consumer-values|exercise-consider-consumer-values]]
    * [[book_md/the-design-of-everyday-things/exercise-reflect-on-the-design-of-everyday-things|exercise-reflect-on-the-design-of-everyday-things]]
  * [[book_md/the-design-of-everyday-things/highlights|highlights]]
  * [[book_md/the-design-of-everyday-things/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 5: Human Error

In this chapter, we’ll break down different types of errors that can happen when humans interact with technology. These errors can take the form of either “slips” or “mistakes," each of which can be broken down further into different categories. The first edition of this book, published in 1988, included many more categories of slips and mistakes, but here they have been pared down to only those most relevant to design. The chapter ends with recommendations for turning knowledge of human error into specific design guidelines.

### The Error of “Human Error”

Industry professionals estimate that between 75 and 95 percent of industrial accidents are attributed to human error. But if we think of “error” as something that goes wrong in a particular system, how is it possible for the vast majority of events in that system to be considered “errors”? Error, by definition, should be the exception rather than the rule. **In other words, what we think of as human errors are more likely outcomes of a system that has been unintentionally designed to _create_ error, rather than prevent it. **

If there’s an underlying cause of these accidents, why do we write them off as human error? One reason is that people analyzing the incident tend to stop as soon as they find someone to blame. This offers a quick and convenient “solution”: identify who made the mistake, punish them for it, and move on. But when errors stem from underlying design issues, punishment doesn’t prevent the same thing from happening again.

### How Do We Define Error?

What, specifically, counts as an error? The term “error” applies to any action that differs from the general understanding of appropriate behavior. In this context, “error” is not the same as “accident”: an error is an incorrect behavior that may or may not lead to an accident (an event that causes harm). Errors can be classified as either “slips” or “mistakes."

  * **A “slip” is an error of execution.** Slips happen when we have the right goal for an action, but end up performing a different action without thinking (like accidentally putting a chopstick in your drink instead of a straw). Slips happen unconsciously—they are errors of _doing_. 
  * **A “mistake” is an error of evaluation** that happens when we execute an action correctly, but our goal, plan, or understanding of the situation is wrong. Mistakes happen at the conscious level—they are errors of _thinking_. 



**The defining difference between slips and mistakes is that slips happen subconsciously while mistakes involve conscious choices.** Slips and mistakes can be further broken down into subtypes. Mistakes can be broken down into knowledge-based, rule-based, and memory-lapse mistakes. Slips can be classified as either memory-lapse or action-based. Action-based slips can then be broken down further into three types. Each of these subcategories will be defined in the following sections.

![the-design-of-everyday-things-15.png](https://media.shortform.com/images/the-design-of-everyday-things-15.png)

#### Slips

Most everyday errors are slips, not mistakes. Remember, the execution phase of action is mostly subconscious. We decide what action we want to take, and we see the results, but the actual process between thinking and doing is below our level of awareness. **Slips happen during that subconscious transition from thinking to doing.**

The fact that slips happen subconsciously and look different for different people leaves a lot open to interpretation. Sigmund Freud proposed that slips were brief glimpses into someone’s subconscious mind. (Shortform note: This is where we get the phrase “Freudian slip.") Most modern psychologists agree that slips are more formulaic and unlikely to reveal hidden secrets.

**Counterintuitively, slips happen _more_ frequently to experts than beginners**. Beginners use conscious thought to work through every step of a task, so the chance for subconscious slips is low. But experts have overlearned the same task after years of practice, so the process no longer requires conscious attention and is executed subconsciously.

There are two classes of slips: memory-lapse and action-based.

  * **Memory-lapse slips** involve simple forgetting, like forgetting your phone at home or driving away with your coffee cup still resting on top of the car. 
  * **Action-based slips** involve simple human blunders, like pouring cream into your coffee and then putting the coffee cup in the refrigerator rather than the cream. The goal of putting cream back into the refrigerator was correct, but the action was mistakenly applied to the coffee instead. 



##### Action-Based Slips

Action-based slips can be broken down even further into subtypes, including capture slips, description-similarity slips, and mode errors.

  * **Capture slips** happen when something we intend to do overlaps with something we frequently do (or did recently), and the overlap makes us accidentally switch to the more familiar task. This is what happens when we are “on autopilot." If you’re driving to a friend’s house, but the first half of the drive is identical to the route you take to work every day, you’re likely to drive right past the turn for your friend’s house. The familiar activity (driving to work) has _captured_ the new activity (driving to a friend’s house). 
    * The opportunity for the familiar activity to capture the new activity happens when we forget that we’re supposed to be focusing on the new activity. The element of forgetting means that capture slips can also be classified as memory-lapse errors.
  * **Description-similarity slips** happen when two objects are so similar that we mistakenly perform the right action on the wrong object. If there are two identical switches mounted above your kitchen sink, accidentally hitting the switch for the garbage disposal instead of the kitchen light is a description-similarity slip. 
  * **Mode errors** happen when there is one control for multiple functions, such as a universal remote. If you want to turn the TV volume up, but the remote is set to “auxiliary mode," you may accidentally turn up the volume on a different device. 
    * Mode errors can also have much more serious consequences, as in the case of an airplane crash caused by a mode error involving the auto-pilot system. The same instrument was used to control both vertical speed and angle of descent, so toggling between the two variables required switching modes. In this case, the pilot entered the correct value for angle of descent, but he did not realize the equipment was still set to the vertical speed mode. 



#### Mistakes

Mistakes are the result of conscious choices based on faulty information, misinterpretation, or simple forgetting. Mistakes can be classified as knowledge-based, rule-based, or memory-lapse mistakes (not to be confused with memory-lapse slips).

  * **Knowledge-based mistakes** happen when we have incomplete or incorrect information to work with, leading us to misunderstand the situation and therefore respond inappropriately. Knowledge-based mistakes usually happen in novel situations where we have no applicable prior knowledge to pull from. 
  * **Rule-based mistakes** , on the other hand, happen in familiar contexts where we’re aware of the “rules” of how to act. These mistakes happen in three ways: misinterpreting the situation and consequently selecting the wrong rule; selecting the correct rule, but the rule itself creates problems; or selecting the right rule, but misinterpreting the results. 
    * The common tendency to set thermostats to extreme temperature in the hopes that it will heat or cool the room faster is an example of the first type of rule-based mistake, based on a faulty conceptual model of the heating system.
  * **Memory-lapse mistakes** are different from memory-lapse slips. In memory-lapse mistakes, the lapse in memory happens during the goal, plan, or evaluation phase, not during execution. For example, if you get distracted while searching for something in various boxes, you might forget which boxes you’ve already checked and end up mistakenly rechecking some of them. 



### What Causes Errors?

The next step in addressing system errors is to determine the underlying cause. Broadly speaking, there are two main causes of error: system designs that fail to account for basic human traits, and the social environment of the users in those systems.

#### Systems Built for Perfect Humans

Every person is different, but we all have nearly identical underlying needs and tendencies. We don’t focus well when we’re tired, hungry, bored, or upset; we can only remember a small amount of new information at a time without a chance to absorb it; we struggle to focus intently for long stretches of time. These are normal human traits, but when something goes wrong, they’re quickly reclassified as “errors." **In order to truly prevent dangerous accidents, we need machines and systems that not only function properly, but are designed in a way that accounts for human nature.**

All of these qualities come into play in cases of interruption. **Interruption is a major cause of error.** As a rule, humans struggle to efficiently pivot between different types of tasks. Most of us have experienced being interrupted while working intently, only to turn back to the task and think “what was I just doing?” Important information can be lost this way, especially when systems are designed for continuous focus (automatic session timeouts on certain website pages are an example of this). **Multitasking is also a form of interruption,** since we are asking the brain to quickly switch focus between multiple tasks.

Like all causes of error, interruptions and multitasking are especially dangerous in high-risk environments like medicine and aviation. To prevent interruption, the Federal Aviation Authority (FAA) requires a “Sterile Cockpit Configuration” during take-off and landing, meaning pilots are prohibited from discussing anything other than the task at hand.

Even systems that are designed for humans and machines to work in tandem can have problems. **Norman calls this “the paradox of automation," where technology can easily take over tasks that are simple for humans but fails on complex tasks when humans need it most.** We learn to rely on technology to the point that we no longer monitor the situation, so when the system fails, it does so with no warning and often with major consequences.

#### The Social Context of Error

The social environment plays a massive role in understanding error. In corporate environments, economic pressure is often the culprit. **The larger the system, the more expensive it is to shut down even temporarily to investigate and fix errors.** The pressure to keep things running as usual is called _time stress_ , which is a major cause of accidents.

Social and economic pressures played a critical role in the Tenerife airport disaster, a 1977 crash in the Canary Islands that remains the deadliest accident in aviation history. The accident involved two planes: one taking off before receiving clearance, the other taxiing down the runway at the wrong time due to miscommunication with air traffic control. The first plane had been rerouted and delayed, and the captain decided to take off early to get ahead of a heavy fog rolling in, ignoring the objections of the first officer. The crew of the second plane questioned the order from air traffic controllers to taxi on the runway, but it obeyed anyway. Social hierarchy and economic pressure to keep things moving led _both_ crews to make critical mistakes, ultimately costing 583 lives.

Why should designers care about social context? **Although it may not seem obvious, social pressures are a design issue.** They affect the way we think, feel, and behave, ultimately influencing how we interact with the environment. Beyond that, social systems themselves are often the product of design, since they are shaped by institutional rules, hiring practices, traditions, and choices.

##### Finding the Root Cause

When accidents happen in industrial settings, a root cause analysis is usually performed. If the cause is technology-related, there is a full-scale investigation into the root problem, and engineers continue testing until the problem has been designed out. But **if human error is discovered, the investigation usually stops immediately.** Someone is held accountable and the system moves on unchanged.

Aviation history provides an example here too, this time of a 2010 crash involving a US Air Force fighter jet pilot. The particular type of plane the pilot was flying at the time had a history of malfunction that caused oxygen deprivation for the pilot. The official Air Force investigation into the crash concluded that the cause was human error—the pilot had failed to recognize the problem and correct the dive. Years later, the Department of Defense reanalyzed the case and concluded that, while the Air Force report was technically correct, it stopped before asking the crucial question: _why_ didn’t the pilot notice or correct the problem? Given the nature of the crash and the history of the plane, it’s likely the pilot was already unconscious due to lack of oxygen. Is that human error?

**One tool for more effective root cause analyses is the “Five Whys,"** developed by Sakichi Toyoda, the founder of Toyota Industries. As the name suggests, Toyoda’s idea was to ask “why?’ five times in order to find the true root of a problem. In reality, the number can vary, but the core idea is to push past the initial answer to a problem to find underlying causes. This technique is still in use today at Toyota.

Even when a true root cause of an accident is determined, there is always pushback to the idea of changing the system, since change is difficult and often costly. There is also a cultural element here: we are so used to the idea of human error that **people deemed responsible for accidents usually agree that they were, even if other people have made the same mistake.** This makes it hard to reframe as system error.

##### Deliberate Error

Sometimes, we choose to deliberately ignore a certain rule, or do things we know we shouldn’t do. These infractions tend to be seen as minor until tragedy happens. Most drivers have driven over the speed limit at some point—this is regarded as normal behavior. But when a crash happens due to excessive speed, the driver is blamed for the same behavior.

Why do we ignore rules, even when it’s dangerous? In corporate environments, there are often two sets of rules: the formal rules that are written into employee contracts and adhere to the required laws, and the informal social rules that determine what is actually expected of each employee. These rules often conflict to the degree that doing a job well (satisfying informal expectations) requires violating some of the formal rules. The more employees break the rules, the better they appear to perform, which reinforces the rulebreaking behavior. Social expectations can make us more likely to make mistakes.

##### Social Pressure and Error Reporting

When errors inevitably do occur, social pressures affect whether we report them properly.**Fear of judgment or repercussions stops us from reporting both our own errors and errors made by others.** But identifying errors as they occur is crucial for preventing them from escalating into major problems. How can companies combat the stigma against reporting errors?

Once again, Toyota’s manufacturing division offers a helpful example. Part of their error-reduction strategy is _Jidoka_ , or “automation with a human touch." In this philosophy, error is an expected part of the manufacturing process and should be addressed immediately, even if it means shutting down an entire production line. **Employees are expected to report errors immediately and can face repercussions for _not_ reporting them, which combats social pressure to remain quiet.**

Toyota engineers are also responsible for the idea of _poka-yoke,_ or “error proofing," typically through constraints. Covering emergency switches so they can’t be accidentally activated, attaching physical guides to machines to ensure parts are aligned correctly, and designing parts with asymmetrical attachments are all examples of poka-yoke.

Another example of effectively combating social barriers to error reporting is the NASA Aviation Safety Reporting system. This system is voluntary and records are stored without identifying information, so pilots don’t have to worry about repercussions from their employer or the FAA. Beyond that, NASA employees analyze the submissions and provide reports of common sources of error to the FAA and individual airlines, ultimately improving aviation safety across the entire industry.

### How Can Designers Minimize Errors?

Although it’s not possible to eliminate errors completely, thoughtful design can reduce the frequency or severity of those errors. The first step in that process is detecting when and where errors occur.

#### Detecting Error

A truly error-proof design makes it easy to detect errors before they become dangerous. Doing this requires understanding how we notice errors in the first place, and more importantly, why we sometimes fail to notice them.

**In general, slips are easier to detect than mistakes.** Detecting simple errors like action-based slips is typically easy—if you accidentally put your keys in the freezer, you’re likely to realize it pretty quickly. Memory-lapse slips are harder to detect until something cues retrieval of the memory (for example, not realizing you left your wallet at home until you need to pay for gas).

Mistakes are difficult to detect because they are conscious choices. **By definition, we usually don’t recognize mistakes right away because we genuinely believe we’re making the right choice as we’re making it.** Mistakes only become apparent later, when something goes wrong and the cause is traced back to the original mistake.

One reason we don’t catch mistakes earlier is the natural human tendency to explain away minor deviations from the norm. The author tells a story of driving with his family to a ski resort in California and passing several billboards for Las Vegas hotels. The family agreed that advertising on billboards located hours away from Las Vegas must be an odd marketing strategy and carried on with their journey, not realizing until two hours later that they’d missed a turn and were mistakenly headed straight toward Las Vegas. **We’re much more likely to notice novel information in our environment, but once we have an explanation, it’s no longer novel.** This explains why the author’s family was able to ignore all the other Las Vegas advertisements they passed before finally realizing their mistake.

In the aftermath of an accident, the chain of events leading up to it often seems obvious. **This is the power of _hindsight bias_ , or the tendency to estimate our ability to have predicted a certain outcome before it happened.** We wonder how anyone could have missed the signs of an important error when they seem so obvious—in reality, without the benefit of hindsight, we’d most likely have missed them too.

One way to improve error detection is with checklists. **Checklists are helpful tools, but they need to be designed with social influences in mind.** Having multiple people run through checklists helps in error proofing, but this should always take the form of two people working simultaneously, not sequentially. Having one person run through a checklist now and another person double check things later can actually lead to _more_ errors, since there is a tendency to let things slide, knowing someone else is likely to catch the mistake later. But when everyone takes this attitude, errors quickly add up. (Shortform note: To learn how to correctly use checklists, read our summary of The Checklist Manifesto.)

#### Understanding How Accidents Happen

There is rarely only one cause of an accident. More frequently, **accidents are the result of a number of conditions lining up in a particular way.** James Reason, an accident researcher, calls this “the Swiss cheese model." Think of each slice of cheese as a condition affecting a certain task (for example, weather). The holes in each slice are all the possible configurations of that condition (in the weather example, this would mean a hole for rain, a hole for snow, a hole for bright sun, and so on).

For accidents and errors to occur, the holes in several slices have to line up perfectly. If the hole in any one slice doesn’t line up, the event can’t happen.

![the-design-of-everyday-things-16.png](https://media.shortform.com/images/the-design-of-everyday-things-16.png)

In a car accident, for example, the four slices above might represent weather, alertness of the driver, condition of the brakes, and speed. If the holes line up perfectly—if it’s raining, the driver is sleep deprived, the brakes are worn out, and the driver is speeding—an accident is likely. But if any of those holes didn’t line up (like if the driver were more alert or the brakes were new), the accident could likely have been avoided.

To prevent accidents, we need to prevent the holes lining up. There are three main ways to do this:

  * **First** , create more conditions that must be met in order for a product to function properly (this adds more slices of cheese, making it statistically less likely that holes will line up).
  * **Second** , error-proof each condition as much as possible (this reduces the number or size of holes in each slice). 
  * **Third** , include feedback at every stage (this alerts the user if certain holes are beginning to line up, giving them a chance to stop the process before an accident occurs). 



#### Specific Guidelines for Designers

How can we create products and services that minimize errors, especially dangerous ones? Norman suggests some concrete ways:

  * Understand the factors that contribute to errors and identify which ones are possible to control. 
  * Make the system resilient. A certain degree of human error is unavoidable, but a single error should not be able to cause a chain reaction that leads to disaster. 
  * Make the “undo” function easy to access and available at every possible step.
    * For actions that cannot be undone, require multiple confirmations that the user wants to proceed with that action. This should be paired with a clear image of the specific item being acted on (to prevent such mistakes as deleting the wrong file). 
  * Use error messages to present users with guidance on how to fix the problem.
  * Remember that most errors don’t require completely starting over. Make it easier for users to fix a single step instead of the entire chain of actions.
  * Use constraints to prevent incorrect actions.
  * Incorporate “sensibility checks." 
    * Machines follow commands as long as they are in the correct format, regardless of whether they make logical sense. This leads to errors and even tragedy, particularly in medical environments (for example, an x-ray machine that is not error-proofed will allow a technician to mistakenly enter a command for a lethal dose of radiation). Error-proof technology is pre-programmed to understand the limits of each value, and will alert when a value is entered outside the acceptable range. 
  * Use frequent, effective feedback to prevent slips (for example, in hospitals, both prescription labels and patient ID bracelets are scanned before administering any medicine to ensure that the right person is getting the right drug). 



#### Is Design Always the Answer?

Unfortunately, design only goes so far, and there is still such a thing as “real” human error. This is especially true in cases where an activity can have catastrophic results if something goes wrong, but the likelihood of something going wrong in any one particular case is relatively low. When we know there’s a one in a million chance of something terrible happening, we assume we’re safe. The problem, of course, is that one person in that million will be wrong. We see this sense of invincibility at play when people deliberately make risky choices, like ignoring safety measures in order to get a job done faster, or driving after drinking.

**However, even in cases that appear to be pure human error, there is often a design element at play—specifically, the design of systems.** For example, we know that a sleep-deprived doctor is far more likely to make critical errors than a well-rested doctor. Yet hospital procedures still frequently have doctors working dangerously long shifts with little to no sleep. If a doctor in such a hospital makes an error, is it her fault? Or is it the fault of the complex system that required such long working hours in the first place?

Thankfully, some industries have adjusted the design of their systems to ensure employees are in top form before performing potentially dangerous operations. In aviation, pilots are only permitted to fly a certain number of hours without rest, but they must complete a minimum number of flying hours to keep their license active. This ensures that pilots only fly when they’re able to do so safely, and also that they remain in good practice. This kind of preventative strategy is called _resilience engineering._

##### Resilience Engineering

Preventative approaches to safety are especially important in industries where errors could lead to particularly disastrous effects, like medicine, transportation, and electrical power systems. **Resilience engineering focuses on building robust systems that can withstand any complications they might face, whether from human error, system breakdown, or external forces like natural disasters.**

The resilience engineering approach assumes that errors are inevitable, that people perform differently under extreme stress, and that the areas where systems are most vulnerable to error are constantly changing to reflect a changing environment. In practice, resilience engineering involves several processes. These assumptions give rise to three of the main tenets of resilience engineering.

  * **Focus on more than products.** Consider all the systems involved in making, selling, and using the products (including social systems).
  * **Test under real-life conditions.** For computerized systems, this might mean shutting down parts of the system without warning to test backup functions as well as employee response under real-life stress. 
  * **Test continuously, not as a means to an end.** Systems are constantly evolving, so testing only after major changes are implemented is not enough to ensure safety. 



[[book_md/the-design-of-everyday-things/exercise-identify-physical-constraints|exercise-identify-physical-constraints]]

[[book_md/the-design-of-everyday-things/exercise-identify-system-errors|exercise-identify-system-errors]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=4b374b27-a415-4c27-b6ad-bfd1ebd81b60&sid=1711133063fa11eebdec89a8b8ae3bbc&vid=171147a063fa11eea7440fcfeb230d96&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fthe-design-of-everyday-things%2Fchapter-5&r=&lt=396&evt=pageLoad&sv=1&rn=873631)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



