![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Superforecasting

Back to Discover

[[book_md/superforecasting/preview|preview]]

  * [[book_md/superforecasting|superforecasting]]
  * Full Book Guide

    * [[book_md/superforecasting/shortform-introduction|shortform-introduction]]
    * [[book_md/superforecasting/part-1|part-1]]
    * [[book_md/superforecasting/chapter-3|chapter-3]]
    * [[book_md/superforecasting/chapter-4|chapter-4]]
    * [[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]
    * [[book_md/superforecasting/part-2|part-2]]
    * [[book_md/superforecasting/exercise-answer-the-ball-and-bat-problem|exercise-answer-the-ball-and-bat-problem]]
    * [[book_md/superforecasting/exercise-generate-new-perspectives|exercise-generate-new-perspectives]]
    * [[book_md/superforecasting/chapter-6|chapter-6]]
    * [[book_md/superforecasting/exercise-embrace-probabilistic-thinking|exercise-embrace-probabilistic-thinking]]
    * [[book_md/superforecasting/chapter-7|chapter-7]]
    * [[book_md/superforecasting/chapter-8|chapter-8]]
    * [[book_md/superforecasting/exercise-develop-a-growth-mindset|exercise-develop-a-growth-mindset]]
    * [[book_md/superforecasting/chapters-9-10|chapters-9-10]]
    * [[book_md/superforecasting/exercise-identify-psychological-safety|exercise-identify-psychological-safety]]
    * [[book_md/superforecasting/chapter-11|chapter-11]]
    * [[book_md/superforecasting/exercise-weigh-the-impact-of-black-swan-events|exercise-weigh-the-impact-of-black-swan-events]]
    * [[book_md/superforecasting/chapter-12|chapter-12]]
  * [[book_md/superforecasting/highlights|highlights]]
  * [[book_md/superforecasting/community|community]]



Adding to Favorites 

Removing from Favorites 

## Part 2: Superforecaster Traits | Chapter 5: Superforecasters Think Like Foxes

The authors concede that it would be reasonable to assume that superforecasters are just a group of geniuses gifted at birth with the power to see the future. Reasonable, but wrong. What makes superforecasters truly “super” is the way they use their intelligence to approach a problem. In the next few chapters, we’ll explore the specific mental tools that make superforecasters so accurate.

### They Avoid Cognitive Biases

According to the authors, the reason superforecasters make such accurate predictions is that they’re adept at avoiding cognitive biases. We’re all prone to certain cognitive biases that stem from unconscious thinking, or what psychologists like Tetlock often describe as “System 1” thinking. These biases skew our judgment, often without us even noticing. Superforecasters constantly monitor and question their System 1 assumptions. (Daniel Kahneman describes the two-system model of thinking in _Thinking, Fast and Slow_ ; **System 2 governs conscious, deliberate thinking, while System 1 functions automatically and unconsciously.**)

(Shortform note: Kahneman argues that System 1 is also prone to making snap predictions. For example, when we see someone with an angry expression, System 1 automatically predicts that person is about to start yelling. Those automatic predictions could be another mental trap for prudent forecasters to avoid.)

#### Availability Heuristic

The _availability heuristic_ is a form of bias that was formally discovered and named by researchers Daniel Kahneman and Amos Tversky to describe the automatic process of making snap decisions based on memorable experiences. The authors argue that this is an adaptive evolutionary trait. For example, if hearing the snap of a twig on the savannah brings to mind a memory of a lion pouncing on its prey, you automatically conclude that a lion must be the source of the sound and that you are in danger. If you’ve never witnessed or heard of a lion attack, you’ll interpret the sound differently (and not realize the danger you’re in). The availability heuristic plays out unconsciously, in fractions of a second.

In the context of formal forecasting, the availability heuristic might come into play if a typical forecaster is asked to predict an event they have some previous mental association with. For example, if a forecaster who lived in New York City during the 9/11 terrorist attacks was asked to predict the likelihood of a terrorist hijacking an airplane, they may unknowingly predict a much higher likelihood because the question calls up visceral memories of 9/11.

(Shortform note: This is similar to the concept of frugality that author Malcolm Gladwell describes in _Blink_. Gladwell argues that the unconscious mind is “frugal”—that is, it automatically lasers in on the most significant details of a situation and ignores everything else. In the lion example, the unconscious mind automatically connects the dots between the sound of a twig snapping and the most significant possible meaning (a lion approaching). It temporarily ignores all other possible explanations that wouldn’t have life-or-death consequences.)

#### Confirmation Bias

Before making predictions, we often search for evidence. According to Tetlock and Gardner**,** the problem here is our tendency to latch onto the first possible explanation we think of, then only seek out evidence that supports that belief (and ignore evidence that contradicts it).**This is _confirmation bias_ , and it’s a dangerous trap for forecasters.** (Shortform note: Confirmation bias is also known as “motivated reasoning.”)

Cherry-picking evidence can quickly lead to drawing conclusions that are completely off base. For example, if a typical forecaster instinctively assumes an event _will_ happen, they may seek out evidence that supports that prediction and downplay evidence that suggests the event is unlikely to happen.

> **Confirmation Bias Has Dangerous Consequences—Like Medical Misdiagnosis**
> 
> Confirmation bias (like all biases) is particularly dangerous in high-stakes fields like medicine. For example, medical students are often taught the aphorism, “When you hear hoofbeats, think horses, not zebras”; in other words, that the most common explanation is probably the right one. However, when doctors are expecting a medical “horse” (such as the common cold), confirmation bias makes them more likely to ignore evidence that that horse is actually a zebra (such as an autoimmune disorder). Not having an accurate diagnosis can prevent people from accessing helpful (or even life-saving) treatments.

#### “Tip-of-Your-Nose” Perspective

Each of us sees the world from the “tip of our nose”—our own perspective. According to the authors, this view is highly subjective and informed by our own unique experiences. The things in your environment that automatically stand out to you as significant will be completely different from the things that automatically stand out to someone else. Forecasters can fall prey to this bias when they over-rely on information from their own field. For example, if a typical forecaster who works as an economist was asked to predict the likelihood of a certain country’s economy collapsing, they might put too much emphasis on the country’s economic history and ignore the impact of social and political factors.

> **The Limits of System 2**
> 
> The tricky thing about all of these biases is that, according to Kahneman, we’re much less likely to catch our own biased thinking when System 2 is already taxed. That means that if you’re already thinking hard about something else (or you’re tired from thinking hard earlier in the day), System 1 is likely to take over because you’ve already exhausted your System 2 resources. That presents a challenge for superforecasting, which requires intense cognitive effort and can quickly deplete System 2. In turn, this challenge presents a paradox: It’s important for superforecasters to avoid cognitive biases, but the act of superforecasting itself might make them more prone to cognitive biases if they do too much at a time.

### They Generate Multiple Perspectives

Superforecasters also rely on aggregated judgment (aggregation is the process of combining data from multiple sources). Tetlock and Gardner argue that aggregation is a powerful tool for forecasters because the **aggregated judgment of a group of people is usually more accurate than the judgment of an average member of the group.** This phenomenon is often called **“the wisdom of crowds,"** named for the 2004 book that popularized the idea.

Superforecasters take advantage of the wisdom of crowds by pulling from many sources and using many tools to produce an aggregate answer, despite being just one person. This skill doesn’t come naturally to most people—we struggle to even recognize that there are other perspectives beyond our own “tip-of-the-nose” view, let alone fully consider those ideas. This is part of what sets superforecasters apart.

> **How to Generate New Perspectives Like a Superforecaster**
> 
> To think like a superforecaster, you need to look beyond the tip-of-your-nose view and find new ways of viewing a problem. Here are some tips to get you in the right mindset:
> 
>   * Reverse-engineer the problem. For example, if you’re asked to predict the likelihood of the U.S. raising the federal minimum wage to $15 per hour, pretend it’s already happened and work backwards from there. What would have had to change to lead to a change in the minimum wage?
> 
>   * Broaden your horizons. Commit to reading books and following news sources outside of your comfort zone. The more diverse perspectives you expose yourself to, the easier it will be to approach a particular problem from a different vantage point.
> 
>   * Try attacking your own conclusions. This is similar to the technique of “negative empiricism” that Taleb describes in _The Black Swan_ , which involves deliberately searching for evidence that will disprove your argument. For example, for the minimum wage question, if you ultimately conclude that the U.S. federal government _will_ raise the minimum wage, go back and look for evidence that they _won’t_.
> 
> 


### They Use Fermi Estimation

According to the authors, another mental trick that sets superforecasters apart is their ability to break down a complex problem, even without having all the information. One form of this technique was popularized by Enrico Fermi, a Nobel Prize-winning Italian American physicist who also worked on the Manhattan Project. Fermi’s approach was to break down seemingly impossible questions into smaller and smaller questions. **The idea is that eventually, you’ll be able to separate questions that are truly unknown from questions for which you can at least make an educated guess**. Tetlock and Gardner believe that, added together, a handful of educated guesses can get you remarkably close to the correct answer to the original question.

#### Example Fermi Problem

It’s much easier to understand this process with an example, such as **“How many oil changes are performed in one day in the U.S.A.?”** Unless you happen to work at Jiffy Lube, you probably have no idea how to even begin to answer this question. At best, you might take a wild guess. But Fermi’s approach is more methodical.

To answer this question, we need to determine what information we’re missing. What would we need to know to figure this out? The number of oil changes probably depends on the number of cars in the country and how often those cars need an oil change. So, to figure out how many oil changes happen in a given day, we need to know:

  1. The number of cars in the U.S.
  2. How frequently cars need an oil change



If we happened to know the answers to either of those questions, great! But if we don’t, we can break them down even further into questions that we can answer. Let’s do that now.

To figure out the number of cars in the U.S., we need to know:

  1. The population of the U.S.
  2. The percent of the population that owns a car



To figure out how frequently cars need an oil change, we need to know:

  1. How many miles cars can go between oil changes
  2. How many miles the average car is driven in a year



We may still need to wildly guess the answers to _some_ of these questions. However, because those wild guesses are only for smaller parts of the overall question, we’re still likely to be more accurate than if we’d just thrown out an answer to the original question.

##### Solving the Problem

To solve this problem, we’ll need to come up with some estimates for each of those questions. (Shortform note: Fermi questions are commonly used in job interviews. To get the most out of this exercise, assume you’re in the middle of an interview and can’t look up the real answers, so you’ll need to rely on your best guess.)

  1. First, we need to know the population of the U.S. You might have a rough idea of this answer—it’s about **320 million people**. 
  2. Next, we need to know how many of those people own a car. This question isn’t as clear cut, so we can break it down further. Let’s assume that roughly 75% of the population can drive, which gives us **240 million drivers**. 
     1. However, not everyone who can drive owns their own car. Let’s assume there’s an average of two drivers in a household, and one car per household. That’s **120 million cars**. 
  3. If you have a car, you might remember that you’re supposed to change the oil after about 3,000 miles of driving. However, people are busy, and oil changes might not always be a priority. Let’s assume that the average car owner gets their oil changed after about **3,500 miles** of driving. 
  4. Now, we need to know how many miles the average car is driven in a year. This is a tough one, so let’s take a wild guess of **10,000 miles per car, per year**. 



If each car owner gets an oil change after 3,500 miles of driving and drives 10,000 miles per year, they get about three oil changes per year. (Shortform note: In Fermi estimation, there’s no need to stress over the exact math. Feel free to round numbers up or down to make the math easier since the end goal is a very rough guess, not an exact answer.)

Three oil changes per year for 120 million cars is **360 million oil changes per year, total.** However, the question asked about the number of oil changes in one _day_ , not one year, so we need to divide this further. We know there are 365 days in a year, but most auto shops aren’t open seven days a week, so let’s bring that down to 320 days per year in which oil changes are being performed. To get our final answer, we divide 360 oil changes per year into 320 days, which equals **about 1.1 million oil changes per day in the U.S.**

What’s the real answer? Industry data shows that about 450 million oil changes happen per year in the United States. If we keep our estimate of 320 working days per year for oil changes, that’s 1.4 million oil changes per day, which is incredibly close to our rough estimate!

> **Tips for Improving Fermi Estimations**
> 
> As we’ve previously noted, Fermi problems are often used during job interviews to evaluate a candidate’s critical thinking skills. For instance, Google often asks interviewees subject-specific Fermi questions, such as “How many photos are taken on Android phones per day?” If you want to practice this technique, here are some tips to improve your estimation skills:
> 
>   * **Memorize basic facts,** such as the population of the world, the speed of light, or the average number of people per household.
> 
>   * **Memorize basic unit conversions** , like the number of feet in a mile (5,280) or pints in a gallon (eight).
> 
>   * When you finish a problem, **use common sense to double-check your answer**. For example, if you go through all the steps of answering, “How many golf balls would it take to fill a football stadium?” and your answer is “350,” you may need to go back and check some of your estimates.
> 
> 


### They Seek Out Intellectual Challenges

Tetlock and Gardner argue that breaking down questions like Fermi, synthesizing outside and inside views, and aggregating different perspectives may not be natural talents, but they are incredibly mentally demanding, and superforecasters do them for no profit. That tells us that there _is_ something fundamentally different about superforecasters’ brains—not that they _can_ do all that mental work, but that they _want_ to.

According to the authors, people who volunteer for forecasting tournaments are the type who enjoy mental challenges. They’re more likely than others to spend their spare time doing crossword puzzles or reading dense books. **In psychology, this is called having a high “need for cognition,” or the internal motivation to seek out cognitive challenges.**

> **High Need for Cognition Is Related to Bias**
> 
> Superforecasters are “super” largely because they are so skilled at avoiding cognitive biases. However, superforecasters are also almost universally high in need for cognition (NFC), which can make them _more_ prone to certain kinds of bias. For example:
> 
>   * Research shows that people with high need for cognition are more susceptible to anchoring effects (“anchoring” happens when people get mentally stuck on the first piece of information they encounter, even if it’s not the most relevant piece of information. We’ll discuss this concept further in Chapter 7).
> 
>   * People with high NFC are more susceptible to priming effects than people with low NFC (priming is a way of influencing someone’s behavior by exposing them to a particular stimulus first). For example, in one study, researchers primed people with words related to either winning or losing before they placed bets on a roulette wheel. People with high NFC bet more when they were primed with winning and less when they were primed with losing, but the bets of people with low NFC didn’t change based on the prime.
> 
>   * Mood can cause bias in people with high NFC. They’re more likely to overestimate positive outcomes when they’re in a good mood and overestimate negative outcomes when they’re in a bad mood.
> 
> 

> 
> How are superforecasters so good at avoiding biases when their high NFC should make them even _more_ biased? It’s possible that working in teams helps superforecasters overcome their natural biases. We’ll learn more about how superforecasting teams help each other avoid bias in Chapter 9.

[[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]

[[book_md/superforecasting/exercise-answer-the-ball-and-bat-problem|exercise-answer-the-ball-and-bat-problem]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=e4917039-9e4d-440e-b359-82202d3d4c04&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fsuperforecasting%2Fpart-2&r=&lt=369&evt=pageLoad&sv=1&rn=867533)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



