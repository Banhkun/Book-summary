![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Superforecasting

Back to Discover

[[book_md/superforecasting/preview|preview]]

  * [[book_md/superforecasting|superforecasting]]
  * Full Book Guide

    * [[book_md/superforecasting/shortform-introduction|shortform-introduction]]
    * [[book_md/superforecasting/part-1|part-1]]
    * [[book_md/superforecasting/chapter-3|chapter-3]]
    * [[book_md/superforecasting/chapter-4|chapter-4]]
    * [[book_md/superforecasting/exercise-are-you-a-hedgehog-or-a-fox|exercise-are-you-a-hedgehog-or-a-fox]]
    * [[book_md/superforecasting/part-2|part-2]]
    * [[book_md/superforecasting/exercise-answer-the-ball-and-bat-problem|exercise-answer-the-ball-and-bat-problem]]
    * [[book_md/superforecasting/exercise-generate-new-perspectives|exercise-generate-new-perspectives]]
    * [[book_md/superforecasting/chapter-6|chapter-6]]
    * [[book_md/superforecasting/exercise-embrace-probabilistic-thinking|exercise-embrace-probabilistic-thinking]]
    * [[book_md/superforecasting/chapter-7|chapter-7]]
    * [[book_md/superforecasting/chapter-8|chapter-8]]
    * [[book_md/superforecasting/exercise-develop-a-growth-mindset|exercise-develop-a-growth-mindset]]
    * [[book_md/superforecasting/chapters-9-10|chapters-9-10]]
    * [[book_md/superforecasting/exercise-identify-psychological-safety|exercise-identify-psychological-safety]]
    * [[book_md/superforecasting/chapter-11|chapter-11]]
    * [[book_md/superforecasting/exercise-weigh-the-impact-of-black-swan-events|exercise-weigh-the-impact-of-black-swan-events]]
    * [[book_md/superforecasting/chapter-12|chapter-12]]
  * [[book_md/superforecasting/highlights|highlights]]
  * [[book_md/superforecasting/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 7: Superforecasters Start With a Base Rate, Then Update

In this chapter, we’ll discuss another technique that the authors argue superforecasters use: the “outside-in” approach to forecasting. We’ll also discuss how superforecasters update their predictions based on new information they encounter after making their initial forecast. In IARPA-style tournaments, forecasting questions typically remain open anywhere from a few weeks to a few months, and forecasters can adjust their forecasts as often as they like during that window. The authors argue that these adjustments are crucial since a forecast that doesn’t take _all_ the available data into account will likely be less accurate than an up-to-date forecast. (Shortform note: In _Smarter Faster Better_ , author Charles Duhigg interviews poker master Annie Duke, who argues that updating her beliefs about her opponents (instead of sticking to her initial assumptions) helps her avoid prejudice, which in turn makes her less likely to underestimate her opponents.)

### They Think From the Outside In

Tetlock and Gardner argue that when superforecasters first encounter a question, they begin by looking at the wide perspective of that question before accounting for the specifics (in _Thinking, Fast and Slow_ , Daniel Kahneman calls that wider perspective the “outside view.”) Compare this to the “inside view,” which describes the particular details of a situation.

  * For example, imagine someone tells you about their physician friend, Dr. Jones, and asks you to estimate the likelihood that Dr. Jones is a pediatrician. If you start with the outside view, you’ll ignore any details about the specific person. Instead of thinking about whether that person seems like the type to work in pediatrics, you’d try to answer the question “What percentage of doctors specialize in pediatrics overall?” 
  * By contrast, if you take the inside view, you’ll focus on the unique details you learn about Dr. Jones—their personality, whether they have children of their own, and so on. You’ll really be answering the question, “Is this the sort of person who is likely to be a pediatrician?”



By nature, our storytelling minds gravitate toward the inside view. Statistics are dry and abstract—digging into the nitty-gritty details of someone’s personality is much more exciting. But that natural tendency can quickly lead us astray. If we’re told that Dr. Jones loves children and worked at a summer camp for sick children during college, we might say it’s 80% likely that Dr. Jones is a pediatrician. On the other hand, if we’re told that Dr. Jones is a very serious, reserved person and has no plans to become a parent, we might swing to the other extreme and guess 2%.

According to the authors, the problem with this practice is that we have no way of knowing _how_ extreme those answers are. For that, we need a **base rate** to give us an idea of how common it is to specialize in pediatrics in _general_. In reality, only about 6.5% of doctors specialize in pediatrics. A guess of 2% is closer to the mean than a guess of 80%, which means that an 80% guess is more likely to be wrong.

But why does it matter which view we start with? If we’re going to adjust our initial outside-view guess based on information from the inside view, wouldn’t the reverse give the same answer? The authors argue not, due to a psychological concept called **anchoring**. The number we start with has a powerful hold on us, and we tend to under-adjust in the face of new information.

For example, in the pediatrician question, if we start with an inside view guess of 80%, then move to an outside view of 6.5%, anchoring means we’d bump our original number down, but only slightly—maybe to 50%. In this case, our anchor came out of thin air and significantly skewed the final results. But if we start with an anchor of 6.5% (outside view), then move to an inside view and see that Dr. Jones doesn’t seem like the type to specialize in pediatrics, we might adjust that number down to 3%; this outside-in guess is far more likely to be accurate.

(Shortform note: Anchoring is an incredibly powerful form of unconscious bias. In _Predictably Irrational_ , author Dan Ariely describes how we can unknowingly anchor to a number as irrelevant as our social security number. In one experiment, researchers asked people to write down the last two numbers of their social security number before bidding on items in a silent auction. Participants with higher digits in their social security numbers were willing to bid significantly higher for each item than participants with lower digits in their social security numbers.)

> **Master the Outside View With a Premortem**
> 
> In _Thinking, Fast and Slow_ , Daniel Kahneman advises using a “premortem” analysis to avoid the dangers of inside-out thinking. A premortem analysis is a mental exercise in which you imagine that whatever you’re working on (be it a project or a forecast) has already come to fruition—and was a complete disaster. Your goal is to come up with as many reasons as possible to explain this hypothetical “failure.”
> 
> This approach is helpful because, by nature, the inside view makes a situation feel “special”—it predisposes you to focus on what makes the situation unique. That feeling can make it more difficult to notice biases in your prediction because you might assume the current situation won’t abide by the usual “rules.” For example, most newlyweds probably don’t expect to ever get divorced, despite the 40-50% divorce rate. That’s because, from the inside, the relationship feels “special” or distinct from the relationships that ended in divorce.
> 
> The premortem technique can help you reorient to the outside view because assuming your answer is incorrect will likely force you to recognize that the specifics of this situation aren’t as important as the base rate. For example, if you’re predicting whether a startup will succeed, it’s tempting to take the inside view and make your forecast based on the business model or the founder’s previous business experience. However, if you try a premortem analysis, it will be easy to come up with reasons the company failed given that the failure rate for startups is roughly 90%. That sobering statistic can help remind you that even if the inside view looks like a recipe for success, the odds are stacked so strongly against new businesses that failure is much more likely.

### Underreaction

Underreacting to new information is often the result of a cognitive bias sneaking into the equation. Even superforecasters are not immune to this, despite their natural self-awareness and ability to suspend judgment until they have the facts. In particular, **attribute substitution (or “bait and switch”) often slips under the radar** , particularly for questions about an individual person’s decisions. In that situation, we can’t help but mentally insert ourselves, and **the question changes from “What will this person do in this situation?” to “What would _I_ do in this situation?” **

For example, imagine you’re predicting whether your boss will hire a certain job candidate. If you met the candidate and thought they were a terrible fit for the position, you might automatically assume your boss will share that impression and turn them down for the job. Even if you hear your boss speaking highly of that person after interviewing them, you may be so stuck on your own prediction that you underreact to the new information (that your boss was impressed by the candidate) and fail to update your prediction.

> **Stereotypes Are a Form of Attribute Substitution**
> 
> When making predictions about individual people, we may also unconsciously rely on stereotypes, which are another form of attribute substitution. For example, imagine an American school administrator is trying to predict which of two students in their school will perform better on a standardized math test. If the administrator holds an unconscious stereotype that boys are better at math, they may automatically assume that the male student will outperform the female student because they’ve substituted the easy question, “Is this student male?” for the harder question, “Is this student good at math?”
> 
> Now, imagine the administrator gets a new piece of information: The female student is an exchange student from Estonia, a country that vastly outperforms the United States in measures of math education. Depending on the strength of the administrator’s unconscious stereotype, they may underreact to this new piece of information and still predict that the male student will do better on the test.

### Overreaction

At the other end of the spectrum of belief updating is overreaction. According to the authors, the most difficult part of updating forecasts is deciding what new information is relevant and what is just noise. In some situations, it’s easy to tell the difference. For example, imagine you predicted that the U.S. president would veto a controversial new law. The next day, you learn that the president is an avid baker. Would you change your forecast?

In theory, the president’s taste in hobbies seems irrelevant to the question, so it makes sense to ignore it and keep your forecast the same. But in practice, we’re far less logical. Tetlock and Gardner argue that the more information we have about someone, the less confidently we can predict _anything_ about them, even when that information is completely irrelevant. This is called the **dilution effect** , where adding new information dilutes the perceived importance of every piece of information. Irrelevant information makes us less confident in our beliefs because it rounds out the situation and makes it harder to categorize. In the example above, you might overreact to the information about the president’s baking hobby by updating your forecast because the new information makes it harder to decisively categorize the situation.

(Shortform note: There are several ways to use the dilution effect to your advantage. For example, if a family member holds sexist beliefs about women in power, you might tell them stories about your female boss’s likes and dislikes, hobbies, and quirks to round out their mental image of her and possibly lessen the strength of their sexist beliefs. On the other hand, if you want to persuade someone to do something, stick to one or two strong justifications for it—if you provide too many, you could dilute the strength of your argument, even if your reasoning is sound.)

### Striking a Balance

According to the authors, superforecasters make a _lot_ of forecasts. A superforecaster may update a single prediction as many as 77 times in the three-month open window of a question. The most accurate superforecasters update more often and in smaller increments than their peers, who are more likely to under- or over-react to new information. But how do they know how much to update by? While superforecasters don’t often use actual math in their forecasts, the authors argue that the Bayesian belief-updating equation is a helpful way to think about how much a piece of new information should affect your original forecast. In mathematical terms, the theorem looks like this:

> P(H|D)/P(-H|D) = P(D|H)/P(D|-H) • P(H)/P(-H)
> 
> Posterior odds = Likelihood ratio • Prior odds

In plain English, Bayes’ theorem tells us that the new belief (“posterior odds”) should be the product of the old belief (“prior odds”) and the weight of new information (“likelihood ratio”). (Shortform note: In this case, “prior odds'' are typically the “outside view” base rate that we discussed above.) While superforecasters rarely use this theorem to do actual calculations, the principle of adjusting predictions based on the weight of new information is key to their success.

> **Is it Possible to Be a Bayesian Bigot?**
> 
> We’ve seen how cognitive biases like attribute substitution can trick us into relying on stereotypes to make decisions. While applying Bayes’ theorem helps forecasters avoid those cognitive biases, it doesn’t fully eliminate the risk of playing into stereotypes. That’s because Bayes’ theorem requires starting with a statistical base rate—which can reflect uncomfortable inequalities.
> 
> For example, Black Americans are nearly six times more likely than white Americans to be incarcerated at some point in their lifetime. This is what Tetlock called a “forbidden base rate” in his earlier research on moral responses to Bayesian decision-making (forbidden base rates are “forbidden” because they could be offensive to some groups and/or because they could be used to perpetuate harmful stereotypes). Therefore, if a forecaster used Bayes’ theorem to predict the likelihood that a given person had a criminal record, the forecaster would use that base rate to assign a higher probability if the person in question were Black. The forecaster, then, would be what Tetlock calls a “Bayesian bigot.”
> 
> This presents a conundrum: If Bayesian methods are the most rational way to make predictions, what does it mean when they lead us to socially biased conclusions? To answer that, we need to examine the base rate itself. For example, if we assume that Black people are incarcerated at higher rates because they commit more crimes, then it would appear that Bayesian thinking provides a rational justification for the stereotype that Black people are prone to crime.
> 
> However, there is another way to interpret the same base rate. In _Biased_ , Dr. Jennifer Eberhardt describes how Black people who have been arrested are more likely to be held in jail until their trial than white people because of racial bias in the cash bail system. Being held in pre-trial detention increases the likelihood of accepting a plea bargain, which results in an official criminal record. With this explanation, we can accept the Bayesian conclusion—“A given Black person is statistically more likely to have a criminal record than a given white person”—without seeing it as evidence for the stereotype linking Black people and crime.

[[book_md/superforecasting/exercise-embrace-probabilistic-thinking|exercise-embrace-probabilistic-thinking]]

[[book_md/superforecasting/chapter-8|chapter-8]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=e7f1d2ba-0319-43b5-b1dd-1f98289771d7&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fsuperforecasting%2Fchapter-7&r=&lt=349&evt=pageLoad&sv=1&rn=400876)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



