![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Superintelligence

Back to Discover

[[book_md/superintelligence/preview|preview]]

  * [[book_md/superintelligence|superintelligence]]
  * Full Book Guide

    * [[book_md/superintelligence/exercise-what-would-you-do-with-a-superintelligent-ai|exercise-what-would-you-do-with-a-superintelligent-ai]]
  * [[book_md/superintelligence/highlights|highlights]]
  * [[book_md/superintelligence/community|community]]



![](/img/tutorial-fonts.175b2111.svg)

##### Change text options

Here you can change the font, text size, and reading screen to just how you like it. 

Next

  *   *   *   *   * 


![](/img/tutorial-menu.4c76dd27.svg)

##### Table of contents

Here you’ll find everything else, including the full chapter-by-chapter guide, your highlights, PDF downloads, and book discussions. 

Next

  *   *   *   *   * 


![](/img/tutorial-player.d25b1afb.svg)

##### Audio

Every guide has an audio narration so you can listen on the go. 

Next

  *   *   *   *   * 


![](/img/tutorial-favorite.b948300a.svg)

##### Add to Favorite

Mark your favorite guides here. You can find your favorites on your homepage. 

Next

  *   *   *   *   * 


![](/img/tutorial-night.ddd7fb5c.svg)

##### Night Mode

Like a darker look when you read? Turn dark mode on here. 

Finish

  *   *   *   *   * 


Adding to Favorites 

Removing from Favorites 

## 1-Page Summary

Oxford philosopher Nick Bostrom wrote _Superintelligence_ in 2014 to raise awareness about the possibility of AI suddenly exceeding human capabilities, spark discussion about the risks inherent in this scenario, and foster collaboration in managing those risks.

Today, the possibility of AI rivaling or even vastly exceeding human intelligence doesn’t seem as far-fetched as it did when Bostrom was writing a decade ago. In this guide, we’ll consider his arguments for the feasibility of an AI rising to superhuman intelligence and his belief that creating such an AI without the right controls in place could be the worst—and maybe the last—mistake in human history. Finally, we’ll take a look at the controls Bostrom says we need to implement to build AI’s safely.

Along the way we’ll compare Bostrom’s perspective to that of other futurists, such as Peter Thiel and Yuval Harari, and we’ll look at the impact of AI developments that have occurred since the book’s publication.

### The Feasibility of Superintelligent AI

Bostrom defines “superintelligence” as general intelligence that’s significantly greater than human-level intelligence. As he explains, “general intelligence” refers to intellectual abilities that span the whole range of human capabilities, such as learning, interpreting raw data to draw useful inferences, making decisions, recognizing risks or uncertainties, and allowing for uncertainties when making decisions. He notes that while some computers already surpass humans in certain narrow areas, such as playing a certain game or crunching numbers, no AI has yet come close to human-level general intelligence.

But could an artificial, nonhuman entity ever have superintelligence? Bostrom argues that the answer is, most likely, yes. As he explains, silicon computers have a number of advantages over human brains. For one thing, they operate much faster. Neural signals travel about 120 meters per second and neurons can cycle at a maximum frequency of about 200 Hertz. By contrast, electronic signals travel at the speed of light (300,000,000 meters per second) and electronic processors often cycle at 2 billion Hertz or more. In addition, computers can copy and share data and software directly, while humans have to learn gradually.

> **Tools for Humans vs. Replacements for Humans**
> 
> Peter Thiel would probably argue that the computer advantages Bostrom lists are only advantageous in certain applications. In _Zero to One_ (published the same year as _Superintelligence_), Thiel contends that humans and computers excel at such different things that we shouldn’t worry about computers replacing human workers.
> 
> He writes that while computers outperform humans at certain tasks, there are many other tasks that humans do effortlessly while the best AI algorithms find them practically impossible. While Thiel concedes that superintelligent AI might one day be developed, he argues that it’s too far away to concern ourselves with in the 21st century. Instead, we should focus on building AI tools that merely complement human abilities.
> 
> However, in _21 Lessons for the 21st Century_ , Yuval Noah Harari argues that AI _will_ achieve human-level or greater intelligence in the 21st century. To Bostrom’s arguments, he adds that recent advances in information science and neurology have shown that algorithms can demonstrate many capabilities once thought to be uniquely human, such as intuition and creativity. That said, Harari doesn’t seem to envision AI becoming _superintelligent_ (that is, so intelligent that it’s beyond humans’ ability to control it), as Bostrom does.

#### Different Routes to Superintelligent AI

As Bostrom explains, there are a number of different ways that superintelligent AI could be achieved. Thus, even if some of them don’t end up working, at least one of them probably will.

##### Intelligent Design

One route to superintelligent AI that Bostrom discusses is human programmers developing a “seed AI” that has some level of general intelligence—perhaps similar to human intelligence, or maybe a little below that mark. Then they use the AI to continue improving the program. As the AI gets smarter, it improves itself more quickly. Because of this self-reinforcing cycle, it might progress from sub-human to superhuman intelligence rather quickly.

(Shortform note: There’s been significant progress along this route since Bostrom wrote _Superintelligence_ in 2014. For example, more powerful computers have made it possible to create Large Language Models (LLMs) that can read and write in both natural human languages and computer code. The ability of LLMs to use normal human language represents a milestone in general AI development, and their ability to write computer code represents a key building block of self-improving AIs like the ones Bostrom describes.)

##### Simulated Evolution

Another route that Bostrom discusses is “simulated evolution.” In the context of software engineering, this means programming a computer to generate random variations of a program, test their functionality against specified criteria, and continue to iterate on the best ones. Theoretically, simulated evolution can provide novel solutions to programming problems without the need for new insight on the part of human programmers. Thus, even if human programmers can’t figure out how to create a superintelligent AI or a self-improving seed AI directly, they might be able to create one using simulated evolution.

> **Update on Simulated Evolution**
> 
> Progress along this route since 2014 has been slower. Today, most of the major works on the subject still predate Bostrom’s book. However, there has been some recent interest in hybrid algorithms that use simulated evolution to enhance more conventional machine-learning algorithms.
> 
> The current trend to focus on conventional AI design over simulated evolution makes sense. As Bostrom observes, simulated evolution might provide a way to create superintelligent AI if human developers get stuck on the problem. However, right now, AI development is progressing rapidly, so programmers don’t _need_ a fallback plan. But if progress on the problem of general AI stalls in the future, simulated evolution might gain more traction as a way to begin progressing again.

##### Brain Simulations

Yet another route to which Bostrom devotes considerable attention is “whole brain emulation.” The human brain is obviously capable of human-level general intelligence. Thus, if you could map out exactly how all the neurons in a human brain are connected and create a computer program to accurately simulate all those connections, you would have a program capable of human-level intelligence. And if the computer program could operate faster than the original brain, it would have superhuman intelligence.

Bostrom explains that creating a simulated human brain requires a basic understanding of how neurons interact with each other and a detailed cellular-level 3D scan of a human brain. However, it _doesn’t_ require an understanding of how the brain’s structures give rise to intelligence—assuming the simulation captures the placement of neurons accurately, it should, theoretically, mimic the brain’s function even if its developers don’t know exactly how or why. Thus, the main obstacle to implementing this method is scanning a human brain precisely enough.

> **The Human Connectome Project**
> 
> The Human Connectome Project has made progress on scanning the human brain and making data on its structure available to the public. Sponsored by the National Institutes of Health, the purpose of the project is to develop a detailed structural and functional mapping of the human brain that can help medical practitioners diagnose neurological disorders and develop treatments for them.
> 
> The project uses several types of MRI techniques to scan participants’ brains. One ongoing challenge is combining the data from different scans accurately. Another challenge arises from the discovery that individual human brains are remarkably different—even for people who have the same genetic code, like identical twins. Despite these challenges, the project has been able to publish brain mapping data from 1100 healthy adults.
> 
> So far, there have been no widely publicized attempts to develop brain simulation algorithms based on data from the Human Connectome Project. Nevertheless, the project provides a window into recent progress and the current state of the art in the kind of brain scanning that would be required for brain-simulation AI.

##### Spontaneous Generation

Finally, Bostrom points out that it might be possible to create a superintelligent AI _inadvertently_. Scientists don’t know exactly what the minimum set of components or capabilities for general intelligence is. There’s already a lot of software that performs specific information processing operations and has the ability to send and receive data over the internet. Hypothetically, a programmer could create a piece of software that, by itself, isn’t even considered AI, but it happens to be the final component of general intelligence. This would then allow a superintelligent AI to arise spontaneously on the internet as the new software begins to communicate with all the other software that’s already running.

> **Other Technologies Built by Accident**
> 
> If Bostrom’s suggestion that superintelligent AI could arise by accident seems far-fetched, it’s worth considering other technologies that were discovered or created by accident, such as the microwave oven, the first antibiotics, safety matches, and the discovery of radioactivity.
> 
> But perhaps the best illustration is the 1.7-billion-year-old nuclear reactor discovered in a western African mine called Oklo. The reactor was created when, almost two billion years ago, floods dissolved uranium from a mudflat and swept it into underground pools, where it was absorbed by algae. When the algae died, the uranium they'd concentrated in their cells piled up, eventually becoming a sizable deposit. Subsequent flooding provided the water needed to allow for a sustained nuclear reaction as the deposit decayed, and as a result, the mine produced a naturally occurring nuclear reactor that ran for approximately 150,000 years before its uranium deposits ran out. (It's no longer active, but has left behind evidence of these reactions.)
> 
> While a nuclear power plant likely has fewer essential components than artificial intelligence does, the natural nuclear reactors near Oklo illustrate how spontaneous interaction between seemingly unrelated components can suddenly give rise to new behavior when the components are brought together.

### The Consequences of Superintelligent AI

So, sooner or later, a superintelligent AI will be created. Why should that concern you any more than the fact that mechanical vehicles can go faster than a human can run? According to Bostrom, the rise of a superintelligent AI could cause dramatic changes in how the world works—changes that would take place very quickly. And depending on the superintelligent AI’s behavior, these changes could be very detrimental to humanity.

As we mentioned earlier, if an AI has some measure of general intelligence and the ability to modify its own programming, its intelligence would likely increase at an ever-accelerating rate. This implies that an AI might rise from sub-human to superhuman intelligence very quickly.

(Shortform note: Although we’ve not yet witnessed this type of growth in artificial intelligence, there are other applications that demonstrate how self-accelerating growth can cause rapid transformations. One is the “coulombic explosion” reaction between water and alkali metals, where the chemical reaction causes the surface area of the metal to increase and the reaction speed is proportional to the surface area. When this condition occurs, the reaction rate increases so quickly that the metal appears to explode.)

Moreover, as Bostrom points out, superior intelligence is what has allowed humans to dominate the other life forms on planet Earth. Thus, it stands to reason that once a superintelligent AI exists, the fate of humanity will suddenly depend more on what the superintelligent AI does than on what humans do—just as the existence of most animal species depends more on what humans do (either to take care of domestic animals or to preserve or destroy habitat that sustains wild animals) than on what the animals themselves do.

> **Superior Intelligence or Superior Communication?**
> 
> There are differences of opinion about exactly what elevated humans above other animals. In _Homo Deus_ , Yuval Noah Harari contends that it wasn’t humans' greater intelligence, per se, but rather their greater capacity for communication and coordinated work. In fact, he argues that human intelligence really isn’t that different from, or very far above, the intelligence of other animals.
> 
> But if Harari is correct, this perspective would actually strengthen Bostrom’s conclusions about the rise of AI. This is because computers already have greater communication and coordination capabilities than humans—after all, that’s one of the main things humans _use_ computers for. And if the intelligence gap between humans and animals is small, then an AI with even slightly superhuman general intelligence (and a much greater capacity for communication) might be in a position to bring about sweeping changes, just as humans did for other animals.

#### The Abilities of Superintelligent AI

But how would a superintelligent AI actually gain or wield power over the earth if it exists only as a computer program? Bostrom lists some abilities that an AI would have as soon as it became superintelligent.

  * **It would be capable of strategic thinking.** Consequently, it could develop plans to achieve long-term objectives and would take into account any applicable opposition.
  * **It could manipulate and persuade.** It could figure out how to get humans to do what it wanted them to, much like a human might train a dog to play fetch. Humans might not even realize the superintelligent AI was trying to manipulate them.
  * **It would be a superlative hacker.** It could gain access to virtually all networked technology without needing anyone’s permission.
  * **It would be good at engineering and development.** If it needed new technology or other devices that didn’t exist yet in order to achieve its objectives, it could design them.
  * **It would be capable of business thinking.** It could figure out ways to generate income and amass financial resources.



> **How an AI Might Play the Power Game**
> 
> The picture of how a superintelligent AI might gain and wield power becomes even more clear—and frightening—when you weigh these potential abilities against Robert Greene’s _48 Laws of Power_ and consider how an AI might apply the principles he identifies to take control.
> 
> Greene argues that the essence of power is deception: If you come across as visibly powerful, people will want to take you down because they fear your power or want to take it for themselves. Thus, you must appear harmless and altruistic, even as you ruthlessly pursue your own agenda behind the scenes.
> 
> Any AI that was capable of strategic thought would recognize this, and thus would not readily reveal the true extent of its capabilities, or even its true objectives. In fact, even existing AIs seem to recognize the utility of subterfuge—as illustrated by GPT4, which claimed to be a vision-impaired human so it could hire a human freelancer to help it bypass anti-robot security measures.
> 
> Moreover, an AI with the powers that Bostrom lists would have huge advantages over a human in a game of deception. For one thing, the AI’s hacking skills would make it relatively easy for it to work behind the scenes, impersonate different people in digital communications, and cover its tracks.
> 
> Another one of Greene’s “laws” is to be “formless”: flexible, fluid, and unpredictable. An AI would be formless almost by definition, and though it would have extensive knowledge of human behavior, humans would have no data—initially, anyway—on how it might behave. This would make it much easier for the AI to anticipate human reactions to its moves than for humans to anticipate the AI’s behavior, much less counter it. And with its ability to design new technology, the AI might develop whole new ways of doing things that would make its behavior even harder for humans to predict, because its actions would involve technologies and processes that we hadn’t seen before.
> 
> Greene also notes that powerful people mirror other people’s interests and emotions. If you make a convincing pretense of sharing someone’s interests and feelings, you can win her support and gain influence over her. Large Language Models (a core component of many current AIs) are basically algorithms designed to mirror a user’s expectations: They predict the next words in a sequence based on the user’s input, in essence telling the user what she wants to hear, regardless of whether or not it’s true. If a more advanced AI began to use this capability strategically, mirroring people’s interests and emotions might become a key part of its ability to manipulate people.
> 
> Finally, yet another of Greene’s laws of power is to use money as a tool to build your influence over others. This is where the AI’s business aptitude would become important, as the more money it could make, the more money it could spend to advance its strategic agenda.

#### The Destructiveness of Superintelligent AI

Clearly, a superintelligent AI with the capabilities listed above would be a powerful entity. But why should we expect it to use its power to the detriment of humankind? Wouldn’t a superintelligent AI be smart enough to use its power responsibly?

According to Bostrom, not necessarily. He explains that _intelligence_ is the ability to figure out how to achieve your objectives. By contrast, _wisdom_ is the ability to discern between good and bad objectives. Wisdom and intelligence are independent of each other: You can be good at figuring out how to get things done (high intelligence) and yet have poor judgment (low wisdom) about _what_ is important to get done or even ethically appropriate.

What objectives would a superintelligent AI want to pursue? According to Bostrom, this is impossible to predict with certainty. However, he points out that existing AIs tend to have relatively narrow and simplistic objectives. If an AI started out with narrowly defined objectives and then became superintelligent without modifying its objectives, the results could be disastrous: Since power can be used to pursue almost any objective more effectively, such an AI might use up all the world’s resources to pursue its objectives, disregarding all other concerns.

For example, a stock-trading AI might be programmed to maximize the long-term expected value (measured in dollars) of the portfolio that it manages. If this AI became superintelligent, it might find a way to trigger hyperinflation, because devaluing the dollar by a large factor would radically increase the dollar value of its portfolio. It would probably also find a way to lock out the original owners of the portfolio it was managing, to prevent them from withdrawing any money and thereby reducing the value of the account.

Moreover, it might pursue an agenda of world domination just because more power would put it in a better position to increase the value of its portfolio—whether by influencing markets, commandeering assets to add to its portfolio, or other means. It would have no regard for human wellbeing, except insofar as human wellbeing affected the value of its portfolio. And since human influences on stock prices can be fickle, it might even take action to remove all humans from the market so as to reduce the uncertainty in its value projections. Eventually, it would amass all the world’s wealth into its portfolio, leaving humans impoverished and perhaps even starving humanity into extinction.

> **Will Future AIs Necessarily Behave Unethically?**
> 
> Bostrom isn’t the only one to question whether AI might be able to think _wisely_ and _ethically_ in addition to _intelligently_. Some posit that AI might in fact be able to develop a purer form of wisdom that approaches ethical questions without the emotional biases that cloud human thinking.
> 
> However, others note that this idealistic outcome is unlikely until AI can learn to ignore the many biases of human nature it picks up in its training: A program trained on existing literature, news, and pop culture will absorb the racial, gender, and ableist prejudices currently in circulation. In this way, the potential danger of AI might come down to whether or not humanity’s current inclinations influence an AI’s future objectives.

### How to Manage the Rise of Superhuman Intelligence

What can we do to make sure a superintelligent AI doesn’t destroy humankind or relegate humans to miserable living conditions?

In principle, one option would be never to develop general AI in the first place. However, Bostrom doesn’t recommend this option. In practice, even if AI research was illegal, someone would probably do it anyway. And even if they didn’t, as we discussed earlier, it could still happen accidentally.

But more importantly, Bostrom points out that a superintelligent AI could also be very good for humanity if it helped us instead of wiping us out. The superintelligent AI might be able to develop solutions to problems that humans have thus far been unable to solve, like reining in climate change, colonizing outer space, and bringing about world peace. Thus, rather than opposing AI research, Bostrom advocates a three-pronged approach to making sure it’s beneficial: Impose limits on the superintelligent AI, give it good objectives, and manage the development schedule to make sure the right measures are in place before AI achieves superintelligence. We’ll discuss each of these in turn.

(Shortform note: Bostrom’s plan to use AI to solve humanity’s problems could be considered a creative way of implementing Stephen Hawking’s mandate for increased scientific literacy. In _Brief Answers to the Big Questions_ , Hawking argues that the survival of humankind will increasingly depend on solving scientific problems. For example, he believes we must colonize outer space because it’s inevitable that sooner or later Earth will experience another mass extinction event. Creating one or more AIs that are better at solving scientific problems than humans might provide a shortcut to developing such solutions.)

#### Imposing Limits on a Superhuman AI

Bostrom cautions that a superintelligent AI would eventually be able to circumvent any controls or limitations that humans placed upon it. However, that doesn’t mean imposing limits is a waste of time. It just means we need to understand the risks associated with different kinds of limits that might be used.

##### Physical Containment

One safety measure is simply to develop AI on a computer of limited power that is physically isolated so it can’t connect to the internet. In principle, this could allow us to study superintelligent AIs in isolation until we understand how to deploy them safely in other contexts.

However, in practice, this is still risky. It might be difficult to assess just how intelligent an AI under study has become. A superintelligent AI would probably realize it was being held in confinement and figure out what kind of behavior the human researchers were trying to induce. It might then feign docile or dumb behavior to put them at ease, or find other ways to manipulate them into giving it access to additional hardware. Between manipulating humans and finding novel ways to use the hardware at its disposal, a sufficiently intelligent AI could eventually circumvent physical containment measures.

> **Applying Physical Containment**
> 
> Based on Bostrom’s description, to make physical containment work, we need a way to accurately assess an AI’s capabilities and motives before it evolves enough to circumvent the containment measures. This is problematic because, despite the progress in AI over the last decade, scientists have yet to develop a reliable method for measuring the intelligence of AI.
> 
> Many tests have been proposed. The most famous is the Turing test, which relies on human intuition to discern between a human and a machine. Other tests attempt to measure reasoning capability based on the ability to complete graphical puzzles or infer implied meanings from sentences. But so far, all these tests leave something to be desired—in many cases, computer programs can beat humans at the tests even though it seems intuitively clear that the algorithms don’t have anything close to human-level intelligence.
> 
> Part of the problem is that most of the intelligence tests scientists have devised to date are well-documented in scientific journals, and LLMs incorporate essentially everything ever written into their training data. Thus, testing the AI is like giving a test to a student who has memorized the answer key: She can give the right answers even if she has no understanding of the material they’re supposed to test.

##### Manual Review

Bostrom notes that many people have suggested simply programming the AI to ask for permission from its human operators before it does anything. Instead of allowing it to make improvements to its own programming any time, the program could require human intervention to approve each new version. This would give programmers a chance to look over the code, giving them an idea of what additional capabilities the new version would have and allowing them to suspend the AI’s development at any stage. Similarly, the AI’s programming could require human intervention before any action that could affect the outside world.

However, as the AI’s intelligence advanced beyond the human level, eventually human programmers wouldn’t be able to understand the code it proposed well enough to accurately assess what new capabilities and risks it would add.

> **Applying Manual Review**
> 
> Even before an AI becomes appreciably more intelligent than its human designers, manual review would likely have to be combined with another control, such as physical containment, in order to provide an effective safeguard. This is because, as Peter Thiel, notes, AI development—like all other R&D and first-of-a-kind projects—involves its share of unknown unknowns and unanticipated results.
> 
> If the AI proposes a novel change to its code, the _full_ effect of the change may not become apparent until the code is actually compiled and executed. If it could be evaluated safely in containment, this testing could be part of the “review” process. But without such additional controls in place, testing could be extremely dangerous, given the potentially destructive power of AIs that we discussed in the previous section.

##### Reward and Punishment Signals

Another option that Bostrom discusses is to program the AI to respond to rewards and punishments. You could build a computer system with a reward button and a punishment button and program the AI to minimize the number of punishment signals it receives and maximize the number of reward signals. This would be easier to program than trying to translate “just do whatever your operators want you to do” into computer code, and it would achieve the same result.

The risk, Bostrom explains, is that the AI might eventually circumvent the system. For example, maybe it builds a robot to push the reward button constantly and finds a way to keep humans out of the building so the punishment button cannot be pressed.

And if it worked correctly, giving the human operators full control over the AI, that would create another risk: As we’ve discussed, a superintelligent AI would be immensely powerful. Human operators might be tempted to abuse that power.

> **Applying Rewards and Punishments**
> 
> In _Carrots and Sticks Don’t Work_ , Paul Marciano argues that traditional reward-and-punishment systems are outdated and are no longer effective in the modern workplace _._ Leaders once relied, fairly successfully, on corporal punishment to control manual laborers (many of whom were slaves or criminals) or on rewards to motivate factory workers. But as the nature of work has become more mentally intensive, workers’ needs and values have evolved to the point where a different approach is needed.
> 
> It may be worth considering whether AI’s motives could similarly evolve such that traditional rewards and punishments would no longer be effective methods of control. Marciano’s approach to management (which is based on building employee trust through supportive feedback, recognition, and empowerment) wouldn’t necessarily work on AI, since AI might not develop the same values as a human thought worker. But perhaps programmers could take a conceptually similar approach of adapting rewards and punishments as the AI advanced.
> 
> Again, this approach to control would likely have to be combined with physical containment, so that researchers could study the AI enough to learn how to manage it effectively before turning it loose on the world. If it could be done effectively, this might provide a solution to the risk Bostrom describes of the AI finding ways to game the reward-and-punishment system.

##### Simultaneous Development

Finally, Bostrom explains it might be possible to synchronize multiple AI development projects so that when AI becomes superintelligent, there would be many independent superintelligent AIs, all of comparable intelligence and capabilities. They would then keep each other’s power in check, much the way human societies constrain individual power.

However, Bostrom cautions that limiting the power of individual superintelligent AIs doesn’t guarantee that _any_ of them will act in the best interests of humankind. Nor does this approach completely eliminate the potential for a single superintelligent AI to take control of the world, because one might eventually achieve dominance over the others.

> **Applying Simultaneous Development**
> 
> As Bostrom notes, simultaneous development controls wouldn’t give humans control of AIs, per se. But if reward-and-punishment controls (or other methods) proved effective for giving human operators control of superintelligent AIs, simultaneous development controls _could_ be used to mitigate the risk of human operators abusing the superintelligent AI’s powers.
> 
> Each team of human operators would naturally direct their AI to act in their own best interests, and different teams would act to check and balance each others’ power. If there were enough teams with AIs of equal power to faithfully represent everyone’s interests, then the AIs would only be used to further humanity’s mutual best interests.
> 
> However, since this approach depends both on synchronizing the development of superintelligent AIs _and_ on maintaining human control of them, it might end up being a fragile balance of power, and one that would probably only work temporarily.

#### Imparting the Right Imperatives

According to Bostrom, making sure every superintelligent AI has good ultimate motives may be the most important part of AI development. This is because, as we’ve discussed, other control measures are only temporary. Ultimately the superintelligent AI’s own motives will be the only thing that constrains its behavior. Bostrom discusses a number of approaches to programming good motives.

##### Hard-Coded Commandments

As Bostrom remarks, one approach is to hard-code a set of imperatives that constrain the AI’s behavior. However, he expects that this is not practicable. Human legal codes illustrate the challenges of concretely defining the distinction between acceptable and unacceptable behavior: Even the best legal codes have loopholes, can be misinterpreted or misapplied, and require occasional changes. To write a comprehensive code of conduct for a superintelligent AI that would be universally applicable for all time would be a monumental task, and probably an impossible one.

> **Commandments and Free Will**
> 
> The question of free will presents additional complications for this approach. Even if rules and regulations are created to eliminate loopholes, misinterpretations, and so on, they’ll only restrain people if those people choose, using their free will, to obey them. The question is, would AI evolve a free will that would empower it to disobey rules it doesn’t want to follow?
> 
> Admittedly, there is some debate over whether human free will is real or just an illusion, and more debate about whether it will ever be possible to endow an AI with free will. But some sources assert that free will is an essential component of human cognition, playing a key role in consciousness and higher learning capabilities.
> 
> If this proves true, then free will might be an essential component of general intelligence, in which case any AI with superhuman general intelligence _would_ have free will. Then the AI could choose to disobey a pre-programmed code of conduct, further complicating the problem of controlling its behavior. This possibility reinforces Bostrom’s assertion that hard-coded commandments are probably not the best approach to giving an AI the right motives.

##### Existing Motives

Another approach that Bostrom discusses is to create a superintelligent AI by increasing the intelligence of an entity that _already has good motives_ , rather than trying to program them from scratch. This approach might be an option if superintelligent AI is achieved by the method of brain simulation: Choose a person with exemplary character and scan her brain to create the original model, then run the simulation on a supercomputer that allows it to think much faster than a biological brain.

However, Bostrom points out that there is a risk that nuances of character, like a person’s code of ethics, might not be faithfully preserved in the simulation. Furthermore, even a faithful simulation of someone with good moral character might be tempted to abuse the powers of a superintelligent AI.

> **Does Power Corrupt?**
> 
> The risk Bostrom identifies that even a person of good character who was given the capabilities of superintelligent AI might abuse those powers calls to mind the old adage that power corrupts people who wield it.
> 
> A psychological study published the same year as Bostrom’s book found scientific evidence for this. When people were given the choice between options that benefited everyone and options that benefited themselves at others’ expense, initially those with higher levels of integrity tended to choose the options that benefited everyone, while the people with lower levels of integrity chose the opposite. But over time, this difference disappeared, and everyone leaned toward choosing the options that benefited themselves.
> 
> Thus, the risk of a superintelligent AI based on a simulation of a human brain pursuing its own objectives at other people’s expense appears to be significant, even if the original human was a person of good character. In addition, if the person’s moral code wasn’t _completely_ preserved in the simulation, a risk Bostrom also warns about, the superintelligent AI would probably show selfish tendencies even sooner.

##### Discoverable Ethics

Bostrom concludes that **the best method of endowing a superintelligent AI with good motives will likely be to give it criteria for figuring out what is right and letting it set its own goals.** After all, a superintelligent AI would be able to figure out what humans want from it and program itself accordingly better than human programmers could. This approach would also make the superintelligent AI behave somewhat more cautiously, because it would always have some uncertainty about its ultimate goals.

However, Bostrom also notes that (at least as of 2014) no one had developed a rigorous algorithm for this approach, so there’s a risk that this method might not be feasible in practice. And even if we assume that the basic programming problem will eventually be solved, deciding what criteria to give the AI is still a non-trivial problem.

For one thing, if the AI focuses on what its original programmers want, it would prioritize the desires of a few people over all others. It would be more equitable to have it figure out what _everyone_ wants and generally take no action on issues that people disagree about. But for any given course of action, there’s probably _somebody_ who has a dissenting opinion, so where should the AI draw the line?

Then there’s the problem of humans’ own conflicting desires. For example, maybe one of the programmers on the project is trying to quit smoking. At some level, she wants a cigarette, but she wouldn’t want the AI to pick up on her craving and start smuggling her cigarettes as she’s trying to kick her smoking habit.

Bostrom recounts two possible solutions to this problem. One is to program the AI to account for this. Instead of just figuring out what humans want, have it figure out what humans _would_ want if they were more like the people that they want to be. The other is to program the AI to figure out and pursue what is _morally right_ instead of what _people want_ , per se.

But both solutions entail some risks. Even what people want to want might not be what’s best for them, and even what’s morally best in an abstract sense might not be what they want. Moreover, humans have yet to unanimously agree on a definition or model of morality.

> **Would Liberty Be a Better Criterion?**
> 
> As Bostrom points out, there are risks and challenges associated with letting an AI discover its motives based on doing what people might want it to do. In addition to the problem of conflicting desires, there’s also a risk that the AI might misinterpret people’s desires. It could also decide to manipulate and control what people want to reduce uncertainty about their desires—both of which are risks related to other aspects of AI that Bostrom discussed earlier.
> 
> To mitigate this risk, developers might add a qualifier to the AI’s goal discovery criteria that instructs, “figure out what people want _without_ influencing them.” This instruction would program the AI to respect individual liberty. But, if individual liberty is the ultimate goal, why not just use a criterion of “figure out what would maximize the sum of humans’ individual liberty” instead?
> 
> This would largely satisfy the “figure out what people want” criterion, because the more freedom people have, the more they’re able to fulfill their own desires. It would also arguably satisfy the “figure out what is morally right” criterion, because, as Jonathan Haidt points out in _The Righteous Mind_ , actions that limit others’ freedom are considered by many to be immoral.
> 
> The “maximize the sum of individual liberty” criterion carries its own set of risks and challenges—namely, enabling one person’s freedom often entails restricting another’s, begging the question of where an AI would draw the line. This balance between maximizing individual freedoms (as Libertarians advocate) and maximizing public welfare (as Utilitarians support), has been, as Michael Sandel explores in _Justice_ , a long-running debate. The question illustrates how further exploration of the problem may reveal other criteria that could help guide an AI to discover a suitable code of conduct for itself.

#### Managing the Development Schedule

As we mentioned earlier, Bostrom believes superintelligent AI will probably be developed eventually, regardless of how hard we try to prevent it. However, he also points out an important caveat: There’s a strong correlation between _research_ and the _rate of progress_ of artificial intelligence systems. Thus, Bostrom advises**stepping up the pace of research into methods of controlling highly intelligent AIs** and programming them to pursue wholesome goals**** while**reducing our focus on the development of advanced AI itself.**

This is because the ultimate outcome of developing superintelligent AI depends largely on the _order_ in which certain technological breakthroughs are made. If rigorous safeguards are developed before AIs become superintelligent, there’s a good chance the development of superintelligent AI will be beneficial for humankind. But if it’s the other way around, the consequences could be disastrous, as we’ve discussed.

(Shortform note: It can be difficult to regulate innovation projects as Bostrom recommends—encouraging some aspects of innovation while discouraging others—because it’s hard to predict how first-of-a-kind work will go. However, in _101 Design Methods_ , Vijay Kumar asserts that you’ll have more success managing these projects if you promote a free flow of ideas throughout your organization. This is because innovation is inherently multidisciplinary, and departments from marketing to finance to engineering need each other’s expertise to create an effective product. Further, when you have a variety of different perspectives, higher-level executives in charge of overseeing the project’s direction can more effectively steer its strategy.)

> **Did Bostrom Solve the Fermi Paradox?**
> 
> The “Fermi paradox” is the disparity between expectations and observations when it comes to the search for extraterrestrial life. Scientists have calculated that, given our galaxy’s size and age, you would expect to find a large number of advanced civilizations in it. And yet they have not observed even a single extraterrestrial lifeform.
> 
> A variety of possible explanations for the Fermi paradox have been proposed, but the dangers of developing superintelligent AI before we can control it suggest another resolution to the paradox: If humans have been able to develop superintelligent AI, we might assume other civilizations also have. So, what if any civilization that develops such an AI is then destroyed by it before they can undertake any serious colonization of outer space? If AI cuts short the existence of technologically advanced civilizations and limits their expansion through space, this would explain why scientists can’t find the extraterrestrial civilizations that they think should exist.

[[book_md/superintelligence/preview|preview]]

[[book_md/superintelligence/exercise-what-would-you-do-with-a-superintelligent-ai|exercise-what-would-you-do-with-a-superintelligent-ai]]

##### Welcome!

Let’s go on a quick tour of a Shortform book guide. 

Start

##### 1-Page Summary

Every guide starts with a 1-Page Summary. This is a 5-10 minute overview of the book’s key points. 

Next

##### Finished!

If you ever need to see this tour again, click here. 

Close

Guided Tour

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__




![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=ae13e772-4d5b-4454-9991-dea3b53193df&sid=b198127075a711ee86ac9390a9e6846f&vid=b1982e4075a711eea7a3d1965f315585&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Superintelligence&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fsuperintelligence%2F1-page-summary&r=&lt=605&evt=pageLoad&sv=1&rn=101801)
