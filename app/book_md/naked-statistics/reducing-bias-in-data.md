![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Naked Statistics

Back to Discover

[[book_md/naked-statistics/preview|preview]]

  * [[book_md/naked-statistics|naked-statistics]]
  * Full Book Guide

    * [[book_md/naked-statistics/shortform-introduction|shortform-introduction]]
    * [[book_md/naked-statistics/why-learn-statistics|why-learn-statistics]]
    * [[book_md/naked-statistics/using-descriptive-statistics-to-describe-measures-of-central-tendency|using-descriptive-statistics-to-describe-measures-of-central-tendency]]
    * [[book_md/naked-statistics/using-descriptive-statistics-to-summarize-data-distribution|using-descriptive-statistics-to-summarize-data-distribution]]
    * [[book_md/naked-statistics/exercise-spot-misleading-statistics|exercise-spot-misleading-statistics]]
    * [[book_md/naked-statistics/using-probability-to-make-decisions|using-probability-to-make-decisions]]
    * [[book_md/naked-statistics/safe-assumptions-with-inferential-statistics|safe-assumptions-with-inferential-statistics]]
    * [[book_md/naked-statistics/finding-answers-with-regression-analysis|finding-answers-with-regression-analysis]]
    * [[book_md/naked-statistics/exercise-design-a-study|exercise-design-a-study]]
    * [[book_md/naked-statistics/quality-data|quality-data]]
    * [[book_md/naked-statistics/reducing-bias-in-data|reducing-bias-in-data]]
    * [[book_md/naked-statistics/exercise-examine-your-sources-of-bias|exercise-examine-your-sources-of-bias]]
  * [[book_md/naked-statistics/highlights|highlights]]
  * [[book_md/naked-statistics/community|community]]



Adding to Favorites 

Removing from Favorites 

## Reducing Bias in Data

Now that we've discussed some of the challenges and strategies for data _collection_ , we'll review Wheelan’s discussion of the reliability of our data itself.

One of the most important characteristics of reliable data is that it provides a true representation of the population we're studying. As we discussed in our section on inferential statistics, many research projects and statistical analyses rely on sampling as a way to learn about a larger population.**If the data collected in our sample doesn't accurately represent our population, then our resulting statistics will be unreliable.** Wheelan highlights two main ways to ensure a representative sample:

**Random Sampling:** A truly random sample is ideal for data collection. Random sampling allows us to be reasonably confident that we're capturing the diversity of the underlying population because any individual has as much chance as any other of being selected. Therefore, the diversity of the sample should be close to the diversity of the population. **When a sample accurately reflects the composition of its population, it's referred to as a “representative sample."**

**Large Sample Sizes:** The larger our sample size, the greater the likelihood that it will represent the underlying population. This is in part because **a larger sample size means more chances for the inclusion of diversity and in part because a larger sample reduces the influence of outliers.** As a rule, the larger the sample, the more reliable the statistics. However, Wheelan reminds us that this rule only applies to a truly random sample. A biased sample, big or small, will produce biased statistics.

> **Effect Size as a Source of Bias**
> 
> Analysis of close to 50,000 research studies across 22 fields of science has shown that much of the bias in published research stems from the misinterpretation or misrepresentation of a study’s effect size.
> 
> The effect size of a study is a measure of the difference in outcomes between the treatment and experimental groups. Statisticians argue that the effect size is just as, if not more important than the p-value (which tells us if a study is statistically significant) because a study can show very small differences between outcomes for treatment and control groups and still report statistically significant findings. As Wheelan explains, while the differences in outcomes may be mathematically significant, they may be negligible in the real world. Therefore, in addition to studying a large and random sample, one way to reduce bias in published research is to publish the effect size alongside measures of statistical significance.
> 
> A famous example of this phenomenon is the five-year study of 22,000 people that resulted in the recommendation that people take aspirin to prevent heart attacks. The p-value in the study was .00001, meaning that there was a .001% chance that the observed reduction in heart attack rates while taking aspirin was due to random chance. However, the generalized recommendation that people take aspirin to prevent heart attacks has since been modified because the effect size of the study was only 0.77%, with an R2 value of .001. Since this effect size was so small, the possible side effects of taking aspirin likely outweigh the benefits for many people.

### Sources of Bias

Biased data can sabotage otherwise sound research methods and statistical calculations. Sources of bias in data may be glaringly obvious or so subtle as to go unnoticed. If we want our data to be reliable, we should be aware of and take steps to mitigate potential sources of bias.

> **Reducing Bias Is a Researcher's Responsibility**
> 
> As individuals and as a society, we rely on scientific research to make informed decisions and understand the world around us. Therefore, researchers have a practical and ethical obligation to identify and address sources of bias in their research. Bias can make its way into a research project anywhere along the way, from the study’s conception to the framing of the research question, the data collection, the statistical analysis, the reporting of findings, and the study’s publication. Therefore, keeping bias out of research takes effort and attention, beginning with an awareness of the myriad sources of bias, both glaringly obvious and inadvertent.

Wheelan highlights the following common sources of bias in data:

**Selection bias:** Selection bias happens when our sample is not random, and certain subsets of the population are over- or underrepresented. Selection bias can be subtle. If researchers are not cognizant of selection bias when developing data collection methods, the fact that a sample is not truly random might go unnoticed.

For example: Say you wanted to collect data on people’s political leanings before an election, and you decided to collect your data at an art show outside of town. You might think that your sample was random because the art show was a public event, the crowd was a mix of people from different parts of town, and people of all ages were represented. However, it's likely that your data would be biased towards the opinions of wealthier residents because the people at the art show can afford the cars they used to drive out of town and the art for sale.

Selection bias can also happen when people are able to self-select into (or out of) a study. When we allow the people who feel strongly enough about a study to become the sample, our data is automatically skewed. For example, if you were to stand on the sidewalk with a banner promoting a local dog park and asked “random” people to take your survey, chances are that dog lovers strongly in favor of a dog park would be over-represented in your results, since they would take the time to come over.

> **The Modern Anti-Vax Movement**
> 
> Wheelan references the anti-vax movement as an example of the detrimental effects of a lack of data literacy. But the study that sparked the current anti-vax movement is also an example of problematic selection bias.
> 
> Andrew Wakefield’s 1998 study published in _The Lancet_ proposed an unfounded link between the MMR vaccine and autism, which promoted fear and suspicion among parents and largely precipitated the modern wave of anti-vaccination sentiment. Wakefield’s proposed link between vaccines and autism has been discredited in several research studies. However, the panic that his study caused remains.
> 
> In addition to Wakefield’s findings being discredited, his research methods have also drawn intense criticism. The study was problematic for several reasons, only two of which we'll cover. First, the sample size was 12 children; as Wheelan highlights throughout the book, large sample sizes increase reliability of results. Additionally, the families of those 12 children were recruited into the study through an anti-MMR vaccine campaign. As Wheelan explains, in addition to being large, reliable samples should be _random_ and representative. This precludes the self-selection of participants with a strong personal interest in the results of the study.

**Recall Bias** : Recall bias happens when we ask people to give us data on the effect of a treatment or event retroactively. The challenge with obtaining reliable data from the past is that memory is not static. Wheelan explains that when we try to recall data from the past, our memory will be influenced by the meaning and emphasis our mind has placed on the event. For example, a person who fails calculus in college might be more likely to report that they “have always hated math,” even if they enjoyed math classes in high school, because their negative experience in college calculus is affecting their memory of prior classes.

> **Information Bias**
> 
> Recall bias falls into a category of bias called information bias, which includes interviewer (also known as observer) bias. As Wheelan explains, in recall bias people’s own knowledge of their status (such as whether they have a certain disease) impacts their memory of events they associate with that status. Wheelan uses the example of people who receive a cancer diagnosis recalling unhealthy lifestyle choices (and likely forgetting the healthy ones).
> 
> Similarly, in interview or observer bias, the researcher’s knowledge of a person’s status impacts the data they collect on that person. For example, a researcher who knows a person has cancer may probe harder for memories of exposures associated with that type of cancer than they would for someone who is cancer-free. Differential data collection based on an observer’s knowledge of research subjects biases study results.
> 
> “Blinding” is a research term that can help address observer bias in clinical trials. It refers to the concealment of information from one or more individuals involved in a study. In a single-blind study, participants don’t know if they are receiving the treatment or a placebo. In a double-blind study, neither participants nor researchers know who is in the treatment or control groups. In a triple-blind study, neither participants nor researchers nor data analysts know who is in the treatment or control groups. Blinding observers and analysts can help ensure that all data is treated equally and can reduce information bias in research findings.

**Survivorship Bias:** Any time a portion of a study sample is able to “leave” the study, we should be wary of survivorship bias. Wheelan explains that survivorship bias happens when our sample consists of only those who remain at the end of a “treatment” or over a significant period of time.

For example, if you were an aerobics instructor, you might be interested in improving your instruction by collecting data on what people think of your class. If you collected data at the end of a class of regular clients, all of whom have been coming to class for a long time, your sample would be biased toward a positive response. Those who did not enjoy the class would have dropped out, and therefore would not be part of the sample. By sampling a class of regulars you’d be left with “survivors” who enjoy your instruction and likely have positive things to say.

> **Survivorship Bias and the Likelihood of Success**
> 
> In _Fooled by Randomness, _Nassim Nicholas Taleb introduces a different usage of the term “survivorship bias” with a similar consequence to the more traditional definition offered by Wheelan.
> 
> As Wheelan notes, survivorship bias can make groups of people or interventions appear more “successful” than they were. For example, if you collected data on the percent of D1 collegiate soccer players who go pro, you would have a skewed perception of how many soccer players actually make it to this level, since you haven’t included all of the people who dropped out of the sport between Pee Wee soccer camp and college.
> 
> Taleb offers a similar definition of the term outside the formal research context. He explains that it's often the stories of success that “survive” their way to public attention. For example, an aspiring actress who moves to Hollywood and survives on a shoestring budget only to eventually win an Oscar would be a memorable story that people would retell. Examples of wild success like this one make their way into the mainstream and skew people’s perceived likelihood of success, making outcomes like this seem more plausible than they really are. In both cases, perceptions of the statistical likelihood of an outcome are skewed.

**Healthy-User Bias:** Samples can be biased because of the types of people who engage in the treatment we're interested in studying. As Wheelan explains, the people who choose to engage in whichever activity or habit that we're collecting data on are likely to be different in significant ways from people who don't engage in the “treatment.” Therefore, isolating whether a treatment actually accounts for differences between individuals becomes challenging.

For example, say we were interested in studying the relationship between swimming and health in senior citizens. We would need to be cognizant of the fact that seniors who are swimming are likely different from their average peers outside of the pool as well. Seniors who are taking the time to swim likely take care of their health in other aspects of their life, such as their diet. Additionally, those seniors who are able to swim into old age are likely in better physical condition than their peers to begin with, hence their continued participation in sports.

> **Healthy User Bias in Heart Health**
> 
> A study published in the _British Medical Journal_ examined the effect of healthy user bias in evaluating the effectiveness of antihypertensive and lipid-lowering medications. Results showed that patients who took the medications were also more likely to have higher levels of income and more education, were more physically active, and were less likely to be smokers than patients who didn’t take the medications.
> 
> The authors of the study cite the importance of assessing medical interventions for healthy user bias because unstudied variables can make interventions seem more successful than they truly are. In this case, for instance, being more physically active and not smoking both have health benefits that could be boosting the drug-takers’ overall health.

**Publication Bias** : Publication bias has less to do with data in an individual study and more to do with the overall representation of data or an idea in public awareness. Wheelan explains that a large portion of statistics-based research is never published. This is in part, he explains, because “negative findings” don't make for attention-grabbing headlines. For example, people might not go out of their way to read a paper entitled “Wearing Skinny Jeans Has No Impact On Your Health.”

People are more interested in attention-grabbing headlines, such as a hypothetical: “Are Your Skinny Jeans Killing You? Research Finds Link Between Skinny Jeans and Risk of Heart Attack!”

Wheelan explains that publication bias can lead to misconceptions and inflated confidence in research findings because people never get to see corresponding studies that might negate or temper published research results. For example, suppose a hypothetical study found a small positive correlation between owning a dog and lower rates of a certain type of cancer. In that case, people might rush to adopt a dog without ever knowing that five other studies found no such link.

To combat publication bias, Wheelan explains that medical journals may enforce a policy that all studies on a particular research question be reported, not just the positive ones.

> **Positive Headlines Grab Public Attention**
> 
> As Wheelan explains, researchers and publishers may be more inclined to share positive research findings because they make for more attention-grabbing headlines. Additionally, even when negative research findings do find their way to publication, they can be overshadowed by positive findings even if they present more reliable data.
> 
> For example, the CDC notes that the agency either funded or conducted nine studies between 1999 and 2010 that showed “no link” between childhood vaccinations and autism. Despite the public availability of these research results, the public’s focus remains on the single “positive finding” between research and autism published in 1998, even though this study has been thoroughly discredited.

Eliminating all sources of bias in a sample may not be feasible. But **the more aware we are of bias in our data and the challenges of data collection, the better we'll be able to produce reliable statistics** and add context to the statistics produced by others.

[[book_md/naked-statistics/quality-data|quality-data]]

[[book_md/naked-statistics/exercise-examine-your-sources-of-bias|exercise-examine-your-sources-of-bias]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=8ac364b5-1cfb-4dbb-88eb-a77f92de2f6f&sid=f30c5e70639211ee87d33f0876d93783&vid=f30c9700639211eeb3a75d830392c94f&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fnaked-statistics%2Freducing-bias-in-data&r=&lt=471&evt=pageLoad&sv=1&rn=267570)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



