![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# Thinking in Systems

Back to Discover

[[book_md/thinking-in-systems/preview|preview]]

  * [[book_md/thinking-in-systems|thinking-in-systems]]
  * Full Book Guide

    * [[book_md/thinking-in-systems/introduction|introduction]]
    * [[book_md/thinking-in-systems/part-1|part-1]]
    * [[book_md/thinking-in-systems/exercise-define-a-system|exercise-define-a-system]]
    * [[book_md/thinking-in-systems/chapter-1-2|chapter-1-2]]
    * [[book_md/thinking-in-systems/exercise-feedback-loops|exercise-feedback-loops]]
    * [[book_md/thinking-in-systems/chapter-2-1|chapter-2-1]]
    * [[book_md/thinking-in-systems/chapter-2-2|chapter-2-2]]
    * [[book_md/thinking-in-systems/part-2|part-2]]
    * [[book_md/thinking-in-systems/exercise-improve-your-system|exercise-improve-your-system]]
    * [[book_md/thinking-in-systems/chapter-4|chapter-4]]
    * [[book_md/thinking-in-systems/chapter-5|chapter-5]]
    * [[book_md/thinking-in-systems/part-3|part-3]]
    * [[book_md/thinking-in-systems/chapter-7|chapter-7]]
  * [[book_md/thinking-in-systems/highlights|highlights]]
  * [[book_md/thinking-in-systems/community|community]]



Adding to Favorites 

Removing from Favorites 

## Chapter 4: Why We Don’t Understand Systems

We try to understand systems to predict their behavior and know how best to change them. However, we’re often surprised by how differently a system behaves than we expected. Systems thinking is counter-intuitive in many ways, even for trained systems thinkers.

At the core of this confusion is our limitation in comprehension. **Our brains prefer simplicity and can only handle so much complexity.** That prevents us from seeing things as they really are.

This chapter discusses a collection of such limitations. The underlying themes are:

  * Our cognitive biases color how we take in information.
  * We tend to focus on obvious points, ignoring the more subtle and complex drivers that really matter.
  * Systems often behave in ways that we’re not used to, such as changing nonlinearly or inducing delays.



### Limitation #1: Focusing on Events

When we try to understand systems, we tend to see them as a series of events.

  * History is presented as a series of factual events, such as presidential elections, wars, and treaties.
  * The news reports on stock market movements with great earnestness.



While events are entertaining, they’re not useful for _understanding_ the system. **Events are merely the output of a system** , and are often the system’s most visible aspects. But events often don’t provide clarity into how the system works or the system structure, which means they don’t help you predict how the system will behave in the future or tell you how to change the system.

In fact, an event-driven view of the world is entertaining precisely _because_ it has no predictive value. If people could merely study daily events and predict the future, the news would lose its novelty and stop being fascinating.

#### The Better Approach

Instead of individual events, the better way to understand a system is to **see its performance over time** , and to see patterns of behavior. Look for historical data and trends, and plot graphs over time.

Understanding how the system behaves over time can help you deduce the structure of the system, which can in turn help you predict the behavior into the future.

(Shortform examples:

  * If you looked at the performance of the stock market over time, you might see that it’s increased by double-digit percentages over the past three years. In this context, the daily small movements have little importance. Instead, you can focus on understanding the economic system that causes it to grow steadily over time, despite a bevy of world events.
  * If you looked at a football team’s performance over time, instead of focusing on individual games, you may better understand the system of how the team works—how players are recruited and trained, the influence of coaching, and how the team is managed by its owners.)



### Limitation #2: Ignoring Nonlinearities

In our daily lives, we see the world act linearly. There is a straight-line relationship between two things. For example:

  * An object that is twice as heavy to move requires twice as hard a push to move it.
  * If you earn a salary, your bank account increases the same amount each month you work.
  * If it takes one hour to read a chapter of a book, it takes two hours to read two chapters.



However, much of the world doesn’t act linearly. Two elements can’t be related on a straight line. For example:

  * When a new product is released, a small amount of advertising may pique your interest. In contrast, too much aggressive advertising makes you annoyed at the ad, and you actively avoid buying the product.
  * On an empty highway, a single car can move at the speed limit without any problems. Add a few more cars, and the average speed doesn’t change much. This continues over a wide range of car density. But at a certain point, adding more cars causes traffic and slows down the average speed considerably. Add a few more cars, and traffic can come to a total stop.
  * A viral infection can simmer in a low number of cases for some time, then explode exponentially.



Nonlinearities often exist as a result of feedback loops. As we learned, a reinforcing feedback loop can lead to exponential growth. Even more complex nonlinearities can result when an action changes the _relative_ strength of feedback loops in the system—thus, the system flips from one pattern of behavior to another.

#### The Better Approach

Recognize that nonlinearities can exist.

Be wary of extrapolating from a small part of the curve out linearly.

  * If a small amount of something yields good results, adding more of it doesn’t always improve the situation; it might even cause harm.
    * A little bit of fertilizer can nurture a healthy sprouting plant. Too much fertilizer can poison the plant.
  * If a small amount of something causes only negligible harm, increasing it might cause devastating results.
    * Fishermen can overfish a small amount, and the fish population will recover without much of a problem. But past a certain amount of overfishing, the fish population may collapse and go completely extinct.



### Limitation #3: Oversimplifying System Boundaries

When we studied stock-and-flow diagrams above, we represented the inflows and outflows as clouds. These mark the boundaries of the system we’re studying.

We cap systems to simplify them. This is partially because we can only tolerate so much complexity, but also because too many extra details can obscure the main question. When thinking about a fishing population, it would be confusing to have to model the entire world economy and predict how a football team’s performance might work its way down to fishing behavior.

However, **we tend to oversimplify systems. We draw narrow boundaries that cause us to ignore elements that materially affect the system.** This can cause the system to behave against our expectations, because we had an incomplete model of the system. For example:

  * A highway designer that ignores how people settle in a city will build a highway of a certain capacity. She may ignore that people tend to settle along a highway, which now makes it insufficient for the traffic to support.
  * A car manufacturer might only concern itself with buying parts from its supplier, ignoring how the supplier itself operates and gets its materials. If a global aluminum shortage happened, the car manufacturer would be unpleasantly surprised.



We also tend to draw natural boundaries that are irrelevant to the problem at hand. We draw boundaries between nations, between rich and poor, between age groups, between management and workers, when in reality we want happiness and prosperity for all. These boundaries can distort our view of the system.

(Shortform note: A hierarchy, discussed in the previous chapter, naturally introduces system boundaries. A subsystem is largely concerned with how it operates and can ignore the operations of other subsystems. This is one of the virtues of hierarchy that leads to efficiency, but it can also prevent a subsystem from perceiving the whole system accurately. For example, a corporation’s product development team may happily work on developing its new products, ignoring that the accounting department is engaging in fraud that will cause the entire company to collapse.)

#### The Better Approach

The ideal balance is to **set the system boundaries at the appropriate scope for the question at hand**. Don’t exclude anything that’s important, and don’t include anything irrelevant.

Furthermore, **when you move to a new question, readjust the boundaries to be appropriate for the new question**. Don’t be stuck to your old boundaries.

  * The boundary of a river may be logical for drawing the boundary between nations, but it’s a terrible boundary for managing water quality. 



### Limitation #4: Ignoring Limits

In this world, nothing physical can grow without limit. At some point, something will constrain that growth. However, we naturally tend to avoid thinking about limits, especially when they involve our hopes and aspirations.

Consider a typical company that sells a product. At all points, it depends on a variety of inputs:

  * Money
  * Labor
  * Raw materials
  * Machines to produce its product
  * Marketing and customer demand
  * Order fulfillment systems



At any point, one of these factors can limit the growth. No matter how much you supply of the other inputs, the output stays the same; the limit is constraining the growth of the system. For example, the company may have more than enough labor and machines to produce the product, but it might lack the raw materials to make full use of the other inputs. (Shortform note: This limit is often called a “bottleneck.”)

We make two major mistakes around ignoring limits:

**Mistake 1: Misidentifying the Limit**

At times, we may simply incorrectly diagnose what the limit is. We may supply more of other inputs, then be confused about why the system hasn’t improved its output.

For example, an NGO may send aid as money to a developing country, thinking that capital is the limiting factor in the country’s growth. In reality, the limiting factor might be a corrupt government, a weak legal system, or human capital.

**Mistake 2: Ignoring that the Limit Changes**

In the process of solving one limiting factor, a new limiting factor may appear. This requires the intervention to change over time; doing more of the same won’t cause the system to grow.

For example, a company may have so much business that its limiting factor is having enough workers to keep up with demand. It hires rapidly, but the new workers aren’t trained well, and the product quality suffers. Customer demand drops, and management creates new worker training programs. Customer demand improves again, but the new bottleneck becomes its order fulfillment systems, which break under the heavy load.

This iterative process of finding the limit and relieving it may continue until the company reaches a more intractable limiting factor, such as saturating the customer demand for its product.

#### The Better Approach

To find a system’s limit, consider all the relevant inputs into the system. Then find the input that is most limiting the system’s performance, and relieve it.

Be aware that the system won’t be able to grow forever, so make peace with what limits you are willing to live with. For example, a national economy may decide that it’s willing to grow only at a pace that reduces environmental consequences and social inequality.

### Limitation #5: Ignoring Delays

As we saw in the car dealership example, delays happen at each step of a flow:

  * Delays in perceiving information
  * Delays in reacting to information
  * Delays for the reaction to cause a change



We tend to underestimate delays as a habit. You’ve had personal experience with this when a product took far longer than you expected, even if you tried to anticipate that delay.

Delays can cause surprising behavior with significant ramifications for society:

  * In understanding a disease, there is a delay between being exposed to a disease and showing symptoms. (Shortform note: This was relevant during the COVID-19 pandemic in 2020, when a long incubation period caused undetectable transmission.)
  * In the growth of an economy, there is a delay between pollution being emitted, when the pollution concentrates enough to cause noticeable problems, and a delay to perceiving these problems. Then there is a delay to changing the system to reduce pollution.
  * In policy, there is a delay between signing a new law and it having an impact on the economy. This can take years or decades.



**Delays cause us to make decisions with imperfect information.** We may be perceiving information that is too old. In turn, our actions may overshoot or undershoot what the system needs. Like the manager of the car lot, this can lead to oscillations.

#### The Better Approach

The author shares a rule of thumb when estimating delays: predict the delay to the best of your ability, then multiply that by 3.

When exposed to delays, recognize that your decisions will overshoot or undershoot.

Delays are often places to intervene in a system. In some cases, reducing the response delay can increase the accuracy of the actions. However, beware of the tendency to overreact to information—remember the car inventory lot, where acting more aggressively to shortages caused more pronounced oscillations.

### Limitation #6: Bounded Rationality

We can only make decisions with the information we comprehend accurately.

First, sometimes the information just doesn’t exist. Especially if it’s far removed from us.

  * We don’t know what others will do.
  * We don’t foresee how our actions will affect the system.



Second, even if we had total information, we’re limited in the information we can take in and digest.

Third, even the information we can take in is biased.

  * We overweigh the present and discount the past, we are biased to information that confirms our beliefs.
  * (Shortform note: Read more about cognitive biases in _Thinking, Fast and Slow_ and _Influence_.)



It’s small wonder then that we can act with good intentions but cause bad results.

  * A fisherman is thinking about the loan on his boat, his newborn child, the risk of injury that might jeopardize his career. He doesn’t have information about the global stock of fish. So he tends to overfish.



#### The Better Approach

Don’t blame an individual for the behavior. If you were put in that situation, with exactly the information they had and their preferences, you would probably behave the same.

The way to fix a system is not to put in new people, but to change the system.

[[book_md/thinking-in-systems/exercise-improve-your-system|exercise-improve-your-system]]

[[book_md/thinking-in-systems/chapter-5|chapter-5]]

![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=ba74a03b-73e3-4b4e-945b-a5afcbb946b0&sid=48a964a0642711eeb2d9b36fc717f5e2&vid=48a9a1e0642711eebeaf23361361f0d4&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20Book&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fthinking-in-systems%2Fchapter-4&r=&lt=965&evt=pageLoad&sv=1&rn=877518)

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__



