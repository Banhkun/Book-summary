![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

Discover

Books

Articles

My library

Search

Discover

![Shortform App](/img/logo.36a2399e.svg)![Shortform App](/img/logo-dark.70c1b072.svg)

# The Signal and the Noise

Back to Discover

[[book_md/the-signal-and-the-noise/preview|preview]]

  * [[book_md/the-signal-and-the-noise|the-signal-and-the-noise]]
  * Full Book Guide

    * [[book_md/the-signal-and-the-noise/exercise-improve-your-decisions|exercise-improve-your-decisions]]
  * [[book_md/the-signal-and-the-noise/highlights|highlights]]
  * [[book_md/the-signal-and-the-noise/community|community]]



![](/img/tutorial-fonts.175b2111.svg)

##### Change text options

Here you can change the font, text size, and reading screen to just how you like it. 

Next

  *   *   *   *   * 


![](/img/tutorial-menu.4c76dd27.svg)

##### Table of contents

Here you’ll find everything else, including the full chapter-by-chapter guide, your highlights, PDF downloads, and book discussions. 

Next

  *   *   *   *   * 


![](/img/tutorial-player.d25b1afb.svg)

##### Audio

Every guide has an audio narration so you can listen on the go. 

Next

  *   *   *   *   * 


![](/img/tutorial-favorite.b948300a.svg)

##### Add to Favorite

Mark your favorite guides here. You can find your favorites on your homepage. 

Next

  *   *   *   *   * 


![](/img/tutorial-night.ddd7fb5c.svg)

##### Night Mode

Like a darker look when you read? Turn dark mode on here. 

Finish

  *   *   *   *   * 


Adding to Favorites 

Removing from Favorites 

## 1-Page Summary

The early 21st century has already seen numerous catastrophic failures of prediction: From terrorist attacks to financial crises to natural disasters to political upheaval, we routinely seem unable to predict the events that change the world. In _The Signal and the Noise_ , statistician and analyst Nate Silver sets out to explain why our predictions typically fail and how we can do better.

According to Silver, prediction depends heavily on detecting a _signal_ —relevant information—amidst a sea of _noise_ —irrelevant or misleading information. Most of the time, he says, our predictions falter because mental mistakes such as incorrect assumptions, overconfidence, bias, and warped incentives cause us to mistake noise for signal. However, he also suggests that we can mitigate these thinking errors (and thus make better predictions) with the help of a method called Bayesian inference.

Silver is the creator of _FiveThirtyEight_ , a political analysis website known for its track record of accurate forecasts of US elections. He’s also worked as a professional poker player and a baseball analyst—during which time he developed a well-respected system (PECOTA) for predicting players’ future performances.

In _The Signal and the Noise_ , Silver explores a wide range of fields that depend on predictions, from politics, poker and baseball to economics, meteorology, and military intelligence. Though the book focuses mostly on these sorts of high-stakes examples, Silver suggests that its lessons are valuable for all readers—after all, decisions as mundane as what to wear on a given day are, in essence, predictions (for example, about the weather).

The book was originally published in 2012, though our guide is based on the updated 2020 version which includes a new preface discussing the 2016 US Presidential election and the early stages of the Covid-19 pandemic.

This guide is organized into four parts.

  * Part 1 explores why prediction is inherently difficult. 
  * Part 2 covers the mental errors that Silver says make our predictions worse than they could be. 
  * Part 3 explains what Bayesian logic is and how it can help us make better predictions. 
  * Part 4 introduces a brief caveat about the limitations of predictions in our current climate of fractured trust and political polarization. 



Throughout the guide, we’ll update Silver’s examples with the latest research and place his arguments in conversation with other experts on prediction such as Philip E. Tetlock and Daniel Kahneman.

### Part 1: Why Prediction Is Hard

Before we discuss our prediction mistakes and Silver’s advice for how to avoid them, it’s worth acknowledging that even under the best circumstances, prediction is an inherently challenging endeavor. In this section, we’ll explore Silver’s analysis of how insufficient information, system complexity, and the tendency of small errors to compound all limit the accuracy of our predictions.

#### We Often Lack Sufficient Information

To make good predictions, Silver says, **you need information about the phenomenon you’re trying to predict as well as a good understanding of how that phenomenon works**. For example, today’s meteorologists have abundant information about atmospheric conditions as well as a good understanding of the physical laws by which those conditions develop. Accordingly, Silver says, they can make reasonably accurate predictions about the weather.

(Shortform note: Though Silver regards meteorology as a relatively successful field when it comes to predictions, some scientists suggest that climate change may have affected the accuracy of weather forecasts since _The Signal and the Noise_ was published in 2012. Specifically, one study suggests that shifting temperature patterns have made it harder to predict summer precipitation—which could in turn make it harder to predict catastrophic floods such as those that afflicted large portions of Europe in 2021.)

The problem is that in many situations, **we don’t have much useful data to go on**. Silver illustrates this point by comparing meteorology to seismology, explaining that seismologists have no way to predict the timing, location, or strength of specific earthquakes. That’s because earthquakes happen rarely and on a geological timescale, which makes it hard to discern any patterns that might be at play. And whereas meteorologists can directly observe atmospheric conditions, there’s no way to measure factors such as the pressures currently at work on a given fault line nor to foresee future tectonic activity.

(Shortform note: Seismology also highlights a related problem—sometimes we’re not even sure how to interpret the data we do have because we don’t have a sufficient _theory_ about how a given phenomenon works. For example, though geologists agree that earthquakes result from ruptures along tectonic fault lines, they’re still not sure why some ruptures release energy more quickly (and dangerously) than others. So even if we suddenly had much more _information_ about earthquakes and the forces that drive them, it might take a while to develop a workable theory that could turn that information into reliable predictions.)

#### Many Systems Are Too Complex to Fully Understand

Also, Silver argues that even when we have rich data and a good understanding of the underlying principles, many systems still defy accurate prediction because of their inherent complexity. Silver discusses economic forecasts—which he says are notoriously unreliable—to lay out several ways complex systems hinder forecasting:

  * **They blur cause and effect** , making it hard to determine root causes. For example, unemployment leads to a lack of consumer demand, which causes companies to scale down production and cut their workforces—which raises unemployment.
  * **They contain feedback loops** that complicate their behaviors. For instance, employers, consumers, and politicians all make decisions based on economic forecasts, which means that the forecasts themselves can change the outcomes they’re meant to predict.
  * **Their rules and current status are unclear**. Silver points out that economists don’t agree on what indicators might predict recessions; moreover, they often don’t realize when the economy is _already_ in a recession (only recognizing that after the fact).



(Shortform note: With online banking and digital payments becoming ever more common, some experts argue that economic forecasters may be starting to improve on these weaknesses. Since the Covid-19 pandemic, economists have relied more heavily on real-time data to glean an up-to-the-minute understanding of factors such as consumer behaviors and supply chain delays. This improved understanding may help them to better discern the current state of the economy and to untangle the complicated causal and feedback loops Silver describes.)

#### Small Errors Compound

Finally, Silver explains that even when you have good data and you’re not dealing with an inherently unpredictable system like the economy, your predictions are still limited by the fact that even **minuscule mistakes in the predictive process compound into large errors over time**. Silver argues that this principle of compounding error is at work even in fields with relatively accurate predictions, such as meteorology. Because each day’s weather depends on the previous day’s conditions, even the smallest inaccuracy in the initial data you plug into a simulation will add up to big discrepancies over time. Silver says that’s why weather forecasts are generally accurate a few days out but become increasingly unreliable the further into the future they try to predict.

(Shortform note: In fact, the current scientific consensus suggests that perfect long-range predictions may never be possible in any field. That’s because, as we’ve discussed, accurate predictions rely on accurate theory and accurate observations—and there may be a fundamental limitation on the latter. In _A Brief History of Time_ , Stephen Hawking explains that the nature of physics guarantees a degree of uncertainty in our measurements, which means that even if we had a perfect theory of physics (which we don’t), we’d _still_ be unable to perfectly predict the future because we’d be unable to accurately measure the current conditions—a fact that, as Silver explains, would lead to compounding errors down the line.)

### Part 2: Why Our Predictions Are Worse Than They Could Be

So far, we’ve discussed challenges that are inherent to prediction—but according to Silver, these challenges don’t tell the whole story. Instead, he argues, we exacerbate these challenges through a series of mental errors that make our predictions even less accurate. In this section, we’ll examine these mental errors, which include making faulty assumptions, being overconfident, trusting data and technology too readily, seeing what we want to see, and following the wrong incentives.

#### We Make Faulty Assumptions

As we’ve seen, our predictions tend to go awry when we don’t have enough information or a clear enough understanding of how to interpret our information. This problem gets even worse, Silver says, when we assume that we know more than we actually do. He argues that **we seldom recognize when we’re dealing with the unknown because our brains tend to make faulty assumptions based on analogies to what we _do_ know**.

(Shortform note: In _Thinking, Fast and Slow_ , Daniel Kahneman explains that the brain uses these analogies to conserve energy by substituting an easier problem in place of a hard one. For example, determining whether a stock is a good investment is a complicated question that requires research, analysis, and reflection. Left to its own devices, your brain is likely to skip all that work and instead answer an easier question such as whether you have a good feeling about the company or whether the stock has done well recently. According to Kahneman, you can overcome this quick, associative reasoning by deliberately engaging your brain’s slower, more analytical faculties.)

To illustrate the danger of assumptions, Silver says that the 2008 financial crisis resulted in part from two flawed assumptions made by ratings agencies who gave their highest ratings to financial products called CDOs that depended on mortgages not defaulting.

  * The first assumption was that these complicated new products were analogous to previous ones the agencies had rated. In fact, Silver suggests, the new products bore little resemblance to previous ones in terms of risk.
  * The second assumption was that CDOs carried low risk because there was little chance of widespread mortgage defaults—even though these products were backed by poor-quality mortgages issued during a housing bubble.



Silver argues that these assumptions exacerbated a bad situation because they created the illusion that these products were safe investments when in fact, no one really understood their actual risk.

(Shortform note: These assumptions also highlight an especially pernicious thinking error called _availability bias_. In _Thinking, Fast and Slow_ , Kahneman explains that availability bias occurs when we misjudge an event’s likelihood based on how readily we can think of examples of that event. This error is, in part, what led to the assumption that CDOs were safe. As Michael Lewis explains in _The Big Short_ , investors and analysts repeatedly reasoned that because mortgages hadn’t tended to default recently, they were unlikely to do so going forward—even though the CDO bubble was created by an unprecedented laxity in lending practices.)

#### We’re Overconfident

According to Silver, the same faulty assumptions that make our predictions less accurate also make us too confident in how good these predictions are. One dangerous consequence of this overconfidence is that **by overestimating our certainty, we underestimate our risk**. Silver argues that we can easily make grievous mistakes when we _think_ we know the odds but actually don’t. You probably wouldn’t bet anything on a hand of cards if you didn’t know the rules of the game—you’d realize that your complete lack of certainty would make any bet too risky. But if you _misunderstood_ the rules of the game (say you mistakenly believed that three of a kind is the strongest hand in poker), you might make an extremely risky bet while thinking it’s safe.

(Shortform note: One way to avoid this overconfidence trap is to literally think of our decisions as bets. In _Thinking in Bets_ , former professional poker player Annie Duke argues that most of the decisions we make carry some degree of uncertainty and therefore risk—after all, we rarely have complete information about any given situation. She argues that, as a result, making a good decision isn’t about picking the “right” choice or avoiding a “wrong” choice: It’s a matter of determining which option is most likely to give you the outcome you want. When you think this way, you can act knowing that you’ve given yourself the best chance of success—but that success isn’t a sure thing.)

Silver further argues that **it’s easy to become overconfident if you assume a prediction is accurate just because it’s precise**. With information more readily available than ever before and with computers to help us run detailed calculations and simulations, we can produce extremely detailed estimates that don’t necessarily bear any relation to reality, but whose numerical sophistication might fool forecasters and their audiences into thinking they’re valid. Silver argues that this happened in the 2008 financial crisis, when financial agencies presented calculations whose multiple decimal places obscured the fundamental unsoundness of their predictive methodologies.

(Shortform note: Whereas Silver argues specifically that _precise_ predictions inspire overconfidence, in _The Black Swan_ , Nassim Nicholas Taleb argues that numerical projections of any kind—even imprecise ones—tend to lead to (and reflect) overconfidence because random events make most predictions fundamentally unreliable given enough time. For example, Taleb cites forecasts from 1970 that predicted that oil prices would stay the same or decline in the next decade. In reality, events like the Yom Kippur War (1973) and the Iranian Revolution (1979) caused unprecedented oil price spikes during that period. Because it’s impossible to predict events like these, Taleb suggests that we should have very little confidence in predictions in general.)

#### We Trust Too Much in Data and Technology

As noted earlier, one of the challenges of making good predictions is a lack of information. By extension, the _more_ information we have, the better our predictions should be, at least in theory. By that reasoning, today’s technology should be a boon to predictors—we have more data than ever before, and thanks to computers, we can process that data in ways that would have been impractical or impossible in the past. However, Silver argues that data and computers both present their own unique problems that can hinder predictions as much as they help them.

For one thing, Silver says, having more data doesn’t _inherently_ improve our predictions. He argues that **as the total amount of available data has increased, the amount of signal (the useful data) hasn’t** —in other words, the proliferation of data means that there’s more noise (irrelevant or misleading data) to wade through to find the signal. At worst, this proportional increase in noise can lead to convincingly precise yet faulty predictions when forecasters inadvertently build theories that fit the noise rather than the signal.

(Shortform note: In _Everybody Lies_ , data scientist Seth Stephens-Davidowitz goes into more detail about why too much data can actually be a bad thing. He explains that the more data points you have, the greater your chances of detecting a false signal—an apparent correlation that occurs due to chance. For example, if you flip 1,000 coins every day for two years, record the results for each coin, and record whether the stock market went up or down each day, it’s possible that through pure luck, one of the coins will appear to “predict” the stock market. And the more coins you flip, the likelier you are to find one of these “lucky” coins because more coins means more chances to generate randomness that seems predictive.)

Likewise, Silver warns that **computers can lead us to baseless conclusions when we overestimate their capabilities or forget their limitations**. Silver gives the example of the 1997 chess series between grandmaster Garry Kasparov and IBM’s supercomputer Deep Blue. Late in one of the matches, Deep Blue made a move that seemed to have little purpose, and afterward, Kasparov became convinced that the computer must have had far more processing and creative power than he’d thought—after all, he assumed, it must have had a good reason for the move. But in fact, Silver says, IBM eventually revealed that the move resulted from a bug: Deep Blue got stuck, so it picked a move at random.

According to Silver, Kasparov’s overestimation of Deep Blue was costly: He lost his composure and, as a result, the series. To avoid making similar errors, Silver says you should **always keep in mind the respective strengths and weaknesses of humans and machines** : Computers are good at brute-force calculations, which they perform consistently and accurately, and they excel at solving finite, well-defined, short- to mid-range problems (like a chess game). On the other hand, Silver says, computers aren’t flexible or creative, and they struggle with large, open-ended problems. Conversely, humans can do several things computers can’t: We can think abstractly, form high-level strategies, and see the bigger picture.

> **Machine Intelligence Revisited**
> 
> With the proliferation of large language model (LLM) AIs such as ChatGPT and Bing Chat, computing has changed radically since _The Signal and the Noise_ ’s publication—and as a result, it’s easier than ever to forget the limitations of computer intelligence. LLMs are capable of complex and flexible behaviors such as holding long conversations, conducting research, and generating text and computer code, all in response to simple prompts given in ordinary language.
> 
> In fact, these AIs can seem so lifelike that in 2022, a Google engineer became convinced that an AI he was developing was sentient. Other experts dismissed these claims—though the engineer’s interpretation was perhaps understandable in light of interactions in which AIs have declared their humanity, professed their love for users, threatened users, and fantasized about world domination.
> 
> Given these surprisingly human-like behaviors, we might be left wondering, like Kasparov with Deep Blue, whether these programs can actually _think_. According to AI experts, they can’t: Though LLMs can effectively emulate human language, they do so not through a deep understanding of meaning but through an elaborate matching operation. In essence, when you enter a query, the program consults a massive database to find statistically likely words and phrases to respond with. (Some observers have likened this to the Chinese Room thought experiment, in which a person with no understanding of Chinese conducts a written exchange in that language by copying the appropriate symbols according to instructions.)
> 
> That’s why, despite AI’s impressive advances, many experts recommend that the best way forward isn’t to replace people with AI—it’s to combine the two, much as Silver recommends, in order to maximize the strengths of both. Interestingly, Kasparov himself promotes this approach, arguing that the teamwork between specific humans and specific machines is more important than the individual capabilities of each.

#### Our Predictions Reflect Our Biases

Another challenge of prediction is that **many forecasters have a tendency to see what they want or expect to see in a given set of data—and these types of forecasters are often the most influential ones**. Silver draws on research by Phillip E. Tetlock (author of _Superforecasting_) that identifies two opposing types of thinkers: hedgehogs and foxes.

  * _Hedgehogs_ see the world through an ideological filter (such as a strong political view), and as they gather information, they interpret it through this filter. They tend to make quick judgments, stick to their first take, and resist changing their minds.
  * _Foxes_ start not with a broad worldview, but with specific facts. They deliberately gather information from opposing disciplines and sources. They’re slow to commit to a position and quick to change their minds when the evidence undercuts their opinion. 



Silver argues that although foxes make more accurate predictions than hedgehogs, hedgehog predictions get more attention because **the hedgehog thinking style is much more media-friendly** : Hedgehogs give good sound bites, they’re sure of themselves (which translates as confidence and charisma), and they draw support from partisan audiences who agree with their worldview.

> **In Defense of Hedgehogs**
> 
> Although Silver seems to favor fox-style thinking (which makes sense given his focus on making better predictions), other experts point out that both thinking types have their benefits. In _Good to Great_ , for example, Jim Collins argues that hedgehogs make good leaders precisely because of their talent for simplifying reality into a clear, memorable vision and rejecting any distractions from that vision. Plus, in the right role, the same media-friendly charisma that can make hedgehogs unreliable predictors also makes them inspiring leaders.
> 
> Furthermore, while Silver and Collins imply that everyone’s _either_ a fox or a hedgehog, it might be possible to be both at different times and to combine both styles. For example, one company founder shares his experience of shifting from fox-type thinking when he worked as a consultant to hedgehog-type thinking when he started his own company, only to find that neither mindset alone was serving his needs. He argues that his company only found success when they combined the hedgehog’s decisive, big-picture thinking with the fox’s cautious realism.

#### Our Incentives Are Wrong

Furthermore, Silver points to the media’s preference for hedgehog-style predictions as an example of how **our predictions can be warped by bad incentives**. In this case, for forecasters interested in garnering attention and fame (which means more airtime and more money), bad practices pay off. That’s because all the things that make for good predictions—cautious precision, attentiveness to uncertainty, and a willingness to change your mind—are a lot less compelling on TV than qualities that lead to worse predictions—broad, bold claims, certainty, and stubbornness.

(Shortform note: In _The Art of Thinking Clearly_ , Rolf Dobelli explains _why_ we’re drawn to predictors who exude certainty and confidence. For one thing, he says, we dislike ambiguity and uncertainty—in fact, that’s why we try to predict the future in the first place. Meanwhile, we’re quick to assume that experts know what they’re doing (a phenomenon called “authority bias”) and we struggle to understand the probabilities involved in careful forecasts. Taken all together, these mental biases combine to make simplistic, confident predictions appear more convincing than more nuanced (but possibly more accurate) ones.)

Similarly, Silver explains that **predictions can be compromised when forecasters are concerned about their reputations**. For example, he says, economic forecasters from lesser-known firms are more likely to make bold, contrarian predictions in hopes of impressing others by getting a tough call right. Conversely, forecasters at more esteemed firms are more likely to make conservative predictions that stick closely to the consensus view because they don’t want to be embarrassed by getting a call wrong.

(Shortform note: Another reason forecasters might deliberately make bold predictions is to spur audiences into action. For example, in _Apocalypse Never_ , Michael Shellenberger argues that some climate activists employ this tactic by deliberately making alarmist predictions in hopes of prompting policy and behavioral changes. The problem with this approach, Shellenberger says, is that it can backfire: In the case of climate change, if you successfully convince people that we’ve damaged the planet beyond repair, they might simply give up hope rather than enacting the change you’d hoped for.)

### Part 3: Better Predictions Through Bayesian Logic

Although prediction is inherently difficult—and made more so by the various thinking errors we’ve outlined—Silver argues that it’s possible to make consistently more accurate predictions by following the principles of a statistical formula known as Bayes’ Theorem. Though Silver briefly discusses the mathematics of the formula, he’s most interested in how the theorem encourages us to _think_ while making predictions. According to Silver, Bayes’ Theorem suggests that **we make better predictions when we consider the prior likelihood of an event and update our predictions in response to the latest evidence**.

In this section, we’ll briefly describe Bayes’ Theorem, then we’ll explore the broader lessons Silver draws from it and offer concrete advice for improving the accuracy of your predictions.

#### The Principles of Bayesian Statistics

Bayes’s Theorem—named for Thomas Bayes, the English minister and mathematician who first articulated it—posits that **you can calculate the probability of event A with respect to a specific piece of evidence B**. To do so, Silver explains, you need to know (or estimate) three things:

  * The prior probability of event A, regardless of whether you discover evidence B—mathematically written as P(A)
  * The probability of observing evidence B _if_ event A occurs—written as P(B|A)
  * The probability of observing evidence B if event A _doesn’t_ occur—written as P(B|not A)



Bayes’ Theorem uses these values to calculate the probability of A given B—P(A|B)—as follows:

![signal-noise-1.jpg](https://media.shortform.com/images/signal-noise-1.jpg)

(Shortform note: This formula may look complicated, but in less mathematical terms, what it’s calculating is [the probability that you observe B _and_ A is true] divided by [the probability that you observe B at all _whether or not_ A is true—or P(B)]. In fact, Silver’s version of the formula (as written above) is a very common special case used when you don’t directly know P(B); that lengthy denominator is actually just a way to calculate P(B) using the information we’ve listed above.)

##### Principle #1: Consider the Prior Probability

To illustrate how Bayes’ Theorem works in practice, imagine that a stranger walks up to you on the street and correctly guesses your full name and date of birth. What are the chances that this person is psychic? Say that you estimate a 90% chance that _if_ this person is psychic, they’d successfully detect this information, whereas you estimate that a non-psychic person has only a 5% chance of doing the same (perhaps they know about you through a mutual friend). On the face of it, these numbers seem to suggest a pretty high chance (90% versus 5%) that you just met a psychic.

But Bayes’ Theorem reminds us that **prior probabilities are just as important as the evidence in front of us**. Say that before you met this stranger, you would’ve estimated a one in 1,000 chance that any given person could be psychic. That leaves us with the following values:

  * P(A|B) is the chance that a stranger is psychic given that they’ve correctly guessed your full name and date of birth. This is what you want to calculate.
  * P(A) is the chance that any random stranger is psychic. We set this at one in 1,000, or 0.001.
  * P(B|A) is the chance that a psychic could correctly guess your name and date of birth. We set this at 90%, or 0.9.
  * P(B|not A) is the chance that a non-psychic could correctly guess the same information. We set this at 5%, or 0.05.



Bayes’ Theorem yields the following calculation:

![signal-noise-2.jpg](https://media.shortform.com/images/signal-noise-2.jpg)

That’s an approximately 1.77% chance that the stranger is psychic based on current evidence. In other words, despite the comparatively high chances that a psychic stranger could detect your personal information while a non-psychic stranger couldn’t, the extremely low prior chance of any stranger being psychic means that even in these unusual circumstances, it’s quite unlikely you’re dealing with a psychic.

(Shortform note: In _Superforecasting_ , Tetlock sums all this math up in plain language by explaining that Bayesian thinkers form new beliefs that are a product of their old beliefs and new evidence. He also borrows Kahneman’s description of this process as taking an “outside view,” because when you start from base probability rates (such as the odds that any random stranger is psychic), you put yourself outside of your specific situation and you’re less likely to be unduly swayed by details that feel compelling but have only limited statistical significance (such as the stranger’s unlikely guesses about your personal information).)

##### Principle #2: Update Your Estimates

Silver further argues that **Bayes’ Theorem highlights the importance of updating your estimates in light of new evidence**. To do so, simply perform a new calculation whenever you encounter new facts and take the results of the previous calculation as your starting point. That way, your estimates build on each other and, in theory, gradually bring you closer to the truth.

For example, imagine that after guessing your name and birth date, the stranger proceeds to read your thoughts and respond to what you’re thinking before you say anything. Perhaps you’d once again set the chances of a psychic doing so (P(B|A)) at 90% versus 5% for a non-psychic (P(B|not A))—perhaps the stranger is a _very_ lucky guesser. But this time, instead of setting the prior chance of the stranger being psychic (P(A)) at one in 1,000, you’d set it at your previously calculated value of 0.017699—after all, this isn’t any random stranger, this is a random stranger who already successfully guessed your name and birth date. Given this prior evidence _and_ the new evidence of possible mind-reading, the new calculation yields:

![signal-noise-3.jpg](https://media.shortform.com/images/signal-noise-3.jpg)

Now you have an approximately 24.49% chance that you’re dealing with a psychic—that’s because, in non-mathematical terms, you updated your previously low estimate of psychic likelihood to account for further evidence of potential psychic ability—and logically enough, even your unlikely conclusion becomes likelier with more evidence in its favor.

(Shortform note: In other words, one of the benefits of Bayesian thinking is that it accounts for your prior assumptions while also letting you know when those assumptions might be wrong. In _Smarter Faster Better_ , Charles Duhigg explains how Annie Duke (_Thinking in Bets_) applied this kind of thinking during her professional poker career. Duke could often size up opponents at a glance by observing, for example, that 40-year-old businessmen often played recklessly. But to keep her edge, she had to stay alert to new information—such as a 40-year-old businessman who plays cautiously and rarely bluffs. Otherwise, her initial assumptions might lead her to bad decisions.)

#### Bayesian Lesson #1: Consider All Possibilities

Now that we’ve explored the mathematics of Bayes’ Theorem, let’s look at the broader implications of its underlying logic. First, building on the principle of considering prior probabilities, Silver argues that **it’s important to be open to a wide range of possibilities** , especially when you’re dealing with noisy data. Otherwise, you might develop blind spots that hinder your ability to predict accurately.

To illustrate this point, Silver argues that the US military’s failure to predict the Japanese attack on Pearl Harbor in 1941 shows that **it’s dangerous to commit too strongly to a specific theory when there’s scant evidence for _any_ particular theory**. He explains that in the weeks before the attack, the US military noticed a sudden dropoff in intercepted radio traffic from the Japanese fleet. According to Silver, most analysts concluded that the sudden radio silence was because the fleet was out of range of US military installations—they didn’t consider the possibility of an impending attack because they believed the main threat to the US navy was from domestic sabotage by Japanese Americans.

(Shortform note: More recent historical analysis challenges Silver’s account of US intelligence ahead of the Pearl Harbor attack. According to the National Archives, the US suspected a Japanese attack was imminent, but they misidentified the likely target. That’s because Japanese radio traffic in October and November suggested a concentration of forces in the Marshall Islands, which the US assumed meant that Japan would target the Philippines, not Hawaii. They even ordered a reconnaissance mission to gather more information about the suspected attack. As it happened, the mission was to leave from Pearl Harbor, and the aircraft chosen for the mission was still awaiting preparations when it was destroyed in the Japanese attack.)

Silver explains that **one reason we sometimes fail to see all the options is that it’s common to mistake a lack of precedence for a lack of possibility**. In other words, when an event is extremely uncommon or unlikely, we might conclude that it will _never_ happen, even when logic and evidence dictate that given enough time, it will. For example, Silver points out that before the attack on Pearl Harbor, the previous foreign attack on US territory came in the early 19th century—a fact that made it easy to forget that such an attack was even possible.

> **How to Estimate Prior Probabilities More Accurately**
> 
> Silver’s Pearl Harbor example also points out the importance of generating the best possible prior estimate: Even though Bayesian logic will in theory lead you to the correct answer eventually, on a practical level, you might not find the evidence you need to get there until it’s too late. Therefore, the closer your prior estimate is to the truth, the better off you’ll be.
> 
> In _Algorithms to Live By_ , Brian Christian and Tom Griffiths offer one way to come up with better prior estimates: base your predictions on the likely distribution of the event in question. For example, if the event typically falls within a bell curve distribution (in which most values cluster around a central average), you should start with the average and then adjust. So if you’re trying to predict a student’s grade in a class, you should start with the average grade (C) then adjust based on available evidence such as the student’s study habits and prior grades.
> 
> Basing your prior estimates on distributions also gives you a better chance of avoiding the precedence-possibility conflation Silver warns against. For example, if you’re trying to guess a stranger’s net worth, knowing that wealth follows a power law distribution (in which most values are clustered at one extreme with a few values at the opposite extreme) will help you remember that even if you’ve never met a billionaire before, it’s _possible_ (though unlikely) that the stranger is one. In fact, as Christian and Griffiths explain, you’ll need to rely more heavily than usual on the available evidence when dealing with power law distributions—so if the stranger drives up in a million-dollar supercar, revise your initial estimates quickly.

#### Bayesian Lesson #2: Follow the Evidence, Not Emotions or Trends

In addition to emphasizing the importance of considering all possibilities, Bayesian logic also highlights the need to stay focused on the evidence rather than getting sidetracked by other factors such as your emotional responses or the trends you observe.

For one thing, Silver argues that **when you give in to strong emotions, the quality of your predictions suffers**. He gives the example of poker players _going on tilt_ —that is, losing their cool due to a run of bad luck or some other stressor (such as fatigue or another player’s behavior). Poker depends on being able to accurately predict what cards your opponent might have—but Silver argues that when players are on tilt, they begin taking ill-considered risks (such as betting big on a weak hand) based more on anger and frustration than on solid predictions.

(Shortform note: To help keep your emotions in check, remember that getting a prediction wrong (such as by losing to an unlikely poker hand) doesn’t mean you _did_ anything wrong—it’s not uncommon for the best possible predictions to fail simply due to luck. In fact, in _Fooled By Randomness_ , Nassim Nicholas Taleb argues that luck is more important than skill in determining outcomes. He says that this is especially true because in many situations, early success leads to future success, such as when a startup experiences some early good fortune that allows it to survive long enough to benefit from more good fortune down the road.)

Furthermore, Silver says, it’s important not to be swayed by trends because **psychological factors such as herd mentality distort behaviors in unpredictable ways**. For instance, the stock market sometimes experiences bubbles that artificially inflate prices because of a runaway feedback loop: Investors see prices going up and want to jump on a hot market, so they buy, which drives prices up even further and convinces more investors to do the same—even when there’s no rational reason for prices to be spiking in the first place.

(Shortform note: The authors of _Noise_ explain that these kinds of feedback loops result from a psychological phenomenon known as an _information cascade_ in which real or perceived popularity influences how people interpret information. Moreover, the authors argue that information cascades often amount to little more than luck—whichever idea receives initial support (which typically correlates with which idea is presented first) tends to be the idea that wins out regardless of its merits.)

### Part 4: Two Challenges for Today’s Forecasters

Although Silver maintains that his suggestions can improve the quality of our predictions, he cautions that it’s more difficult than ever to translate better prediction _theory_ into better predictions _in practice_. We’ve already discussed one reason for that: namely, that contemporary technology forces us to wade through more noise to find meaningful signal. But Silver also argues that **since the late 2010s, unprecedented political and social fragmentation have made prediction even harder**. In this section, we’ll discuss how this fragmentation has complicated forecasters’ jobs by decreasing the diversity of thought and eroding public trust in expert advice.

#### Challenge #1: Increasingly Insular Groups

One problem for contemporary forecasters, according to Silver, is that **contemporary news and social media encourage people to sort themselves into like-minded subgroups, which harms predictions** by encouraging herd mentality and quashing opposing views. According to Silver, the best predictions often come when we combine diverse, independent viewpoints in order to consider a problem from all angles. Conversely, when you only listen to people who think the way you do, you’re likely to simply reinforce what you already believe—which isn’t a recipe for good predictions.

(Shortform note: In _Think Again_ , Adam Grant suggests countering this insularity through _reconsideration_ —a process of deliberately questioning and challenging your beliefs. According to Grant, one of the keys to reconsideration is to actively seek out opposing views and genuinely attempt to understand them. Doing so, he says, makes your understanding of the world more complex and helps you avoid the kinds of stereotypes that so easily develop in the “us versus them” atmosphere of the like-minded groups Silver describes.)

#### Challenge #2: The Erosion of Trust

Moreover, Silver argues, the same factors that have led to more insular, polarized groups have also led many people to dismiss opinions that come from outside of the group’s narrow consensus. One effect of this trend is that people trust public institutions and expert opinions less than ever before, which creates a climate in which**people tend to dismiss accurate—and urgent—predictions** , sometimes without clear reasons. He gives the example of the 2020 outbreak of Covid-19, when a significant number of people dismissed expert predictions about the virus’s likely spread and impact and, as a result, ignored or pushed back against the recommended public health protocols.

(Shortform note: The reasons that people ignore expert opinions may go deeper than recent political polarization. For example, some studies suggest that inherent distrust of experts isn’t necessarily linked to belonging to a specific ideological group as Silver suggests. Nor is it a new problem: Historian Richard Hofstadter wrote about ambivalence toward expertise in US culture as early as 1963. Moreover, it’s possible that humans are simply prone to doing the opposite of what they’re told—behavioral scientists point out that we’re susceptible to a type of behavior called _reactance, which involves acting contrarily when we perceive threats to our individual freedom_ —such as when we’re asked to wear masks or avoid public gatherings.)

[[book_md/the-signal-and-the-noise/preview|preview]]

[[book_md/the-signal-and-the-noise/exercise-improve-your-decisions|exercise-improve-your-decisions]]

##### Welcome!

Let’s go on a quick tour of a Shortform book guide. 

Start

##### 1-Page Summary

Every guide starts with a 1-Page Summary. This is a 5-10 minute overview of the book’s key points. 

Next

##### Finished!

If you ever need to see this tour again, click here. 

Close

Guided Tour

__

  *   * Allow anyone to **view** this annotation
  * Allow anyone to **edit** this annotation



* * *

Save Cancel

__




![](https://bat.bing.com/action/0?ti=56018282&Ver=2&mid=5374f55c-337b-42c3-8032-388fefa4255d&sid=1711133063fa11eebdec89a8b8ae3bbc&vid=171147a063fa11eea7440fcfeb230d96&vids=0&msclkid=N&pi=0&lg=en-US&sw=800&sh=600&sc=24&nwd=1&tl=Shortform%20%7C%20The%20Signal%20and%20the%20Noise&p=https%3A%2F%2Fwww.shortform.com%2Fapp%2Fbook%2Fthe-signal-and-the-noise%2F1-page-summary&r=&lt=453&evt=pageLoad&sv=1&rn=121949)
